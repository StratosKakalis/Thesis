{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T08:52:19.177726Z","iopub.status.busy":"2024-05-17T08:52:19.177340Z","iopub.status.idle":"2024-05-17T08:52:34.596619Z","shell.execute_reply":"2024-05-17T08:52:34.595606Z","shell.execute_reply.started":"2024-05-17T08:52:19.177696Z"},"trusted":true},"outputs":[],"source":["!pip install sparqlwrapper"]},{"cell_type":"markdown","metadata":{},"source":["Get prompt."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:05:48.100474Z","iopub.status.busy":"2024-05-17T09:05:48.100014Z","iopub.status.idle":"2024-05-17T09:05:48.106125Z","shell.execute_reply":"2024-05-17T09:05:48.105106Z","shell.execute_reply.started":"2024-05-17T09:05:48.100443Z"},"trusted":true},"outputs":[],"source":["#user_prompt = input(\"Enter a prompt: \")\n","user_prompt = \"Select all cities of Greece and their population\"\n","\n","prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: {user_prompt}\n","Generator: ```\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["Pass the prompt to the LLM."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T08:52:34.606576Z","iopub.status.busy":"2024-05-17T08:52:34.606210Z","iopub.status.idle":"2024-05-17T08:52:37.982890Z","shell.execute_reply":"2024-05-17T08:52:37.981898Z","shell.execute_reply.started":"2024-05-17T08:52:34.606546Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def run_inference(model, tokenizer, prompt):\n","    results = []\n","    \n","    if tokenizer == None:\n","        # Generate output\n","        with torch.no_grad():\n","            outputs = model(prompt)\n","            \n","        # Decode and print output\n","        print(\"Prompt:\", prompt)\n","        print(\"Generated text:\" + outputs + \"\\n\")\n","        results.append(\"Generated text:\" + outputs)\n","    else:\n","        # Move model to GPU\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","        model.eval()  # Set model to evaluation mode\n","            \n","        # Tokenize prompt\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","            \n","        # Generate output\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, \n","                            max_length=500,  # Set a maximum length for generated text\n","                            #do_sample=True,  # Enable sampling\n","                            #top_k=7,        # Top-k sampling\n","                            #top_p=0.1,      # Top-p sampling (nucleus sampling)\n","                            #num_return_sequences=1,\n","                            #repetition_penalty=1, # No penalty for instruction tuned models.\n","                            repetition_penalty=1.2, # Penalty on repeating tokens.\n","                            eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n","                            pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n","                            )\n","        \n","        # Extract generated text\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Remove the prompt text\n","        prompt_length = len(prompt)\n","        generated_text = generated_text[prompt_length:]\n","\n","        # Decode and print output\n","        print(\"Prompt:\", prompt)\n","        print(generated_text)\n","        results.append(generated_text)\n","    \n","    # Clear model from RAM\n","    del model\n","    torch.cuda.empty_cache()\n","    \n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T08:52:37.985456Z","iopub.status.busy":"2024-05-17T08:52:37.985004Z","iopub.status.idle":"2024-05-17T08:55:04.359969Z","shell.execute_reply":"2024-05-17T08:55:04.357655Z","shell.execute_reply.started":"2024-05-17T08:52:37.985427Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:05:56.319048Z","iopub.status.busy":"2024-05-17T09:05:56.318621Z","iopub.status.idle":"2024-05-17T09:06:05.873098Z","shell.execute_reply":"2024-05-17T09:06:05.872051Z","shell.execute_reply.started":"2024-05-17T09:05:56.319005Z"},"trusted":true},"outputs":[],"source":["results = run_inference(model, tokenizer, prompt)\n","\n","end_index = results[0].find(\"```\")\n","\n","# Extract the substring from the start of the string up to the first occurrence of ```\n","if end_index != -1:\n","    query = results[0][:end_index]\n","else:\n","    # If ``` is not found, keep the original string\n","    query = results[0]\n","\n","# Now remove the SPARQL prefix that the model adds.\n","start_index = query.find(\"SPARQL\")\n","if start_index == 0:\n","    # Remove the prefix and all characters leading up to it\n","    query = query[start_index + len(\"SPARQL\"):]\n","\n","print(\"QUERY: \", query)"]},{"cell_type":"markdown","metadata":{},"source":["Run the generated SPARQL query against a Dbpedia endpoint and get the results."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:06:49.379466Z","iopub.status.busy":"2024-05-17T09:06:49.379046Z","iopub.status.idle":"2024-05-17T09:06:50.240202Z","shell.execute_reply":"2024-05-17T09:06:50.239148Z","shell.execute_reply.started":"2024-05-17T09:06:49.379435Z"},"trusted":true},"outputs":[],"source":["from SPARQLWrapper import SPARQLWrapper, JSON\n","\n","# Set the DBpedia endpoint URL\n","endpoint_url = \"http://dbpedia.org/sparql\"\n","\n","# Create a SPARQLWrapper object, specifying the endpoint URL\n","sparql = SPARQLWrapper(endpoint_url)\n","\n","# Define your SPARQL query\n","# sparql_query = \"\"\"\n","#     SELECT ?country ?population\n","#     WHERE {\n","#         ?country rdf:type dbo:Country ;\n","#                  dbo:populationTotal ?population .\n","#         FILTER (?population > 5000000000)\n","#     }\n","#     LIMIT 10\n","# \"\"\"\n","\n","sparql_query = query\n","\n","# Set the SPARQL query string\n","sparql.setQuery(sparql_query)\n","\n","# Set the query type (in this case, it's a SELECT query)\n","sparql.setReturnFormat(JSON)\n","\n","# Execute the SPARQL query and parse the results\n","try:\n","    # Execute the query and convert the result into JSON format\n","    results = sparql.query().convert()\n","    \n","    print(results)\n","    # Process the results\n","#     for result in results[\"results\"][\"bindings\"]:\n","#         country_name = result[\"country\"][\"value\"]\n","#         population = result[\"population\"][\"value\"]\n","#         print(f\"Country: {country_name}, Population: {population}\")\n","\n","except Exception as e:\n","    print(f\"Error executing SPARQL query: {e}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
