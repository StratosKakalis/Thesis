{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T10:24:49.558650Z","iopub.status.busy":"2024-04-25T10:24:49.558274Z","iopub.status.idle":"2024-04-25T10:24:49.907304Z","shell.execute_reply":"2024-04-25T10:24:49.905741Z","shell.execute_reply.started":"2024-04-25T10:24:49.558620Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"markdown","metadata":{},"source":["Define a function to run inference on a given model on a set of pre-defined prompts and return an array with the responses."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:24:49.908030Z","iopub.status.idle":"2024-04-25T10:24:49.908399Z","shell.execute_reply":"2024-04-25T10:24:49.908239Z","shell.execute_reply.started":"2024-04-25T10:24:49.908224Z"},"trusted":true},"outputs":[],"source":["!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:24:49.910113Z","iopub.status.idle":"2024-04-25T10:24:49.910541Z","shell.execute_reply":"2024-04-25T10:24:49.910338Z","shell.execute_reply.started":"2024-04-25T10:24:49.910322Z"},"trusted":true},"outputs":[],"source":["# List of prompts\n","prompts = [\n","\"\"\"Generator is an expert SPARQL query generator.\n","For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question.\n","Human: \"Where is the Dorset county located?\"\n","Generator:\"\"\",\n","\"\"\"Generator creates valid SPARQL queries.The user will provide a question and the generator will convert it into an equivalent SPARQL query that answers the user's question.\n","Human: \"What is the population of Aegina ?\"\n","Generator: \"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the YAGO knowledge graph.\n","Human: \"Where is the Dorset county located?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph.\n","Human: \"What is the river whose mouth is in deadsea?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the Wikidata knowledge graph.\n","Human: \"Was Hans Ertl a screenwriter?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator.\n","For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question.\n","Human: \"Where is Oxfordshire located?\"\n","Generator: \"SELECT ?WKT WHERE { yago:Oxfordshire geo:hasGeometry ?o. ?o geo:asWKT ?WKT. }\"\n","Human: \"What is the total area of County Galway?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the YAGO knowledge graph.\n","Human: \"What is the total area of Glengarra Wood forest?\"\n","Generator: \"select distinct (strdf:area(?geoWKT) as ?area) where { <http://yago-knowledge.org/resource/geoentity_Glengarra_Wood_3300941> geo:hasGeometry ?o. ?o geo:asWKT ?geoWKT. }\"\n","Human: \"What is the population of Piraeus?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph.\n","Human: \"What is the river whose mouth is in deadsea?\"\n","Generator: \"SELECT DISTINCT ?uri WHERE {?uri <http://dbpedia.org/ontology/riverMouth> <http://dbpedia.org/resource/Dead_Sea>  . ?uri <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://dbpedia.org/ontology/River>}\"\n","Human: \"What is the region of Tom Perriello ?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the Wikidata knowledge graph.\n","Human: \"Was Hans Ertl a screenwriter?\"\n","Generator: \"ASK WHERE {\\nwd:Q103013 wdt:P106 wd:Q69423232}\"\n","Human: \"Who were Jean-François Champollion's parents?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator.\n","For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question.\n","Human: \"Where is Oxfordshire located?\"\n","Generator: \"SELECT ?WKT WHERE { yago:Oxfordshire geo:hasGeometry ?o. ?o geo:asWKT ?WKT. }\"\n","Human: \"What is Dublin's administrative type?\"\n","Generator: \"select ?e where { yago:Dublin rdf:type ?e }\"\n","Human: \"What population does Icaria have?\"\n","Generator: \"SELECT ?population WHERE{ yago:Icaria  y2geoo:hasGAG_Population ?population . }\"\n","Human: \"Where is Scotland located?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator.\n","For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question.\n","Human: \"Which localities are located east of forests in County Wicklow?\"\n","Generator: \"SELECT DISTINCT ?a WHERE { yago:County_Wicklow geo:hasGeometry ?o . ?o geo:asWKT ?geoWKT . ?a rdf:type y2geoo:OSM_locality; geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?b rdf:type y2geoo:OSM_forest; geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . FILTER (strdf:within(?geoWKT1, ?geoWKT) &&  strdf:within(?geoWKT2, ?geoWKT) &&  strdf:right(?geoWKT1, ?geoWKT2)) }\"\n","Human: \"Is there a stream located east of a lake in Corfu?\"\n","Generator: \"ASK { yago:Corfu geo:hasGeometry ?o2 . ?o2 geo:asWKT ?xWKT2 . ?x2 rdf:type y2geoo:OSM_stream . ?x2 geo:hasGeometry ?x2Geom. ?x2Geom geo:asWKT ?iWKT2. ?x1 rdf:type y2geoo:OSM_lake . ?x1 geo:hasGeometry ?x1Geom. ?x1Geom geo:asWKT ?iWKT1. FILTER(geof:sfWithin(?iWKT1, ?xWKT2) && geof:sfWithin(?iWKT2,?xWKT2) && strdf:right(?iWKT2,?iWKT1)) }\"\n","Human: \"Which region of Greece has the most inhabitants?\"\n","Generator: \"SELECT DISTINCT ?region WHERE { ?region rdf:type y2geoo:GAG_Region . ?region y2geoo:hasGAG_Population ?population } ORDER BY DESC (?population) LIMIT 1\"\n","Human: \"Which county in the British Isles is the smallest by area?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the YAGO knowledge graph.\n","Human: \"Which are the 2 newest bridges of Ireland?\"\n","Generator: \"SELECT DISTINCT ?bridge  WHERE {  ?type rdfs:subClassOf+ yago:wordnet_bridge_102898711. ?bridge a ?type .?bridge yago:isLocatedIn+ yago:California.?bridge yago:wasCreatedOnDate ?date.} ORDER BY DESC(?date)  LIMIT 2\"\n","Human: \"Is there a stream located east of a lake in Corfu?\"\n","Generator: \"ASK { yago:Corfu geo:hasGeometry ?o2 . ?o2 geo:asWKT ?xWKT2 . ?x2 rdf:type y2geoo:OSM_stream . ?x2 geo:hasGeometry ?x2Geom. ?x2Geom geo:asWKT ?iWKT2. ?x1 rdf:type y2geoo:OSM_lake . ?x1 geo:hasGeometry ?x1Geom. ?x1Geom geo:asWKT ?iWKT1. FILTER(geof:sfWithin(?iWKT1, ?xWKT2) && geof:sfWithin(?iWKT2,?xWKT2) && strdf:right(?iWKT2,?iWKT1)) }\"\n","Human: \"Which region of Greece has the most inhabitants?\"\n","Generator: \"SELECT DISTINCT ?region WHERE { ?region rdf:type y2geoo:GAG_Region . ?region y2geoo:hasGAG_Population ?population } ORDER BY DESC (?population) LIMIT 1\"\n","Human: \"Is Leitrim the least populated county in the Republic of Ireland?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph.\n","Human: \"What is the river whose mouth is in deadsea?\"\n","Generator: \"SELECT DISTINCT ?uri WHERE {?uri <http://dbpedia.org/ontology/riverMouth> <http://dbpedia.org/resource/Dead_Sea>  . ?uri <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://dbpedia.org/ontology/River>}\"\n","Human: \"What is the region of Tom Perriello ?\"\n","Generator: \"SELECT DISTINCT ?uri WHERE { <http://dbpedia.org/resource/Tom_Perriello> <http://dbpedia.org/ontology/region> ?uri }\"\n","Human: \"How many people live in Wilton, Connecticut?\"\n","Generator: \"SELECT DISTINCT COUNT(?uri) WHERE {?uri <http://dbpedia.org/property/residence> <http://dbpedia.org/resource/Wilton,_Connecticut>  . }\"\n","Human: \"In which part of the world can i find Xynisteri and Mavro?\"\n","Generator:\"\"\",\n","\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the Wikidata knowledge graph.\n","Human: \"Was Hans Ertl a screenwriter?\"\n","Generator: \"ASK WHERE {\\nwd:Q103013 wdt:P106 wd:Q69423232}\"\n","Human: \"Who were Jean-François Champollion's parents?\"\n","Generator: \"SELECT DISTINCT ?x0 WHERE {\\n?x0 wdt:P40|wdt:P355 wd:Q260 \\n}\"\n","Human: \"What did Andrei Tarkovsky edit?\"\n","Generator: \"SELECT DISTINCT ?x0 WHERE {\\n?x0 wdt:P1040 wd:Q853 \\n}\"\n","Human: \"Was Andrei Tarkovsky a screenwriter?\"\n","Generator:\"\"\"]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T15:59:36.980951Z","iopub.status.busy":"2024-04-24T15:59:36.980617Z","iopub.status.idle":"2024-04-24T15:59:40.574101Z","shell.execute_reply":"2024-04-24T15:59:40.573294Z","shell.execute_reply.started":"2024-04-24T15:59:36.980921Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def run_inference(model, tokenizer, model_name):\n","    results = []\n","    \n","    print(f\"Using model: {model_name}\")\n","    \n","    # Loop through each prompt\n","    for prompt in prompts:\n","        if tokenizer == None:\n","            # Generate output\n","            with torch.no_grad():\n","                outputs = model(prompt)\n","            \n","            # Decode and print output\n","            print(\"Prompt:\", prompt)\n","            print(\"Generated text:\" + outputs + \"\\n\")\n","            results.append(\"Generated text:\" + outputs)\n","        else:\n","            # Move model to GPU\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            model.to(device)\n","            model.eval()  # Set model to evaluation mode\n","            \n","            # Tokenize prompt\n","            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","            \n","            # Generate output\n","            with torch.no_grad():\n","                outputs = model.generate(**inputs, \n","                                 max_length=500,  # Set a maximum length for generated text\n","                                 #do_sample=True,  # Enable sampling\n","                                 #top_k=7,        # Top-k sampling\n","                                 #top_p=0.1,      # Top-p sampling (nucleus sampling)\n","                                 #num_return_sequences=1,\n","                                 #repetition_penalty=1, # No penalty for instruction tuned models.\n","                                 repetition_penalty=1.2, # Penalty on repeating tokens.\n","                                 eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n","                                 pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n","                                )\n","        \n","            # Extract generated text\n","            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","            # Remove the prompt text\n","            prompt_length = len(prompt)\n","            generated_text = generated_text[prompt_length:]\n","\n","            # Decode and print output\n","            print(\"Prompt:\", prompt)\n","            print(\"Generated text:\" + generated_text + \"\\n\")\n","            results.append(\"Generated text:\" + generated_text)\n","    \n","    # Clear model from RAM\n","    del model\n","    torch.cuda.empty_cache()\n","    \n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["## Now run inference for each of the following models:"]},{"cell_type":"markdown","metadata":{},"source":["* ### meta-llama/Llama-2-7b-hf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T15:51:18.461442Z","iopub.status.busy":"2024-04-16T15:51:18.460412Z","iopub.status.idle":"2024-04-16T15:53:02.971473Z","shell.execute_reply":"2024-04-16T15:53:02.970552Z","shell.execute_reply.started":"2024-04-16T15:51:18.461409Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n"," \n","run_inference(model, tokenizer, \"llama-2-7b-hf\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### ekshat/Llama-2-7b-chat-finetune-for-text2sql"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T15:55:02.932959Z","iopub.status.busy":"2024-04-16T15:55:02.932250Z","iopub.status.idle":"2024-04-16T15:59:01.659565Z","shell.execute_reply":"2024-04-16T15:59:01.658605Z","shell.execute_reply.started":"2024-04-16T15:55:02.932932Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"ekshat/Llama-2-7b-chat-finetune-for-text2sql\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"ekshat/Llama-2-7b-chat-finetune-for-text2sql\")\n","\n","run_inference(model, tokenizer, \"ekshat/Llama-2-7b-chat-finetune-for-text2sql\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### TheBloke/Llama-2-13B-GGUF"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T16:55:31.235605Z","iopub.status.busy":"2024-04-16T16:55:31.234749Z","iopub.status.idle":"2024-04-16T16:55:44.859366Z","shell.execute_reply":"2024-04-16T16:55:44.858341Z","shell.execute_reply.started":"2024-04-16T16:55:31.235572Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install ctransformers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T16:02:27.902216Z","iopub.status.busy":"2024-04-16T16:02:27.901891Z","iopub.status.idle":"2024-04-16T16:11:26.473494Z","shell.execute_reply":"2024-04-16T16:11:26.472579Z","shell.execute_reply.started":"2024-04-16T16:02:27.902186Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from ctransformers import AutoModelForCausalLM\n","\n","# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n","model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-13B-GGUF\", model_file=\"llama-2-13b.Q6_K.gguf\", model_type=\"llama\", gpu_layers=50)\n","\n","run_inference(model, tokenizer=None, model_name=\"TheBloke/Llama-2-13B-GGUF\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### TheBloke/CodeLlama-13B-GGUF"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T16:55:44.861759Z","iopub.status.busy":"2024-04-16T16:55:44.861404Z","iopub.status.idle":"2024-04-16T17:05:36.206654Z","shell.execute_reply":"2024-04-16T17:05:36.205775Z","shell.execute_reply.started":"2024-04-16T16:55:44.861730Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from ctransformers import AutoModelForCausalLM\n","\n","# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n","model = AutoModelForCausalLM.from_pretrained(\"TheBloke/CodeLlama-13B-GGUF\", model_file=\"codellama-13b.Q6_K.gguf\", model_type=\"llama\", gpu_layers=50)\n","\n","run_inference(model, tokenizer=None, model_name=\"TheBloke/CodeLlama-2-13B-GGUF\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Mistral-7b-v0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T17:13:22.528707Z","iopub.status.busy":"2024-04-16T17:13:22.527713Z","iopub.status.idle":"2024-04-16T17:18:23.369272Z","shell.execute_reply":"2024-04-16T17:18:23.367286Z","shell.execute_reply.started":"2024-04-16T17:13:22.528663Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")\n","\n","run_inference(model, tokenizer, \"alpindale/Mistral-7B-v0.2-hf\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Mistral-7b-Instruct-v0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T17:21:33.309492Z","iopub.status.busy":"2024-04-16T17:21:33.308616Z","iopub.status.idle":"2024-04-16T17:25:46.840243Z","shell.execute_reply":"2024-04-16T17:25:46.837976Z","shell.execute_reply.started":"2024-04-16T17:21:33.309459Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n","\n","run_inference(model, tokenizer, \"mistralai/Mistral-7B-Instruct-v0.2\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Dolphin-2.8-mistral-7b-v0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T17:27:34.185676Z","iopub.status.busy":"2024-04-16T17:27:34.184972Z","iopub.status.idle":"2024-04-16T17:31:21.286094Z","shell.execute_reply":"2024-04-16T17:31:21.285167Z","shell.execute_reply.started":"2024-04-16T17:27:34.185630Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"cognitivecomputations/dolphin-2.8-mistral-7b-v02\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"cognitivecomputations/dolphin-2.8-mistral-7b-v02\")\n","\n","run_inference(model, tokenizer, \"cognitivecomputations/dolphin-2.8-mistral-7b-v02\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Hermes-2-Pro-Mistral-7b"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:32:09.368698Z","iopub.status.busy":"2024-04-16T18:32:09.368331Z","iopub.status.idle":"2024-04-16T18:32:09.377770Z","shell.execute_reply":"2024-04-16T18:32:09.376736Z","shell.execute_reply.started":"2024-04-16T18:32:09.368668Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def t5_inference(model, tokenizer, model_name):\n","    results = []\n","    \n","    print(f\"Using model: {model_name}\")\n","    \n","    # Loop through each prompt\n","    for prompt in prompts:\n","        # Move model to GPU\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","        model.eval()  # Set model to evaluation mode\n","            \n","        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n","        outputs = model.generate(input_ids, min_length=500, max_length=1000, early_stopping=False, \n","                                 repetition_penalty=1.2, # Penalty on repeating tokens.\n","                                 eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n","                                 pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n","                                )\n","                                \n","        # Extract generated text\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Decode and print output\n","        print(\"Prompt:\", prompt)\n","        print(\"Generated text:\" + generated_text + \"\\n\")\n","        results.append(\"Generated text:\" + generated_text)\n","    \n","    # Clear model from RAM\n","    del model\n","    torch.cuda.empty_cache()\n","    \n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T17:31:28.644294Z","iopub.status.busy":"2024-04-16T17:31:28.643520Z","iopub.status.idle":"2024-04-16T17:37:49.661739Z","shell.execute_reply":"2024-04-16T17:37:49.660794Z","shell.execute_reply.started":"2024-04-16T17:31:28.644259Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Hermes-2-Pro-Mistral-7B\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Hermes-2-Pro-Mistral-7B\")\n","\n","run_inference(model, tokenizer, \"NousResearch/Hermes-2-Pro-Mistral-7B\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Google T5-3b"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:32:12.611872Z","iopub.status.busy":"2024-04-16T18:32:12.611468Z","iopub.status.idle":"2024-04-16T18:33:33.971687Z","shell.execute_reply":"2024-04-16T18:33:33.970692Z","shell.execute_reply.started":"2024-04-16T18:32:12.611842Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n","model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n","\n","t5_inference(model, tokenizer, \"T5-3b\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Google T5v1.1-XL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:33:33.973859Z","iopub.status.busy":"2024-04-16T18:33:33.973374Z","iopub.status.idle":"2024-04-16T18:46:14.105233Z","shell.execute_reply":"2024-04-16T18:46:14.104169Z","shell.execute_reply.started":"2024-04-16T18:33:33.973830Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google/t5-v1_1-xl\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-v1_1-xl\")\n","\n","t5_inference(model, tokenizer, \"T5v1.1-xl\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Falcon-7b-instruct"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T11:56:32.641263Z","iopub.status.busy":"2024-04-21T11:56:32.640583Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code=True, torch_dtype=torch.float16)\n","\n","run_inference(model, tokenizer, \"Falcon-7b-Instruct\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Llama-3-8b"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T13:37:23.559100Z","iopub.status.busy":"2024-04-24T13:37:23.558405Z","iopub.status.idle":"2024-04-24T13:37:27.144448Z","shell.execute_reply":"2024-04-24T13:37:27.143647Z","shell.execute_reply.started":"2024-04-24T13:37:23.559066Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def Quantized_Inference(model, tokenizer, model_name):\n","    results = []\n","    \n","    print(f\"Using model: {model_name}\")\n","    \n","    # Loop through each prompt\n","    for prompt in prompts:\n","        # Move model to GPU\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.eval()  # Set model to evaluation mode\n","            \n","        # Tokenize prompt\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","            \n","        # Generate output\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, \n","                                 max_length=350,  # Set a maximum length for generated text\n","                                 #do_sample=True,  # Enable sampling\n","                                 #top_k=7,        # Top-k sampling\n","                                 #top_p=0.1,      # Top-p sampling (nucleus sampling)\n","                                 #num_return_sequences=1,\n","                                 repetition_penalty=1.2, # Penalty on repeating tokens.\n","                                 eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n","                                 pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n","                                )\n","        \n","        # Extract generated text\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Remove the prompt text\n","        prompt_length = len(prompt)\n","        generated_text = generated_text[prompt_length:]\n","\n","        # Decode and print output\n","        print(\"Prompt:\", prompt)\n","        print(\"Generated text:\" + generated_text + \"\\n\")\n","        results.append(\"Generated text:\" + generated_text)\n","    \n","    # Clear model from RAM\n","    del model\n","    torch.cuda.empty_cache()\n","    \n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T15:52:00.311988Z","iopub.status.busy":"2024-04-22T15:52:00.311393Z","iopub.status.idle":"2024-04-22T16:18:55.809507Z","shell.execute_reply":"2024-04-22T16:18:55.808454Z","shell.execute_reply.started":"2024-04-22T15:52:00.311948Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n","model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", device_map=\"auto\", load_in_8bit=True)\n","\n","print(model.get_memory_footprint())\n","\n","Quantized_Inference(model, tokenizer, \"Llama-3-8b\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Llama-3-8b-Instruct"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:41:28.656125Z","iopub.status.busy":"2024-04-23T12:41:28.655667Z","iopub.status.idle":"2024-04-23T12:53:17.076319Z","shell.execute_reply":"2024-04-23T12:53:17.075322Z","shell.execute_reply.started":"2024-04-23T12:41:28.656096Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n","model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", load_in_8bit=True)\n","\n","Quantized_Inference(model, tokenizer, \"Llama-3-8b_Instruct\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Google Gemma 2b base"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T15:42:48.318203Z","iopub.status.busy":"2024-04-24T15:42:48.317763Z","iopub.status.idle":"2024-04-24T15:45:58.512685Z","shell.execute_reply":"2024-04-24T15:45:58.511775Z","shell.execute_reply.started":"2024-04-24T15:42:48.318176Z"},"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n","model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", torch_dtype=torch.float32)\n","\n","run_inference(model, tokenizer, \"Gemma-2b\")"]},{"cell_type":"markdown","metadata":{},"source":["* ### Google Gemma 2b Instruct"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T15:59:40.576969Z","iopub.status.busy":"2024-04-24T15:59:40.576111Z","iopub.status.idle":"2024-04-24T16:04:12.212838Z","shell.execute_reply":"2024-04-24T16:04:12.211828Z","shell.execute_reply.started":"2024-04-24T15:59:40.576934Z"},"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n","model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", torch_dtype=torch.float32)\n","\n","run_inference(model, tokenizer, \"Gemma-2b-it\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
