{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8812575,"sourceType":"datasetVersion","datasetId":5299649},{"sourceId":9197625,"sourceType":"datasetVersion","datasetId":5560618},{"sourceId":9276358,"sourceType":"datasetVersion","datasetId":5614381}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load 100 question dataset.","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\n\nwith open('/kaggle/input/100-geo-questions-uri/100_Sub_Dataset_URI.json', 'r') as file:\n    original_dataset = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T18:58:36.704271Z","iopub.execute_input":"2024-09-05T18:58:36.704743Z","iopub.status.idle":"2024-09-05T18:58:40.639500Z","shell.execute_reply.started":"2024-09-05T18:58:36.704706Z","shell.execute_reply":"2024-09-05T18:58:40.638086Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# NER Pipeline","metadata":{}},{"cell_type":"markdown","source":"* Load model","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2024-09-05T18:58:40.642284Z","iopub.execute_input":"2024-09-05T18:58:40.644026Z","iopub.status.idle":"2024-09-05T18:58:41.448056Z","shell.execute_reply.started":"2024-09-05T18:58:40.643952Z","shell.execute_reply":"2024-09-05T18:58:41.446821Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n\n# model = AutoModelForCausalLM.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\", torch_dtype=torch.float16)\n# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T18:58:41.449960Z","iopub.execute_input":"2024-09-05T18:58:41.450318Z","iopub.status.idle":"2024-09-05T18:58:41.455559Z","shell.execute_reply.started":"2024-09-05T18:58:41.450288Z","shell.execute_reply":"2024-09-05T18:58:41.454068Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"* Mistral it","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T18:58:41.458360Z","iopub.execute_input":"2024-09-05T18:58:41.459084Z","iopub.status.idle":"2024-09-05T19:01:21.507246Z","shell.execute_reply.started":"2024-09-05T18:58:41.459045Z","shell.execute_reply":"2024-09-05T19:01:21.504952Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e091fd9a63004bfaaf69d7a71314f05e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b58eca676d24e97bf27a7865161d8bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801d2db29a9f43538a0c07bea2176787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050d8b276a8b45678aeaf3dfed1c8398"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc4e37501a94dfd99ff2d919703d81b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b05b7c1202450988b06dc37f298440"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"971ca58852bd43ffa41593fceccf26eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04de24f049374d8a9de2da456fc02127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b58e4572ac34120aaf8f36c81dd9155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"011b3808c97c46418b4c57973d6644cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424ccd9c3f47480291a8804fdfcd859a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db2f3712e2224619a4051b84bc2469c9"}},"metadata":{}}]},{"cell_type":"markdown","source":"* chat template inference function","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef run_chat_inference(model, tokenizer, system_role, question, max_tokens=15):    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_role},\n        {\"role\": \"user\", \"content\": question}\n    ]\n\n    tokenizer.apply_chat_template(messages, tokenize=False)\n\n    model_inputs = tokenizer.apply_chat_template(messages, return_tensors = \"pt\").to(device)\n    \n    generated_ids = model.generate(\n        model_inputs,\n        max_new_tokens = 15,\n        do_sample = True,\n    )\n\n    # Decode generated text\n    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    # Remove the system message\n    if system_role in generated_text:\n        generated_text = generated_text.split(system_role)[-1].strip()\n\n    # Remove the user message from the output to get only the assistant's response\n    if question in generated_text:\n        generated_text = generated_text.split(question)[-1].strip()\n\n    # Clear model from RAM\n    del model\n    torch.cuda.empty_cache()\n    \n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:21.511629Z","iopub.execute_input":"2024-09-05T19:01:21.512309Z","iopub.status.idle":"2024-09-05T19:01:21.526194Z","shell.execute_reply.started":"2024-09-05T19:01:21.512257Z","shell.execute_reply":"2024-09-05T19:01:21.524843Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"* NER cleanup","metadata":{}},{"cell_type":"code","source":"import re\n\ndef ner_cleanup(results):\n    if ';' not in results: \n        return ''\n    \n    results = results.replace(\"[/INST]\", \"\").strip()\n    # Remove any leading or trailing whitespace\n    results = results.strip()\n    # Search for the pattern in the text\n    match = re.search(r'(.*?);', results, re.DOTALL)\n    entities = results\n    # If a match is found, return the matched text\n    if match:\n        entities = match.group(1).strip()\n\n    return entities","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:21.528111Z","iopub.execute_input":"2024-09-05T19:01:21.528571Z","iopub.status.idle":"2024-09-05T19:01:21.552572Z","shell.execute_reply.started":"2024-09-05T19:01:21.528505Z","shell.execute_reply":"2024-09-05T19:01:21.551197Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"* Function to retrieve URIs of a certain toponym.","metadata":{}},{"cell_type":"code","source":"import requests\nimport pandas as pd\nfrom io import StringIO\n\ndef graphdb_send_request(entities, endpoint_url=\"http://88.197.53.158:7200/repositories/da4dte_final\", accept_format='application/sparql-results+json'):\n    \"\"\"\n    Sends a SPARQL query to a GraphDB endpoint.\n\n    :param query: SPARQL query to be sent\n    :param endpoint_url: URL of the GraphDB SPARQL endpoint\n    :param accept_format: Desired response format (default is JSON)\n    :return: Response from the endpoint\n    \"\"\"\n    query = f\"\"\"SELECT ?s ?name (COUNT(?related) AS ?count) WHERE {{\n  {{\n    SELECT ?s ?name WHERE {{\n      {{\n        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSI_Name> ?name .\n      }} UNION {{\n        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSM_Name> ?name .\n      }} UNION {{\n        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGADM_Name> ?name .\n      }} UNION {{\n        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOS_Name> ?name .\n      }} UNION {{\n        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGAG_Name> ?name .\n      }} UNION {{\n        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSNI_Name> ?name .\n      }}\n      FILTER(CONTAINS(LCASE(?name), LCASE(\"{entities}\")))\n    }}\n  }}\n  {{\n    {{ ?s ?p ?related }} UNION {{ ?related ?p ?s }}\n  }}\n}}\nGROUP BY ?s ?name\nORDER BY DESC(?count)\"\"\"\n#     query = f\"\"\"SELECT * WHERE {{\n#   {{\n#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSI_Name> ?name .\n#   }} UNION {{\n#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSM_Name> ?name .\n#   }} UNION {{\n#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGADM_Name> ?name .\n#   }} UNION {{\n#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOS_Name> ?name .\n#   }} UNION {{\n#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGAG_Name> ?name .\n#   }} UNION {{\n#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSNI_Name> ?name .\n#   }}\n#   FILTER(CONTAINS(LCASE(?name), LCASE(\"{entities}\")))\n# }}\"\"\"\n    \n    headers = {\n        'Accept': accept_format,\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n\n    data = {\n        'query': query\n    }\n    \n    try:\n        response = requests.post(endpoint_url, headers=headers, data=data, auth=requests.auth.HTTPBasicAuth('admin', 'p@sx@'))\n\n        if response.status_code == 200:\n            if accept_format == 'application/sparql-results+json':\n#                 print(response.json())\n                json_response = response.json()\n                return convert_json_to_csv(json_response)\n            else:\n#                 print(response.text)\n                return response.text\n        else:\n            response.raise_for_status()\n    except requests.exceptions.HTTPError as err:\n        print(\"HTTP error (most likely invalid query)\")\n        #print(query)\n        #print(err)\n    except Exception as err:\n        print(err)\n        print(\"Endpoint error ENDPOINT DOWN\")\n\ndef convert_json_to_csv(json_data):\n    \"\"\"\n    Converts JSON data to CSV format.\n\n    :param json_data: JSON data to be converted\n    :return: CSV formatted data as a string\n    \"\"\"\n    if 'boolean' in json_data:\n        # Handling boolean result\n        headers = ['value']\n        rows = [[json_data['boolean']]]\n    else:\n        # Extracting header and rows from JSON response\n        headers = json_data['head']['vars']\n        rows = [{var: result.get(var, {}).get('value', '') for var in headers} for result in json_data['results']['bindings']]\n    \n    # Creating DataFrame and converting to CSV\n    df = pd.DataFrame(rows, columns=headers)\n    csv_output = StringIO()\n    df.to_csv(csv_output, index=False)\n    \n    return csv_output.getvalue()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:21.554569Z","iopub.execute_input":"2024-09-05T19:01:21.555820Z","iopub.status.idle":"2024-09-05T19:01:22.736887Z","shell.execute_reply.started":"2024-09-05T19:01:21.555766Z","shell.execute_reply":"2024-09-05T19:01:22.735456Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"* ner system role","metadata":{}},{"cell_type":"code","source":"ner_system_role = \"\"\"You are a specialized Named Entity Recognition (NER) system focused on identifying and extracting toponyms (place names) from the given text. Your task is to recognize and list all geographical entities, including but not limited to:\n\n- Countries\n- Cities\n- States/Provinces\n- Regions\n- Mountains\n- Rivers\n- Oceans/Seas\n- Lakes\n- Islands\n- Continents\n\nFor each input, provide a list of extracted toponyms, separated by commas. If no toponyms are found, respond with \"No toponyms found;\" After completing the analysis, end your response with a semicolon.\n\nExamples:\nQ: Where is Swansea located?\nA: Swansea;\n\nQ: Which Greek regions have between 500000 and 1000000 inhabitants?\nA: Greece;\n\nQ: Is Doolin to the south of Dublin?\nA: Doolin, Dublin;\n\nQ: What's the capital of France and how far is it from the Mediterranean Sea?\nA: France, Mediterranean Sea;\n\nQ: What is the biggest island in the world?\nA: No toponyms found;\n\nNow, analyze the following text. Remember to split the toponyms by commas so I can recognize them individualy.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:22.738805Z","iopub.execute_input":"2024-09-05T19:01:22.739482Z","iopub.status.idle":"2024-09-05T19:01:22.746376Z","shell.execute_reply.started":"2024-09-05T19:01:22.739449Z","shell.execute_reply":"2024-09-05T19:01:22.745059Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"* prompt to select best uri candidate.","metadata":{}},{"cell_type":"code","source":"def select_uri_system_role(question):\n    uri_select_system_role = f\"\"\"You perform disambiguation, this means selecting the most relevant URI for a toponym or entity. You will be supplied with a question and a list of URIs. You have to select one URI out of the provided. \n    The list might have multiple toponyms but the supplied URIs will be relevant to only one. Strictly respond with the URI id and no other information or explanation.\n    \n    For example:\n    Q: Consider this question: Is New York south of Florida?\n    Choose the best URI from the provided list: \n    1. yago:New_York, New York City\n    2. yago:New_York_Municipality, Municipality of New York\n    3. geof:York_Museum, Historical Museum of York\n    \n    A: 1\n    \n    Now consider this question: {question}\n    And choose the best URI from the provided list:\"\"\"\n    return uri_select_system_role","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:22.747969Z","iopub.execute_input":"2024-09-05T19:01:22.748313Z","iopub.status.idle":"2024-09-05T19:01:22.763663Z","shell.execute_reply.started":"2024-09-05T19:01:22.748284Z","shell.execute_reply":"2024-09-05T19:01:22.762230Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def select_uri_system_role(question):\n    uri_select_system_role = f\"\"\"You perform disambiguation, this means selecting the most relevant URI for a toponym or entity. You will be supplied with a question and a list of URIs. You have to select one URI out of the provided. \n    The list might have multiple toponyms but the supplied URIs will be relevant to only one. Strictly respond with the URI id and no other information or explanation.\n    \n    For example:\n    Q: Consider this question: Is New York south of Florida?\n    Choose the best URI from the provided list: \n    1. yago:New_York\n    2. yago:New_York_Municipality\n    3. geof:York_Museum\n    \n    A: 1\n    \n    Now consider this question: {question}\n    And choose the best URI from the provided list:\"\"\"\n    return uri_select_system_role","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:22.768398Z","iopub.execute_input":"2024-09-05T19:01:22.769012Z","iopub.status.idle":"2024-09-05T19:01:22.776359Z","shell.execute_reply.started":"2024-09-05T19:01:22.768978Z","shell.execute_reply":"2024-09-05T19:01:22.775260Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# This prefix map will be used to shrink the uri's down to the prefix level, to help the model better understand them and decrease mistakes.\nprefix_map = {\"http://www.opengis.net/ont/geosparql#\" : \"geo:\",\n               \"http://www.opengis.net/def/function/geosparql/\" : \"geof:\",\n               \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" : \"rdf:\",\n               \"http://www.w3.org/2000/01/rdf-schema#\" : \"rdfs:\",\n               \"http://www.w3.org/2001/XMLSchema#\" : \"xsd:\",\n               \"http://yago-knowledge.org/resource/\" : \"yago:\",\n               \"http://kr.di.uoa.gr/yago2geo/resource/\" : \"y2geor:\",\n               \"http://kr.di.uoa.gr/yago2geo/ontology/\" : \"y2geoo:\",\n               \"http://strdf.di.uoa.gr/ontology#\" : \"strdf:\",\n               \"http://www.opengis.net/def/uom/OGC/1.0/\" : \"uom:\",\n               \"http://www.w3.org/2002/07/owl#\" : \"owl:\"}","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:22.778147Z","iopub.execute_input":"2024-09-05T19:01:22.778649Z","iopub.status.idle":"2024-09-05T19:01:22.794702Z","shell.execute_reply.started":"2024-09-05T19:01:22.778611Z","shell.execute_reply":"2024-09-05T19:01:22.793278Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"* Concept Identifier.","metadata":{}},{"cell_type":"code","source":"!pip install stanza\n!pip install rapidfuzz","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:01:22.796387Z","iopub.execute_input":"2024-09-05T19:01:22.796817Z","iopub.status.idle":"2024-09-05T19:02:01.617105Z","shell.execute_reply.started":"2024-09-05T19:01:22.796785Z","shell.execute_reply":"2024-09-05T19:02:01.615591Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting stanza\n  Downloading stanza-1.8.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.12.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.26.4)\nRequirement already satisfied: protobuf>=3.15.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.32.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from stanza) (3.2.1)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from stanza) (0.10.2)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.1.2+cpu)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.13.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\nDownloading stanza-1.8.2-py3-none-any.whl (990 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: stanza\nSuccessfully installed stanza-1.8.2\nCollecting rapidfuzz\n  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz\nSuccessfully installed rapidfuzz-3.9.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import stanza\nimport nltk\nimport csv\nfrom nltk.util import ngrams\nfrom rapidfuzz.distance import Levenshtein\nfrom rapidfuzz import fuzz\n\n# Ensure necessary packages are available\nnltk.download('punkt')\n\n# Initialize the Stanford CoreNLP Pipeline\nstanza.download('en')\nnlps = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n\ndef read_file(filepath):\n    with open(filepath, 'r') as file:\n        reader = csv.reader(file)\n        data = {row[0].lower(): row[1] for row in reader}  # Store labels and corresponding URIs\n    return data\n\ndef compute_similarity(ngram, label):\n    ngram_str = ' '.join(ngram)\n    if ' ' in ngram_str:\n        # It's a bigram (or higher order n-gram) if it contains a space\n        #print(f\"jaro {fuzz.WRatio(ngram_str, label) / 100}\")\n        return fuzz.WRatio(ngram_str, label) / 100\n    else:\n        # It's a unigram\n        #print(f\"Lev: {ngram_str},{label} : {1 - (Levenshtein.distance(ngram_str, label) / max(len(ngram_str), len(label)))}\")\n        return 1 - (Levenshtein.distance(ngram_str, label) / max(len(ngram_str), len(label)))\n\ndef Concept_Identifier(question):\n    # Read files and prepare data\n    file1_data = read_file('/kaggle/input/yagoclasses1/YAGO2geoClasses.txt')\n    #file2_data = read_file('/kaggle/input/yagoclasses1/YAGOClasses.txt')\n    \n    uris = []\n    \n    # Process the question using the NLP pipeline\n    doc = nlps(question)\n    \n    for sentence in doc.sentences:\n        i = 0\n        while i < len(sentence.words):\n            word = sentence.words[i]\n            ngrams_to_check = []\n            \n            # Check for specific POS tags to form n-grams\n            if word.xpos in {\"NN\", \"NNS\", \"NNP\", \"NNPS\"}:\n                current_ngram = [word.lemma.lower()]\n                i += 1\n                \n                # Continue adding to the n-gram if subsequent words have the same relevant POS tags\n                while i < len(sentence.words) and sentence.words[i].xpos in {\"NN\", \"NNS\", \"NNP\", \"NNPS\"}:\n                    current_ngram.append(sentence.words[i].lemma.lower())\n                    i += 1\n                \n                # Add the formed n-gram to the list\n                ngrams_to_check.append(' '.join(current_ngram))\n                \n                max_similarity = 0\n                threshold = 0.7\n                best_uri = None\n\n                # Compare against all labels in file1 and file2\n                for label, uri in {**file1_data}.items():\n                    for ngram in ngrams_to_check:\n                        similarity = compute_similarity([ngram], label)\n                        if similarity > max_similarity and similarity > threshold:\n                            max_similarity = similarity\n                            best_uri = uri\n                \n                if best_uri:\n                    f_uri = best_uri.replace(\" \", \"\")\n                    uris.append(f_uri)\n            else:\n                i += 1  # Move to the next word if it doesn't match the POS tags\n    \n    return list(set(uris))\n\n# Example usage\nquestion = \"Which bays intersect with county councils that border with County Mayo?\"\nuris = Concept_Identifier(question)\nprint(uris)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:04:34.112454Z","iopub.execute_input":"2024-09-05T19:04:34.113597Z","iopub.status.idle":"2024-09-05T19:04:48.305138Z","shell.execute_reply.started":"2024-09-05T19:04:34.113549Z","shell.execute_reply":"2024-09-05T19:04:48.301636Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4a9df147894e38803932dbfd495783"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0c89d345e24bc897bc8c67e5a277ea"}},"metadata":{}},{"name":"stdout","text":"['y2geoo:OSM_bay', 'y2geoo:OS_County', 'y2geoo:OSI_County_Council']\n","output_type":"stream"}]},{"cell_type":"code","source":"# import spacy\n# import csv\n# from rapidfuzz.distance import Levenshtein\n# import jellyfish\n\n# # Load spaCy English model for lemmatization\n# nlp = spacy.load('en_core_web_sm')\n\n# def read_file(filepath):\n#     with open(filepath, 'r') as file:\n#         reader = csv.reader(file)\n#         data = {row[0].lower(): row[1] for row in reader}  # Store labels and corresponding URIs\n#     return data\n\n# def compute_similarity(ngram, label):\n#     ngram_str = ' '.join(ngram)\n#     if ' ' in ngram_str:\n#         # It's a multi-word n-gram, use Jaro-Winkler similarity\n#         return jellyfish.jaro_winkler_similarity(ngram_str, label)\n#     else:\n#         # It's a single-word n-gram, use Levenshtein distance\n#         return 1 - (Levenshtein.distance(ngram_str, label) / max(len(ngram_str), len(label)))\n\n# def Concept_Identifier(question):\n#     # Read files and prepare data\n#     file1_data = read_file('/kaggle/input/yagoclasses1/YAGO2geoClasses.txt')\n#     #file2_data = read_file('/kaggle/input/yagoclasses1/YAGOClasses.txt')\n    \n#     uris = []\n    \n#     # Process the question using the spaCy pipeline\n#     doc = nlp(question)\n    \n#     for sentence in doc.sents:\n#         i = 0\n#         tokens = list(sentence)\n#         while i < len(tokens):\n#             word = tokens[i]\n#             ngrams_to_check = []\n            \n#             # Check for specific POS tags to form n-grams (Nouns/Proper Nouns)\n#             if word.pos_ in {\"NOUN\", \"PROPN\"}:\n#                 current_ngram = [word.lemma_.lower()]\n#                 i += 1\n                \n#                 # Continue adding to the n-gram if subsequent words have the same relevant POS tags\n#                 while i < len(tokens) and tokens[i].pos_ in {\"NOUN\", \"PROPN\"}:\n#                     current_ngram.append(tokens[i].lemma_.lower())\n#                     i += 1\n                \n#                 # Add the formed n-gram to the list\n#                 ngrams_to_check.append(' '.join(current_ngram))\n                \n#                 max_similarity = 0\n#                 threshold = 0.7\n#                 best_uri = None\n\n#                 # Compare against all labels in file1_data\n#                 for label, uri in {**file1_data}.items():\n#                     for ngram in ngrams_to_check:\n#                         similarity = compute_similarity([ngram], label)\n#                         if similarity > max_similarity and similarity > threshold:\n#                             max_similarity = similarity\n#                             best_uri = uri\n                \n#                 if best_uri:\n#                     f_uri = best_uri.replace(\" \", \"\")\n#                     uris.append(f_uri)\n#             else:\n#                 i += 1  # Move to the next word if it doesn't match the POS tags\n    \n#     return list(set(uris))\n\n# # Example usage\n# question = \"Which bays intersect with county councils that border with County Mayo?\"\n# uris = Concept_Identifier(question)\n# print(uris)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:02:17.758949Z","iopub.execute_input":"2024-09-05T19:02:17.759728Z","iopub.status.idle":"2024-09-05T19:02:17.770236Z","shell.execute_reply.started":"2024-09-05T19:02:17.759693Z","shell.execute_reply":"2024-09-05T19:02:17.768753Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!pip install -q jellyfish Levenshtein","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:02:17.772305Z","iopub.execute_input":"2024-09-05T19:02:17.773530Z","iopub.status.idle":"2024-09-05T19:02:35.911321Z","shell.execute_reply.started":"2024-09-05T19:02:17.773460Z","shell.execute_reply":"2024-09-05T19:02:35.909463Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!pip install -q spacy\n!python -m spacy download en_core_web_sm","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-05T19:02:35.913754Z","iopub.execute_input":"2024-09-05T19:02:35.914223Z","iopub.status.idle":"2024-09-05T19:03:18.423480Z","shell.execute_reply.started":"2024-09-05T19:02:35.914181Z","shell.execute_reply":"2024-09-05T19:03:18.421834Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport spacy\nfrom jellyfish import jaro_winkler_similarity\nimport Levenshtein as lev\n\n# Load spaCy English model\nnlp = spacy.load('en_core_web_sm')\n\ndef read_file(file_path):\n    with open(file_path, 'r') as file:\n        return file.readlines()\n\ndef levenshtein_similarity(str1, str2):\n    distance = lev.distance(str1, str2)\n    max_len = max(len(str1), len(str2))\n    similarity = 1 - (distance / max_len)  # Normalize the distance to get a similarity measure\n    return similarity\n\ndef lemmatize_word(word):\n    doc = nlp(word)\n    return doc[0].lemma_\n\ndef Plain_Concept_Identifier(question, threshold=0.99):\n    file1_data = read_file('/kaggle/input/yagoclasses1/YAGO2geoClasses.txt')\n    #file2_data = read_file('/kaggle/input/yagoclasses1/YAGOClasses.txt')\n    labels = file1_data \n    \n    words = question.lower().split()\n    words = [lemmatize_word(word) for word in words]  # Lemmatize each word\n    num_words = len(words)\n\n    uris = []\n    \n    for label in labels:\n        label_text = label.strip().split(',')[0].lower()  # Lowercase label\n        uri = label.strip().split(',')[1]\n        label_words = label_text.split()\n        label_length = len(label_words)\n\n        # Check similarity for single-word labels using Levenshtein distance\n        if label_length == 1:\n            for word in words:\n                similarity = levenshtein_similarity(word, label_text)\n                if similarity > threshold:\n                    uris.append(uri)\n#                     print(f\"{word} matched with {label_text}\")\n#                     print(uri)\n\n        # Check similarity for multi-word labels using Jaro-Winkler similarity\n        else:\n            for i in range(num_words - label_length + 1):\n                word_sequence = \" \".join(words[i:i + label_length])\n                similarity = jaro_winkler_similarity(word_sequence, label_text)\n                if similarity > threshold:\n                    uris.append(uri)\n#                     print(f\"{word_sequence} matched with {label_text}\")\n#                     print(uri)\n    return list(set(uris))\n\nquestion = \"Which bays intersect with county councils that border with County Mayo?\"\n\nprint(Plain_Concept_Identifier(question))","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:03:18.426167Z","iopub.execute_input":"2024-09-05T19:03:18.426773Z","iopub.status.idle":"2024-09-05T19:03:21.372360Z","shell.execute_reply.started":"2024-09-05T19:03:18.426716Z","shell.execute_reply":"2024-09-05T19:03:21.370719Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['y2geoo:OSM_bay', 'y2geoo:OSI_County_Council', 'y2geoo:OS_County']\n","output_type":"stream"}]},{"cell_type":"code","source":"def read_file_as_string(filepath):\n    with open(filepath, 'r') as file:\n        return file.read()\n\n# Reading the two files and saving their contents as strings\nfile1_data = read_file_as_string('/kaggle/input/yagoclasses1/YAGO2geoClasses.txt')\nfile2_data = read_file_as_string('/kaggle/input/yagoclasses1/YAGOClasses.txt')\n\nconcept_identifier_role = f\"\"\"You are an expect concept identifier. You are given a knowledge base of URIs that represent various concepts.\nEach URI is associated by a descriptive label. The format is \"label,URI\". Your job is to identify concepts within the user-supplied questions and return only the URIs that correspond to them.\nIf a specific concept is not mentioned you do not report it, even if it is semantically relevant.\nYour answers include only the relevant URIs seperated by commas. If no URI is present you do not answer anything. You do not provide any explanations.\n\nYour knowledge base is the following: {file1_data}\n\nFor example: \nQ: \"Which bays intersect with county councils that border with County Mayo?\"\nA: \"y2geoo:OSI_City_and_County_Council,y2geoo:OSM_bay\"\nQ: \"Is Doolin to the south of Dublin?\"\nA: \"\"\nQ: \"Which forests are within baronies in the Republic of Ireland?\"\nA: \"y2geoo:OSM_forest,y2geoo:OSI_Barony\"\n\nNow based on your knowledge base answer the user question.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:03:21.374180Z","iopub.execute_input":"2024-09-05T19:03:21.375084Z","iopub.status.idle":"2024-09-05T19:03:21.390065Z","shell.execute_reply.started":"2024-09-05T19:03:21.375045Z","shell.execute_reply":"2024-09-05T19:03:21.388616Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def LLM_Concept_Identifier(question):\n    result = run_chat_inference(model, tokenizer, concept_identifier_role, question, max_tokens=20)\n    result = result.replace(\"[/INST]\", \"\").strip()\n    result = result.replace(\"\\\\\", \"\").strip()\n    uri_pattern = r'\\b\\w+:[\\w_]+\\b'\n    \n    # Find all URIs that match the pattern\n    uris = re.findall(uri_pattern, result)\n    \n    #print(uris)\n    return uris\n    \nLLM_Concept_Identifier(\"Which forests are within baronies in the Republic of Ireland?\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:03:21.391988Z","iopub.execute_input":"2024-09-05T19:03:21.392536Z","iopub.status.idle":"2024-09-05T19:03:24.281524Z","shell.execute_reply.started":"2024-09-05T19:03:21.392464Z","shell.execute_reply":"2024-09-05T19:03:24.279223Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#print(uris)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uris\n\u001b[0;32m---> 13\u001b[0m \u001b[43mLLM_Concept_Identifier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhich forests are within baronies in the Republic of Ireland?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[19], line 2\u001b[0m, in \u001b[0;36mLLM_Concept_Identifier\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLLM_Concept_Identifier\u001b[39m(question):\n\u001b[0;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_chat_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcept_identifier_role\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      4\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n","Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mrun_chat_inference\u001b[0;34m(model, tokenizer, system_role, question, max_tokens)\u001b[0m\n\u001b[1;32m     12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Decode generated text\u001b[39;00m\n\u001b[1;32m     23\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1200\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1197\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1200\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:976\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    965\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    966\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    967\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         cache_position,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:718\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:613\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    602\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    603\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    611\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 613\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    615\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: \"addmm_impl_cpu_\" not implemented for 'Half'"],"ename":"RuntimeError","evalue":"\"addmm_impl_cpu_\" not implemented for 'Half'","output_type":"error"}]},{"cell_type":"markdown","source":"* GeoQA Instance Identifier system:","metadata":{}},{"cell_type":"code","source":"import requests, json\n\nMY_GCUBE_TOKEN = 'ea06d074-c9ba-4ff8-a3a7-a902aef9bf98-843339462'\n\nclass WATAnnotation:\n    # An entity annotated by WAT\n\n    def __init__(self, d):\n\n        # char offset (included)\n        self.start = d['start']\n        # char offset (not included)\n        self.end = d['end']\n\n        # annotation accuracy\n        self.rho = d['rho']\n        # spot-entity probability\n        self.prior_prob = d['explanation']['prior_explanation']['entity_mention_probability']\n\n        # annotated text\n        self.spot = d['spot']\n\n        # Wikpedia entity info\n        self.wiki_id = d['id']\n        self.wiki_title = d['title']\n\n\n    def json_dict(self):\n        # Simple dictionary representation\n        return {'wiki_title': self.wiki_title,\n                'wiki_id': self.wiki_id,\n                'start': self.start,\n                'end': self.end,\n                'rho': self.rho,\n                'prior_prob': self.prior_prob\n                }\n    \ndef wat_entity_linking(text):\n    # Main method, text annotation with WAT entity linking system\n    wat_url = 'https://wat.d4science.org/wat/tag/tag'\n    payload = [(\"gcube-token\", MY_GCUBE_TOKEN),\n               (\"text\", text),\n               (\"lang\", 'en'),\n               (\"tokenizer\", \"nlp4j\"),\n               ('debug', 9),\n               (\"method\",\n                \"spotter:includeUserHint=true:includeNamedEntity=true:includeNounPhrase=true,prior:k=50,filter-valid,centroid:rescore=true,topk:k=5,voting:relatedness=lm,ranker:model=0046.model,confidence:model=pruner-wiki.linear\")]\n\n    response = requests.get(wat_url, params=payload)\n    return [WATAnnotation(a) for a in response.json()['annotations']]","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:04:48.312834Z","iopub.execute_input":"2024-09-05T19:04:48.314071Z","iopub.status.idle":"2024-09-05T19:04:48.349690Z","shell.execute_reply.started":"2024-09-05T19:04:48.313961Z","shell.execute_reply":"2024-09-05T19:04:48.346407Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import requests\n\ndef test_uri(uri, endpoint_url=\"http://88.197.53.158:7200/repositories/da4dte_final\", accept_format='application/sparql-results+json'):\n    \"\"\"\n    Sends a SPARQL ASK query to a GraphDB endpoint to check if a URI exists.\n\n    :param uri: URI to be checked\n    :param endpoint_url: URL of the GraphDB SPARQL endpoint\n    :param accept_format: Desired response format (default is JSON)\n    :return: Boolean indicating if the URI exists\n    \"\"\"\n    query = f\"\"\"ASK WHERE {{\n  <{uri}> ?p ?o .\n}}\"\"\"\n    headers = {\n        'Accept': accept_format,\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n\n    data = {\n        'query': query\n    }\n    \n    try:\n        response = requests.post(endpoint_url, headers=headers, data=data, auth=requests.auth.HTTPBasicAuth('admin', 'p@sx@'))\n\n        if response.status_code == 200:\n            if accept_format == 'application/sparql-results+json':\n                json_response = response.json()\n                return json_response.get('boolean', False)  # Return True if URI exists, otherwise False\n            else:\n                return False  # If not using JSON format, default to False (or handle differently if necessary)\n        else:\n            response.raise_for_status()\n    except requests.exceptions.HTTPError as err:\n        print(\"HTTP error (most likely invalid query):\", err)\n        return False\n    except Exception as err:\n        print(\"Endpoint error or endpoint is down:\", err)\n        return False\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:04:48.354773Z","iopub.execute_input":"2024-09-05T19:04:48.356192Z","iopub.status.idle":"2024-09-05T19:04:48.382368Z","shell.execute_reply.started":"2024-09-05T19:04:48.356073Z","shell.execute_reply":"2024-09-05T19:04:48.379832Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def GeoQAInstanceIdentifier(question):\n    # Process the question using the NLP pipeline\n    doc = nlp(question)\n    uris = []\n    \n    for sentence in doc.sentences:\n        for word in sentence.words:\n            # Display the word, lemma, POS tag, and dependency information in CoNLL-U format\n            #print(f\"{word.id}\\t{word.text}\\t{word.lemma}\\t{word.upos}\\t{word.xpos}\\t_\\t{word.head}\\t{word.deprel}\\t_\\t_\")\n            \n            #print (word.upos)\n            # Check for specific POS tags and map to the geospatial relations\n            if word.xpos in {\"NN\", \"NNS\", \"NNP\", \"NNPS\"}:\n                print(word.text)\n                #lemma = word.lemma.lower()\n                ann = wat_entity_linking(word.text)\n                for result in ann: \n                    title = result.wiki_title\n\n                    yago_link = \"http://yago-knowledge.org/resource/\"\n                    target_uri = yago_link + title\n                    full_uri = target_uri\n                    \n                    # Shorten the uris down to prefixes.\n                    for uri_map, prefix in prefix_map.items():\n                        target_uri = target_uri.replace(uri_map, prefix)\n\n                    target_uri = target_uri.replace(\"&amp;\", \"&\")\n                    target_uri = target_uri.replace(\" \", \"_\")\n                    \n                    exists = test_uri(full_uri)\n                    \n                    if exists == True:\n                        uris.append(target_uri)\n#                     else:\n#                         ##### SEARCH IN KG ##### This performed worse.\n#                         recognized_uris = graphdb_send_request(title)\n#                         # Use StringIO to treat the CSV string as a file\n#                         csv_file = StringIO(recognized_uris)\n#                         csv_reader = csv.reader(csv_file)\n#                         # Skip the first row (headers)\n#                         next(csv_reader)\n\n#                         candidate_uris = \"\"\n#                         relevant_uris = []\n#                         for row in csv_reader:\n#                             uri = row[0]\n#                             name = row[1]\n#                             count = row[2]\n#                             # Shorten the uris down to prefixes.\n#                             for uri_map, prefix in prefix_map.items():\n#                                 uri = uri.replace(uri_map, prefix)\n\n#                             relevant_uris.append((uri, name, count))\n#                             final_uri = f\"Uri: {uri}, Name: {name}, Count: {count}\"\n#                             candidate_uris += final_uri\n\n#                         # Popular uri choice.\n#                         if relevant_uris != []:\n#                             uris.append(relevant_uris[0][0])\n                  \n    return uris","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:04:50.734706Z","iopub.execute_input":"2024-09-05T19:04:50.735228Z","iopub.status.idle":"2024-09-05T19:04:50.748800Z","shell.execute_reply.started":"2024-09-05T19:04:50.735189Z","shell.execute_reply":"2024-09-05T19:04:50.747032Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"* LLM-powered NER Pipeline function","metadata":{}},{"cell_type":"code","source":"import re\n\ndef extract_first_integer(s):\n    match = re.search(r'\\d+', s)\n    if match:\n        return int(match.group(0))\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:03:24.292180Z","iopub.status.idle":"2024-09-05T19:03:24.292935Z","shell.execute_reply.started":"2024-09-05T19:03:24.292584Z","shell.execute_reply":"2024-09-05T19:03:24.292613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nfrom io import StringIO\n\ndef retrieve_uris(question):\n    # Extract toponyms from the questions using NER.\n    result = run_chat_inference(model, tokenizer, ner_system_role, question)\n    result = ner_cleanup(result)\n    # Split the toponym string to a list of toponyms.\n    if result == '':\n        toponyms = []\n    else:\n        toponyms = result.split(',')\n    \n    uris = []\n    # Disambigation with WAT\n    for toponym in toponyms: \n        ann = wat_entity_linking(toponym)\n        for result in ann: \n            title = result.wiki_title\n\n            yago_link = \"http://yago-knowledge.org/resource/\"\n            target_uri = yago_link + title\n            full_uri = target_uri\n\n            # Shorten the uris down to prefixes.\n            for uri_map, prefix in prefix_map.items():\n                target_uri = target_uri.replace(uri_map, prefix)\n\n            target_uri = target_uri.replace(\"&amp;\", \"&\")\n            target_uri = target_uri.replace(\" \", \"_\")\n\n            exists = test_uri(full_uri)\n\n            if exists == True:\n                uris.append(target_uri)\n            else:\n                ##### SEARCH IN KG #####\n                recognized_uris = graphdb_send_request(title)\n                # Use StringIO to treat the CSV string as a file\n                csv_file = StringIO(recognized_uris)\n                csv_reader = csv.reader(csv_file)\n                # Skip the first row (headers)\n                next(csv_reader)\n\n                candidate_uris = \"\"\n                relevant_uris = []\n                for row in csv_reader:\n                    uri = row[0]\n                    name = row[1]\n                    count = row[2]\n                    # Shorten the uris down to prefixes.\n                    for uri_map, prefix in prefix_map.items():\n                        uri = uri.replace(uri_map, prefix)\n\n                    relevant_uris.append((uri, name, count))\n                    final_uri = f\"Uri: {uri}, Name: {name}, Count: {count}\"\n                    candidate_uris += final_uri\n\n                # Popular uri choice.\n                if relevant_uris != []:\n                    uris.append(relevant_uris[0][0])\n    return uris\n        \n    # List of the uris that will be used.\n    uris = []\n    for toponym in toponyms:\n        recognized_uris = graphdb_send_request(toponym)\n        #print(recognized_uris)\n\n        # Use StringIO to treat the CSV string as a file\n        csv_file = StringIO(recognized_uris)\n        csv_reader = csv.reader(csv_file)\n        # Skip the first row (headers)\n        next(csv_reader)\n        \n        candidate_uris = \"\"\n        relevant_uris = []\n        for row in csv_reader:\n            uri = row[0]\n            name = row[1]\n            count = row[2]\n            # Shorten the uris down to prefixes.\n            for uri_map, prefix in prefix_map.items():\n                uri = uri.replace(uri_map, prefix)\n            \n            relevant_uris.append((uri, name, count))\n            final_uri = f\"Uri: {uri}, Name: {name}, Count: {count}\"\n            candidate_uris += final_uri\n    \n        if relevant_uris == []:\n            return []\n        \n        # Limit the model to select from the 3 most frequently used relevant uris (usually the first is the target).\n        supplied_uris = \"\"\n        limit = min(3, len(relevant_uris))\n        for i in range (0, limit):\n            #supplied_uris += f\"{i}. {relevant_uris[i][0], relevant_uris[i][1]}\"\n            # THIS APPROACH DOES NOT INCLUDE THE NAME OF EACH URI SUPPLIED TO THE MODEL #\n            # This works better, not because the model is confused by the amount of info\n            # but rather because the entities of the KG have sometimes odd and misleading names.\n            supplied_uris += f\"{i}. {relevant_uris[i][0]}\"\n            \n        print(supplied_uris)\n        # Prompt model again, but this time to choose the correct uri to be used.\n        uri_select_system_role = select_uri_system_role(question)\n        \n        result = run_chat_inference(model, tokenizer, uri_select_system_role, supplied_uris)\n        #print(f\"hello? {result}\")\n        # Model might return some characters along the number e.g. \"A: 0\". Simply extract the int from the string.\n        target = extract_first_integer(result)\n        \n        # Instead of using the LLM again to select, naively select the most frequently used URI.\n#         uris.append(relevant_uris[0][0])\n        \n        # \"Intelligent\" uri selection powered by LLM disambiguation.\n        if target in range(0, limit):\n            uris.append(relevant_uris[target][0])\n\n    return uris","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:03:24.296189Z","iopub.status.idle":"2024-09-05T19:03:24.296904Z","shell.execute_reply.started":"2024-09-05T19:03:24.296562Z","shell.execute_reply":"2024-09-05T19:03:24.296591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Evaluation and comparisson","metadata":{}},{"cell_type":"code","source":"count = 0 \n\nfor key in original_dataset:\n    uris = original_dataset[key]['URI']\n    for uri in uris:\n        count += 1\n        \nprint (count)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:04:57.225456Z","iopub.execute_input":"2024-09-05T19:04:57.226081Z","iopub.status.idle":"2024-09-05T19:04:57.235309Z","shell.execute_reply.started":"2024-09-05T19:04:57.226032Z","shell.execute_reply":"2024-09-05T19:04:57.233487Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"784\n","output_type":"stream"}]},{"cell_type":"code","source":"scores = []\nfp_scores = []\nw_count = 0\nc_count = 0\n\nw_scores = []\nwfp_scores = []\nww_count = 0\nwc_count = 0\n\nfor key in original_dataset:\n    # Get the dataset question and dataset uris.\n    question = original_dataset[key]['Question']\n    ground_truth_uris = original_dataset[key]['URI']\n    \n    # Generate uris from the question.\n#     generated_uris = retrieve_uris(question)\n#     concept_uris = LLM_Concept_Identifier(question)\n#     if concept_uris != []:\n#         generated_uris.extend(concept_uris)\n    generated_uris = []\n    print(f\"LLM: {generated_uris}\")\n    # Generate uris with the GeoQA method for comparisson.\n#     wat_uris = GeoQAInstanceIdentifier(question)\n    wat_uris = Concept_Identifier(question)\n#     wat_uris = []\n    print(f\"generated: {wat_uris}\")\n    print(f\"gt: {ground_truth_uris}\")\n\n    # Save the results to the dataset.\n    original_dataset[key]['Gen_URI'] = wat_uris\n    \n    # Evaluate the generated results compared to the ground truth uris.\n    correct = 0\n    wrong = 0\n    for uri in ground_truth_uris: \n        if generated_uris: \n            if uri in generated_uris:\n                correct += 1\n            \n    # Count False positives.\n    if generated_uris: \n        for uri in generated_uris: \n            if uri not in ground_truth_uris: \n                wrong += 1\n    \n    if len(ground_truth_uris) != 0:\n        accuracy = correct/len(ground_truth_uris)\n        fp_perc = wrong/len(ground_truth_uris)\n        print(accuracy)\n        scores.append(accuracy)\n        fp_scores.append(fp_perc)\n        \n    ###### SAME FOR WAT. ######\n    w_correct = 0\n    w_wrong = 0\n    for uri in ground_truth_uris: \n        if wat_uris:\n            if uri in wat_uris:\n                w_correct += 1\n            \n    # Count False positives.\n    if wat_uris: \n        for uri in wat_uris: \n            if uri not in ground_truth_uris: \n                w_wrong += 1\n    \n    if len(ground_truth_uris) != 0:\n        accuracy = w_correct/len(ground_truth_uris)\n        fp_perc = w_wrong/len(ground_truth_uris)\n        print(accuracy)\n        w_scores.append(accuracy)\n        wfp_scores.append(fp_perc)\n    \n    c_count += correct\n    w_count += wrong\n    \n    wc_count += w_correct\n    ww_count += w_wrong\n    \n# Print average of scores.\naverage_accuracy = sum(scores) / len(scores) if scores else 0\nfp_perc = sum(fp_scores) / len(fp_scores) if fp_scores else 0\nprint(f\"Average accuracy: {average_accuracy:.2f}. Total corrects: {c_count}\")\nprint(f\"False Positive rate percentage: {fp_perc:.2f}. Total mistakes: {w_count}\")\n\n# Print average of scores for wat.\naverage_accuracy = sum(w_scores) / len(w_scores) if w_scores else 0\nfp_perc = sum(wfp_scores) / len(wfp_scores) if wfp_scores else 0\nprint(f\"GeoQA Average accuracy: {average_accuracy:.2f}. Total corrects: {wc_count}\")\nprint(f\"GeoQA False Positive rate percentage: {fp_perc:.2f}. Total mistakes: {ww_count}\")\n\n# Save the JSON data to a file\nwith open('concepts_dataset.json', 'w') as json_file:\n    json.dump(original_dataset, json_file, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:09:57.407369Z","iopub.execute_input":"2024-09-05T19:09:57.408823Z","iopub.status.idle":"2024-09-05T19:10:16.516144Z","shell.execute_reply.started":"2024-09-05T19:09:57.408778Z","shell.execute_reply":"2024-09-05T19:10:16.514787Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"LLM: []\ngenerated: ['y2geoo:OSM_park']\ngt: ['strdf:area', 'yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_park', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains']\n0.0\n0.125\nLLM: []\ngenerated: ['y2geoo:OS_District']\ngt: ['yago:Staffordshire', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OS_District', 'geo:hasGeometry', 'geo:asWKT', 'strdf:below', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.07692307692307693\nLLM: []\ngenerated: []\ngt: ['yago:Edinburgh', 'geo:hasGeometry', 'geo:asWKT', 'yago:geoentity_City_of_London_2643744', 'geo:hasGeometry', 'geo:asWKT', 'strdf:below']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_lake']\ngt: ['strdf:area', 'strdf:area', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'yago:geoentity_Eastern_Macedonia_and_Thrace_6697803', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.1\nLLM: []\ngenerated: ['y2geoo:OSI_County_Council', 'y2geoo:OSNI_Townland']\ngt: ['yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSI_Townland', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_nature_reserve']\ngt: ['rdf:type', 'y2geoo:OSM_beach', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_nature_reserve', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches']\n0.0\n0.1111111111111111\nLLM: []\ngenerated: []\ngt: ['yago:Swansea', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Kallithea', 'y2geoo:hasGAG_Population']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:GAG_Region']\ngt: ['y2geoo:GAG_Region', 'y2geoo:hasGAG_Population', ' 1000000). FILTER(?pop ']\n0.0\n0.3333333333333333\nLLM: []\ngenerated: ['y2geoo:GAG_Region', 'y2geoo:GAG_Municipality']\ngt: ['yago:geoentity_PerifereiakΓ\\xad_EnΓ³tita_KentrikoΓΊ_TomΓ©a_AthinΓ³n_8200482', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'strdf:contains']\n0.0\n0.14285714285714285\nLLM: []\ngenerated: ['y2geoo:OSM_lake', 'y2geoo:OSM_town']\ngt: ['yago:geoentity_Eastern_Macedonia_and_Thrace_6697803', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_village', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'geof:sfWithin', 'strdf:left']\n0.0\n0.07142857142857142\nLLM: []\ngenerated: ['y2geoo:OSM_lake', 'y2geoo:OSM_forest']\ngt: ['rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches']\n0.0\n0.2222222222222222\nLLM: []\ngenerated: ['y2geoo:OSM_lake', 'y2geoo:OSM_forest']\ngt: ['rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects']\n0.0\n0.2222222222222222\nLLM: []\ngenerated: []\ngt: ['yago:Manchester', 'geo:hasGeometry', 'geo:asWKT', 'yago:Liverpool', 'geo:hasGeometry', 'geo:asWKT', 'geof:distance', 'uom:metre', ' 41000).FILTER(geof:distance(?geoWKT,?geoWKT1,uom:metre) ']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Cardiff', 'geo:hasGeometry', 'geo:asWKT', 'yago:Birmingham', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OS_County']\ngt: ['yago:County_Leitrim', 'yago:hasPopulation', 'rdf:type', 'y2geoo:OSI_County_Council', 'rdf:type', 'y2geoo:OSI_City_and_County_Council', 'yago:hasPopulation', 'xsd:integer', 'xsd:integer']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_lake']\ngt: ['strdf:area', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'yago:England', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.1111111111111111\nLLM: []\ngenerated: []\ngt: ['yago:Doolin', 'geo:hasGeometry', 'geo:asWKT', 'yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'strdf:below']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'yago:Arklow', 'geo:hasGeometry', 'geo:asWKT', 'geof:distance', 'uom:metre']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_bay', 'y2geoo:OSM_park']\ngt: ['yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_park', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_bay', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'strdf:right']\n0.0\n0.15384615384615385\nLLM: []\ngenerated: []\ngt: ['yago:Kent', 'geo:hasGeometry', 'yago:Norfolk', 'geo:hasGeometry', 'geo:asWKT', 'geo:asWKT', 'strdf:left']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Essex', 'geo:hasGeometry', 'geo:asWKT', 'yago:Hertfordshire', 'geo:hasGeometry', 'geo:asWKT', 'strdf:touches']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSI_Barony', 'y2geoo:OSM_forest']\ngt: ['y2geoo:OSI_Barony', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.2857142857142857\nLLM: []\ngenerated: ['y2geoo:OSM_stream', 'y2geoo:OSM_city']\ngt: ['yago:Wales', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_city', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains', 'geof:sfContains', 'strdf:above']\n0.0\n0.14285714285714285\nLLM: []\ngenerated: ['y2geoo:OSM_stream', 'y2geoo:OS_County']\ngt: ['yago:County_Longford', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSI_County_Council', 'rdf:type', 'y2geoo:OSI_City_and_County_Council', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches', 'strdf:crosses']\n0.0\n0.06666666666666667\nLLM: []\ngenerated: []\ngt: ['yago:geoentity_Dimos_Athens_8133876', 'geo:hasGeometry', 'geo:asWKT', 'yago:Manchester', 'geo:hasGeometry', 'geo:asWKT', 'strdf:below']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_nature_reserve', 'y2geoo:OSM_stream']\ngt: ['y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_nature_reserve', 'geo:hasGeometry', 'geo:asWKT', 'yago:Oxford', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'geof:sfWithin', 'strdf:crosses']\n0.0\n0.16666666666666666\nLLM: []\ngenerated: ['y2geoo:OSM_park', 'y2geoo:GAG_Region', 'y2geoo:GAG_Municipality']\ngt: ['yago:geoentity_PerifereiakΓ\\xad_EnΓ³tita_KentrikoΓΊ_TomΓ©a_AthinΓ³n_8200482', 'geo:hasGeometry', 'geo:asWKT', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains', 'y2geoo:hasGAG_Population', ' 50000 )   ?island rdf:type <y2geoo:OSM_park', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['strdf:area', 'yago:geoentity_Andover_Ponds_5095197', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_beach']\ngt: ['y2geoo:OSM_beach', 'geo:hasGeometry', 'geo:asWKT', 'yago:geoentity_Dimos_Athens_8133876', 'geo:hasGeometry', 'geo:asWKT', 'geof:distance', 'uom:metre']\n0.0\n0.125\nLLM: []\ngenerated: []\ngt: ['yago:Texas', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['rdfs:subClassOf', 'yago:wordnet_university_108286163', 'yago:isLocatedIn', 'yago:Athlone']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_city']\ngt: ['yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'yago:Norilsk', 'geo:hasGeometry', 'geo:asWKT', 'strdf:above']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['strdf:area', 'yago:United_Kingdom', 'geo:hasGeometry', 'geo:asWKT', 'strdf:area', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['rdfs:subClassOf', 'yago:wordnet_bridge_102898711', 'yago:isLocatedIn', 'yago:California', 'yago:wasCreatedOnDate']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:geoentity_Kilkenny_10130170', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:England', 'geo:hasGeometry', 'geo:asWKT', 'yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'strdf:left']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OS_County']\ngt: ['yago:Texas', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:GADM_3rdOrder_AdministrativeUnit', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_lake']\ngt: ['yago:Lake_Kerkini', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_village']\ngt: ['rdf:type', 'y2geoo:OSM_village', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_village', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches']\n0.0\n0.2222222222222222\nLLM: []\ngenerated: ['y2geoo:OSM_park', 'y2geoo:OS_County']\ngt: ['y2geoo:OSM_park', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OS_County', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.2857142857142857\nLLM: []\ngenerated: ['y2geoo:OSM_bay', 'y2geoo:OSM_city']\ngt: ['rdf:type', 'y2geoo:OSM_city', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_bay', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.2222222222222222\nLLM: []\ngenerated: ['y2geoo:OSM_lake', 'y2geoo:OSM_locality']\ngt: ['y2geoo:OSM_locality', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches']\n0.0\n0.2857142857142857\nLLM: []\ngenerated: ['y2geoo:OSM_island']\ngt: ['yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_island', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains', 'geof:sfWithin']\n0.0\n0.09090909090909091\nLLM: []\ngenerated: []\ngt: ['yago:geoentity_Chania_8133762', 'geo:hasGeometry', 'yago:geoentity_Dimos_Athens_8133876', 'geo:hasGeometry', 'geo:asWKT', 'geo:asWKT', 'strdf:right']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['strdf:area', 'yago:West_Sussex', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OS_DistrictWard']\ngt: ['strdf:area', 'yago:Plymouth', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OS_DistrictWard', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects', 'strdf:buffer', 'uom:metre', 'strdf:buffer', 'uom:metre']\n0.0\n0.07692307692307693\nLLM: []\ngenerated: ['y2geoo:NBD_State']\ngt: ['strdf:area', 'yago:United_States', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:GADM_2ndOrder_AdministrativeUnit', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_town']\ngt: ['rdfs:subClassOf', 'yago:wordnet_village_108672738', 'yago:isLocatedIn', 'yago:Norfolk', 'yago:hasPopulation', 'xsd:integer', ' 10000) . FILTER (xsd:integer(?pop) ']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_lake']\ngt: ['strdf:area', 'yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'yago:geoentity_Dimos_Thessaloniki_8133841', 'geo:hasGeometry', 'geo:asWKT', 'yago:geoentity_Dimos_Athens_8133876', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains', 'strdf:below', 'strdf:above']\n0.0\n0.0625\nLLM: []\ngenerated: ['y2geoo:NBD_State']\ngt: ['yago:Key_Largo', 'yago:isLocatedIn']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_bay', 'y2geoo:OSM_reservoir']\ngt: ['yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_reservoir', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within', 'strdf:buffer', 'uom:metre', 'strdf:buffer', 'uom:metre', 'rdf:type', 'y2geoo:OSM_bay', 'geo:hasGeometry', 'geo:asWKT', 'strdf:right']\n0.0\n0.11764705882352941\nLLM: []\ngenerated: ['y2geoo:GAG_Region']\ngt: ['rdf:type', 'y2geoo:GAG_Region', 'y2geoo:hasGAG_Population']\n0.0\n0.3333333333333333\nLLM: []\ngenerated: []\ngt: ['yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'yago:Norilsk', 'geo:hasGeometry', 'geo:asWKT', 'strdf:above']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_beach']\ngt: ['yago:Sennen_Cove', 'geo:hasGeometry', 'geo:asWKT', 'yago:Helston', 'geo:hasGeometry', 'geo:asWKT', 'strdf:right']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:NBD_State', 'y2geoo:OSM_forest', 'y2geoo:OSM_village']\ngt: ['rdf:type', 'y2geoo:OSM_village', 'geo:hasGeometry', 'geo:asWKT', 'yago:Illinois', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'strdf:intersects']\n0.0\n0.15384615384615385\nLLM: []\ngenerated: ['y2geoo:GAG_Municipality']\ngt: ['yago:Gravia', 'y2geoo:hasGAG_Population']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:NBD_State']\ngt: ['yago:wikicategory_States_of_the_United_States', 'yago:wikicategory_InterContinental_hotels', 'yago:isLocatedIn']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_bay', 'y2geoo:OSM_village']\ngt: ['rdf:type', 'y2geoo:OSM_bay', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_village', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches']\n0.0\n0.2222222222222222\nLLM: []\ngenerated: ['y2geoo:OSM_park']\ngt: ['yago:Liverpool', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_park', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.125\nLLM: []\ngenerated: ['y2geoo:OSI_Barony', 'y2geoo:OSM_lake']\ngt: ['y2geoo:OSI_Barony', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.2857142857142857\nLLM: []\ngenerated: []\ngt: ['yago:Southampton', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['rdfs:subClassOf', 'yago:wordnet_church_103028079', 'yago:isLocatedIn', 'yago:New_York_City']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Loch_Awe', 'yago:hasLength']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Plymouth', 'geo:hasGeometry', 'geo:asWKT', 'yago:Cardiff', 'geo:hasGeometry', 'geo:asWKT', 'geof:distance', 'uom:metre']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['y2geor:geoentity_City_of_Belfast_3333223', 'geo:hasGeometry', 'geo:asWKT', 'yago:geoentity_City_of_London_2643744', 'geo:hasGeometry', 'geo:asWKT', 'strdf:left']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Sfakia', 'geo:hasGeometry', 'geo:asWKT', 'yago:Scotland', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:New_York_City', 'yago:hasLatitude', 'yago:New_York_City', 'yago:hasLongitude']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OS_County']\ngt: ['strdf:area', 'rdf:type', 'y2geoo:OS_County', 'rdf:type', 'y2geoo:OSI_County_Council', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.14285714285714285\nLLM: []\ngenerated: ['y2geoo:OS_County']\ngt: ['yago:Kansas', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:hasGADM_Description', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains', 'y2geoo:POPULATION', 'xsd:double']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:GAG_Municipality']\ngt: ['y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:hasGAG_Population', 'yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.125\nLLM: []\ngenerated: ['y2geoo:OSM_city']\ngt: ['rdfs:subClassOf', 'yago:wordnet_city_108524735', 'yago:isLocatedIn', 'yago:Missouri', 'yago:hasPopulation', 'xsd:integer']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Dublin', 'yago:hasPopulation']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Oxfordshire', 'geo:hasGeometry', 'geo:asWKT', 'yago:Kent', 'geo:hasGeometry', 'geo:asWKT', 'geof:distance', 'uom:metre']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:NBD_State', 'y2geoo:OSM_city']\ngt: ['rdfs:subClassOf', 'yago:wordnet_administrative_district_108491826', 'yago:isLocatedIn', 'yago:Missouri', 'yago:hasPopulation', 'xsd:integer']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Ullswater', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:GAG_Municipality']\ngt: ['y2geoo:GAG_Municipality', 'y2geoo:hasGAG_Population', 'geo:hasGeometry', 'geo:asWKT', 'yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.125\nLLM: []\ngenerated: []\ngt: ['yago:Drogheda', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_island', 'y2geoo:OSM_beach']\ngt: ['yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_island', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_beach', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within', 'strdf:within']\n0.0\n0.15384615384615385\nLLM: []\ngenerated: ['y2geoo:OSM_stream']\ngt: ['yago:Alabama', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains']\n0.0\n0.14285714285714285\nLLM: []\ngenerated: []\ngt: ['yago:Howth', 'geo:hasGeometry', 'geo:asWKT', 'yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'geof:distance', 'uom:metre']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_nature_reserve', 'y2geoo:OSM_lake']\ngt: ['rdf:type', 'y2geoo:OSM_nature_reserve', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'yago:Republic_of_Ireland', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'geof:sfContains']\n0.0\n0.15384615384615385\nLLM: []\ngenerated: []\ngt: ['yago:Birmingham', 'geo:hasGeometry', 'geo:asWKT', 'yago:Leicester', 'geo:hasGeometry', 'geo:asWKT', 'strdf:left']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_park', 'y2geoo:NBD_State', 'y2geoo:OSM_forest']\ngt: ['rdf:type', 'y2geoo:OSM_park', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'yago:United_States', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'geof:sfWithin']\n0.0\n0.15384615384615385\nLLM: []\ngenerated: ['y2geoo:OSM_forest', 'y2geoo:OS_District']\ngt: ['y2geoo:OS_District', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'yago:Cambridge', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'geof:sfWithin']\n0.0\n0.18181818181818182\nLLM: []\ngenerated: []\ngt: ['yago:Luton', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_village', 'y2geoo:OS_District']\ngt: ['rdf:type', 'y2geoo:OSM_village', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OS_District', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within']\n0.0\n0.2222222222222222\nLLM: []\ngenerated: ['y2geoo:OSM_city']\ngt: ['xsd:integer', 'strdf:area', 'yago:United_Kingdom', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_city', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains', 'xsd:integer', 'yago:hasPopulation']\n0.0\n0.09090909090909091\nLLM: []\ngenerated: ['y2geoo:OSM_lake', 'y2geoo:OSM_city']\ngt: ['yago:England', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_city', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'strdf:touches']\n0.0\n0.15384615384615385\nLLM: []\ngenerated: []\ngt: ['yago:Chichester', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: []\ngt: ['yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'yago:geoentity_City_of_Belfast_3333223', 'geo:hasGeometry', 'geo:asWKT', 'strdf:below']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_park']\ngt: ['strdf:area', 'rdf:type', 'y2geoo:OSM_park', 'geo:hasGeometry', 'geo:asWKT', 'yago:Dublin', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.1111111111111111\nLLM: []\ngenerated: ['y2geoo:OS_District']\ngt: ['yago:geoentity_Stafford_District_7290631', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OS_District', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches']\n0.0\n0.125\nLLM: []\ngenerated: ['y2geoo:OSM_lake']\ngt: ['yago:Central_Macedonia', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.125\nLLM: []\ngenerated: ['y2geoo:GAG_Municipality']\ngt: ['strdf:area', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin']\n0.0\n0.125\nLLM: []\ngenerated: ['y2geoo:GAG_Municipality']\ngt: ['strdf:area', 'yago:geoentity_Dimos_Thessaloniki_8133841', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfTouches', 'strdf:buffer', 'uom:metre', 'strdf:buffer', 'uom:metre']\n0.0\n0.08333333333333333\nLLM: []\ngenerated: ['y2geoo:OSM_lake', 'y2geoo:GAG_Municipality']\ngt: ['yago:Greece', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'strdf:transform', 'http://www.opengis.net/def/crs/EPSG/0/4326', 'strdf:transform', 'http://www.opengis.net/def/crs/EPSG/0/4326', 'geof:sfWithin', 'strdf:transform', 'http://www.opengis.net/def/crs/EPSG/0/4326', 'strdf:transform', 'http://www.opengis.net/def/crs/EPSG/0/4326', 'geof:sfWithin', 'strdf:transform', 'http://www.opengis.net/def/crs/EPSG/0/4326', 'strdf:transform', 'http://www.opengis.net/def/crs/EPSG/0/4326']\n0.0\n0.07692307692307693\nLLM: []\ngenerated: ['y2geoo:OSI_Barony', 'y2geoo:OSM_forest']\ngt: ['y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSI_Barony', 'geo:hasGeometry', 'geo:asWKT', 'yago:Republic_of_Ireland', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfWithin', 'geof:sfWithin']\n0.0\n0.18181818181818182\nLLM: []\ngenerated: []\ngt: ['yago:Birmingham', 'geo:hasGeometry', 'geo:asWKT']\n0.0\n0.0\nLLM: []\ngenerated: ['y2geoo:OSM_city']\ngt: ['yago:wordnet_city_108524735', 'yago:isLocatedIn', 'yago:Merseyside']\n0.0\n0.0\nAverage accuracy: 0.00. Total corrects: 0\nFalse Positive rate percentage: 0.00. Total mistakes: 0\nGeoQA Average accuracy: 0.07. Total corrects: 69\nGeoQA False Positive rate percentage: 0.04. Total mistakes: 25\n","output_type":"stream"}]}]}