{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Load 100 question dataset."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T18:58:36.704743Z","iopub.status.busy":"2024-09-05T18:58:36.704271Z","iopub.status.idle":"2024-09-05T18:58:40.639500Z","shell.execute_reply":"2024-09-05T18:58:40.638086Z","shell.execute_reply.started":"2024-09-05T18:58:36.704706Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","\n","with open('/kaggle/input/100-geo-questions-uri/100_Sub_Dataset_URI.json', 'r') as file:\n","    original_dataset = json.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["# NER Pipeline"]},{"cell_type":"markdown","metadata":{},"source":["* Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T18:58:40.644026Z","iopub.status.busy":"2024-09-05T18:58:40.642284Z","iopub.status.idle":"2024-09-05T18:58:41.448056Z","shell.execute_reply":"2024-09-05T18:58:41.446821Z","shell.execute_reply.started":"2024-09-05T18:58:40.643952Z"},"trusted":true},"outputs":[],"source":["import os\n","from huggingface_hub import login\n","\n","login(token='')"]},{"cell_type":"markdown","metadata":{},"source":["* Mistral it"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T18:58:41.459084Z","iopub.status.busy":"2024-09-05T18:58:41.458360Z","iopub.status.idle":"2024-09-05T19:01:21.507246Z","shell.execute_reply":"2024-09-05T19:01:21.504952Z","shell.execute_reply.started":"2024-09-05T18:58:41.459045Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"]},{"cell_type":"markdown","metadata":{},"source":["* chat template inference function"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:21.512309Z","iopub.status.busy":"2024-09-05T19:01:21.511629Z","iopub.status.idle":"2024-09-05T19:01:21.526194Z","shell.execute_reply":"2024-09-05T19:01:21.524843Z","shell.execute_reply.started":"2024-09-05T19:01:21.512257Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def run_chat_inference(model, tokenizer, system_role, question, max_tokens=15):    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    \n","    messages = [\n","        {\"role\": \"system\", \"content\": system_role},\n","        {\"role\": \"user\", \"content\": question}\n","    ]\n","\n","    tokenizer.apply_chat_template(messages, tokenize=False)\n","\n","    model_inputs = tokenizer.apply_chat_template(messages, return_tensors = \"pt\").to(device)\n","    \n","    generated_ids = model.generate(\n","        model_inputs,\n","        max_new_tokens = 15,\n","        do_sample = True,\n","    )\n","\n","    # Decode generated text\n","    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    \n","    # Remove the system message\n","    if system_role in generated_text:\n","        generated_text = generated_text.split(system_role)[-1].strip()\n","\n","    # Remove the user message from the output to get only the assistant's response\n","    if question in generated_text:\n","        generated_text = generated_text.split(question)[-1].strip()\n","\n","    # Clear model from RAM\n","    del model\n","    torch.cuda.empty_cache()\n","    \n","    return generated_text"]},{"cell_type":"markdown","metadata":{},"source":["* NER cleanup"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:21.528571Z","iopub.status.busy":"2024-09-05T19:01:21.528111Z","iopub.status.idle":"2024-09-05T19:01:21.552572Z","shell.execute_reply":"2024-09-05T19:01:21.551197Z","shell.execute_reply.started":"2024-09-05T19:01:21.528505Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def ner_cleanup(results):\n","    if ';' not in results: \n","        return ''\n","    \n","    results = results.replace(\"[/INST]\", \"\").strip()\n","    # Remove any leading or trailing whitespace\n","    results = results.strip()\n","    # Search for the pattern in the text\n","    match = re.search(r'(.*?);', results, re.DOTALL)\n","    entities = results\n","    # If a match is found, return the matched text\n","    if match:\n","        entities = match.group(1).strip()\n","\n","    return entities"]},{"cell_type":"markdown","metadata":{},"source":["* Function to retrieve URIs of a certain toponym."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:21.555820Z","iopub.status.busy":"2024-09-05T19:01:21.554569Z","iopub.status.idle":"2024-09-05T19:01:22.736887Z","shell.execute_reply":"2024-09-05T19:01:22.735456Z","shell.execute_reply.started":"2024-09-05T19:01:21.555766Z"},"trusted":true},"outputs":[],"source":["import requests\n","import pandas as pd\n","from io import StringIO\n","\n","def graphdb_send_request(entities, endpoint_url=\"end_url\", accept_format='application/sparql-results+json'):\n","    \"\"\"\n","    Sends a SPARQL query to a GraphDB endpoint.\n","\n","    :param query: SPARQL query to be sent\n","    :param endpoint_url: URL of the GraphDB SPARQL endpoint\n","    :param accept_format: Desired response format (default is JSON)\n","    :return: Response from the endpoint\n","    \"\"\"\n","    query = f\"\"\"SELECT ?s ?name (COUNT(?related) AS ?count) WHERE {{\n","  {{\n","    SELECT ?s ?name WHERE {{\n","      {{\n","        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSI_Name> ?name .\n","      }} UNION {{\n","        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSM_Name> ?name .\n","      }} UNION {{\n","        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGADM_Name> ?name .\n","      }} UNION {{\n","        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOS_Name> ?name .\n","      }} UNION {{\n","        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGAG_Name> ?name .\n","      }} UNION {{\n","        ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSNI_Name> ?name .\n","      }}\n","      FILTER(CONTAINS(LCASE(?name), LCASE(\"{entities}\")))\n","    }}\n","  }}\n","  {{\n","    {{ ?s ?p ?related }} UNION {{ ?related ?p ?s }}\n","  }}\n","}}\n","GROUP BY ?s ?name\n","ORDER BY DESC(?count)\"\"\"\n","#     query = f\"\"\"SELECT * WHERE {{\n","#   {{\n","#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSI_Name> ?name .\n","#   }} UNION {{\n","#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSM_Name> ?name .\n","#   }} UNION {{\n","#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGADM_Name> ?name .\n","#   }} UNION {{\n","#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOS_Name> ?name .\n","#   }} UNION {{\n","#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasGAG_Name> ?name .\n","#   }} UNION {{\n","#     ?s <http://kr.di.uoa.gr/yago2geo/ontology/hasOSNI_Name> ?name .\n","#   }}\n","#   FILTER(CONTAINS(LCASE(?name), LCASE(\"{entities}\")))\n","# }}\"\"\"\n","    \n","    headers = {\n","        'Accept': accept_format,\n","        'Content-Type': 'application/x-www-form-urlencoded'\n","    }\n","\n","    data = {\n","        'query': query\n","    }\n","    \n","    try:\n","        response = requests.post(endpoint_url, headers=headers, data=data, auth=requests.auth.HTTPBasicAuth('username', 'password'))\n","\n","        if response.status_code == 200:\n","            if accept_format == 'application/sparql-results+json':\n","#                 print(response.json())\n","                json_response = response.json()\n","                return convert_json_to_csv(json_response)\n","            else:\n","#                 print(response.text)\n","                return response.text\n","        else:\n","            response.raise_for_status()\n","    except requests.exceptions.HTTPError as err:\n","        print(\"HTTP error (most likely invalid query)\")\n","        #print(query)\n","        #print(err)\n","    except Exception as err:\n","        print(err)\n","        print(\"Endpoint error ENDPOINT DOWN\")\n","\n","def convert_json_to_csv(json_data):\n","    \"\"\"\n","    Converts JSON data to CSV format.\n","\n","    :param json_data: JSON data to be converted\n","    :return: CSV formatted data as a string\n","    \"\"\"\n","    if 'boolean' in json_data:\n","        # Handling boolean result\n","        headers = ['value']\n","        rows = [[json_data['boolean']]]\n","    else:\n","        # Extracting header and rows from JSON response\n","        headers = json_data['head']['vars']\n","        rows = [{var: result.get(var, {}).get('value', '') for var in headers} for result in json_data['results']['bindings']]\n","    \n","    # Creating DataFrame and converting to CSV\n","    df = pd.DataFrame(rows, columns=headers)\n","    csv_output = StringIO()\n","    df.to_csv(csv_output, index=False)\n","    \n","    return csv_output.getvalue()"]},{"cell_type":"markdown","metadata":{},"source":["* ner system role"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:22.739482Z","iopub.status.busy":"2024-09-05T19:01:22.738805Z","iopub.status.idle":"2024-09-05T19:01:22.746376Z","shell.execute_reply":"2024-09-05T19:01:22.745059Z","shell.execute_reply.started":"2024-09-05T19:01:22.739449Z"},"trusted":true},"outputs":[],"source":["ner_system_role = \"\"\"You are a specialized Named Entity Recognition (NER) system focused on identifying and extracting toponyms (place names) from the given text. Your task is to recognize and list all geographical entities, including but not limited to:\n","\n","- Countries\n","- Cities\n","- States/Provinces\n","- Regions\n","- Mountains\n","- Rivers\n","- Oceans/Seas\n","- Lakes\n","- Islands\n","- Continents\n","\n","For each input, provide a list of extracted toponyms, separated by commas. If no toponyms are found, respond with \"No toponyms found;\" After completing the analysis, end your response with a semicolon.\n","\n","Examples:\n","Q: Where is Swansea located?\n","A: Swansea;\n","\n","Q: Which Greek regions have between 500000 and 1000000 inhabitants?\n","A: Greece;\n","\n","Q: Is Doolin to the south of Dublin?\n","A: Doolin, Dublin;\n","\n","Q: What's the capital of France and how far is it from the Mediterranean Sea?\n","A: France, Mediterranean Sea;\n","\n","Q: What is the biggest island in the world?\n","A: No toponyms found;\n","\n","Now, analyze the following text. Remember to split the toponyms by commas so I can recognize them individualy.\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["* prompt to select best uri candidate."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:22.748313Z","iopub.status.busy":"2024-09-05T19:01:22.747969Z","iopub.status.idle":"2024-09-05T19:01:22.763663Z","shell.execute_reply":"2024-09-05T19:01:22.762230Z","shell.execute_reply.started":"2024-09-05T19:01:22.748284Z"},"trusted":true},"outputs":[],"source":["def select_uri_system_role(question):\n","    uri_select_system_role = f\"\"\"You perform disambiguation, this means selecting the most relevant URI for a toponym or entity. You will be supplied with a question and a list of URIs. You have to select one URI out of the provided. \n","    The list might have multiple toponyms but the supplied URIs will be relevant to only one. Strictly respond with the URI id and no other information or explanation.\n","    \n","    For example:\n","    Q: Consider this question: Is New York south of Florida?\n","    Choose the best URI from the provided list: \n","    1. yago:New_York, New York City\n","    2. yago:New_York_Municipality, Municipality of New York\n","    3. geof:York_Museum, Historical Museum of York\n","    \n","    A: 1\n","    \n","    Now consider this question: {question}\n","    And choose the best URI from the provided list:\"\"\"\n","    return uri_select_system_role"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:22.769012Z","iopub.status.busy":"2024-09-05T19:01:22.768398Z","iopub.status.idle":"2024-09-05T19:01:22.776359Z","shell.execute_reply":"2024-09-05T19:01:22.775260Z","shell.execute_reply.started":"2024-09-05T19:01:22.768978Z"},"trusted":true},"outputs":[],"source":["def select_uri_system_role(question):\n","    uri_select_system_role = f\"\"\"You perform disambiguation, this means selecting the most relevant URI for a toponym or entity. You will be supplied with a question and a list of URIs. You have to select one URI out of the provided. \n","    The list might have multiple toponyms but the supplied URIs will be relevant to only one. Strictly respond with the URI id and no other information or explanation.\n","    \n","    For example:\n","    Q: Consider this question: Is New York south of Florida?\n","    Choose the best URI from the provided list: \n","    1. yago:New_York\n","    2. yago:New_York_Municipality\n","    3. geof:York_Museum\n","    \n","    A: 1\n","    \n","    Now consider this question: {question}\n","    And choose the best URI from the provided list:\"\"\"\n","    return uri_select_system_role"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:22.778649Z","iopub.status.busy":"2024-09-05T19:01:22.778147Z","iopub.status.idle":"2024-09-05T19:01:22.794702Z","shell.execute_reply":"2024-09-05T19:01:22.793278Z","shell.execute_reply.started":"2024-09-05T19:01:22.778611Z"},"trusted":true},"outputs":[],"source":["# This prefix map will be used to shrink the uri's down to the prefix level, to help the model better understand them and decrease mistakes.\n","prefix_map = {\"http://www.opengis.net/ont/geosparql#\" : \"geo:\",\n","               \"http://www.opengis.net/def/function/geosparql/\" : \"geof:\",\n","               \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" : \"rdf:\",\n","               \"http://www.w3.org/2000/01/rdf-schema#\" : \"rdfs:\",\n","               \"http://www.w3.org/2001/XMLSchema#\" : \"xsd:\",\n","               \"http://yago-knowledge.org/resource/\" : \"yago:\",\n","               \"http://kr.di.uoa.gr/yago2geo/resource/\" : \"y2geor:\",\n","               \"http://kr.di.uoa.gr/yago2geo/ontology/\" : \"y2geoo:\",\n","               \"http://strdf.di.uoa.gr/ontology#\" : \"strdf:\",\n","               \"http://www.opengis.net/def/uom/OGC/1.0/\" : \"uom:\",\n","               \"http://www.w3.org/2002/07/owl#\" : \"owl:\"}"]},{"cell_type":"markdown","metadata":{},"source":["* Concept Identifier."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:01:22.796817Z","iopub.status.busy":"2024-09-05T19:01:22.796387Z","iopub.status.idle":"2024-09-05T19:02:01.617105Z","shell.execute_reply":"2024-09-05T19:02:01.615591Z","shell.execute_reply.started":"2024-09-05T19:01:22.796785Z"},"trusted":true},"outputs":[],"source":["!pip install stanza\n","!pip install rapidfuzz"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:04:34.113597Z","iopub.status.busy":"2024-09-05T19:04:34.112454Z","iopub.status.idle":"2024-09-05T19:04:48.305138Z","shell.execute_reply":"2024-09-05T19:04:48.301636Z","shell.execute_reply.started":"2024-09-05T19:04:34.113549Z"},"trusted":true},"outputs":[],"source":["import stanza\n","import nltk\n","import csv\n","from nltk.util import ngrams\n","from rapidfuzz.distance import Levenshtein\n","from rapidfuzz import fuzz\n","\n","# Ensure necessary packages are available\n","nltk.download('punkt')\n","\n","# Initialize the Stanford CoreNLP Pipeline\n","stanza.download('en')\n","nlps = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n","\n","def read_file(filepath):\n","    with open(filepath, 'r') as file:\n","        reader = csv.reader(file)\n","        data = {row[0].lower(): row[1] for row in reader}  # Store labels and corresponding URIs\n","    return data\n","\n","def compute_similarity(ngram, label):\n","    ngram_str = ' '.join(ngram)\n","    if ' ' in ngram_str:\n","        # It's a bigram (or higher order n-gram) if it contains a space\n","        #print(f\"jaro {fuzz.WRatio(ngram_str, label) / 100}\")\n","        return fuzz.WRatio(ngram_str, label) / 100\n","    else:\n","        # It's a unigram\n","        #print(f\"Lev: {ngram_str},{label} : {1 - (Levenshtein.distance(ngram_str, label) / max(len(ngram_str), len(label)))}\")\n","        return 1 - (Levenshtein.distance(ngram_str, label) / max(len(ngram_str), len(label)))\n","\n","def Concept_Identifier(question):\n","    # Read files and prepare data\n","    file1_data = read_file('/kaggle/input/yagoclasses1/YAGO2geoClasses.txt')\n","    #file2_data = read_file('/kaggle/input/yagoclasses1/YAGOClasses.txt')\n","    \n","    uris = []\n","    \n","    # Process the question using the NLP pipeline\n","    doc = nlps(question)\n","    \n","    for sentence in doc.sentences:\n","        i = 0\n","        while i < len(sentence.words):\n","            word = sentence.words[i]\n","            ngrams_to_check = []\n","            \n","            # Check for specific POS tags to form n-grams\n","            if word.xpos in {\"NN\", \"NNS\", \"NNP\", \"NNPS\"}:\n","                current_ngram = [word.lemma.lower()]\n","                i += 1\n","                \n","                # Continue adding to the n-gram if subsequent words have the same relevant POS tags\n","                while i < len(sentence.words) and sentence.words[i].xpos in {\"NN\", \"NNS\", \"NNP\", \"NNPS\"}:\n","                    current_ngram.append(sentence.words[i].lemma.lower())\n","                    i += 1\n","                \n","                # Add the formed n-gram to the list\n","                ngrams_to_check.append(' '.join(current_ngram))\n","                \n","                max_similarity = 0\n","                threshold = 0.7\n","                best_uri = None\n","\n","                # Compare against all labels in file1 and file2\n","                for label, uri in {**file1_data}.items():\n","                    for ngram in ngrams_to_check:\n","                        similarity = compute_similarity([ngram], label)\n","                        if similarity > max_similarity and similarity > threshold:\n","                            max_similarity = similarity\n","                            best_uri = uri\n","                \n","                if best_uri:\n","                    f_uri = best_uri.replace(\" \", \"\")\n","                    uris.append(f_uri)\n","            else:\n","                i += 1  # Move to the next word if it doesn't match the POS tags\n","    \n","    return list(set(uris))\n","\n","# Example usage\n","question = \"Which bays intersect with county councils that border with County Mayo?\"\n","uris = Concept_Identifier(question)\n","print(uris)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:02:17.773530Z","iopub.status.busy":"2024-09-05T19:02:17.772305Z","iopub.status.idle":"2024-09-05T19:02:35.911321Z","shell.execute_reply":"2024-09-05T19:02:35.909463Z","shell.execute_reply.started":"2024-09-05T19:02:17.773460Z"},"trusted":true},"outputs":[],"source":["!pip install -q jellyfish Levenshtein"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:02:35.914223Z","iopub.status.busy":"2024-09-05T19:02:35.913754Z","iopub.status.idle":"2024-09-05T19:03:18.423480Z","shell.execute_reply":"2024-09-05T19:03:18.421834Z","shell.execute_reply.started":"2024-09-05T19:02:35.914181Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install -q spacy\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:03:18.426773Z","iopub.status.busy":"2024-09-05T19:03:18.426167Z","iopub.status.idle":"2024-09-05T19:03:21.372360Z","shell.execute_reply":"2024-09-05T19:03:21.370719Z","shell.execute_reply.started":"2024-09-05T19:03:18.426716Z"},"trusted":true},"outputs":[],"source":["import os\n","import spacy\n","from jellyfish import jaro_winkler_similarity\n","import Levenshtein as lev\n","\n","# Load spaCy English model\n","nlp = spacy.load('en_core_web_sm')\n","\n","def read_file(file_path):\n","    with open(file_path, 'r') as file:\n","        return file.readlines()\n","\n","def levenshtein_similarity(str1, str2):\n","    distance = lev.distance(str1, str2)\n","    max_len = max(len(str1), len(str2))\n","    similarity = 1 - (distance / max_len)  # Normalize the distance to get a similarity measure\n","    return similarity\n","\n","def lemmatize_word(word):\n","    doc = nlp(word)\n","    return doc[0].lemma_\n","\n","def Plain_Concept_Identifier(question, threshold=0.99):\n","    file1_data = read_file('/kaggle/input/yagoclasses1/YAGO2geoClasses.txt')\n","    #file2_data = read_file('/kaggle/input/yagoclasses1/YAGOClasses.txt')\n","    labels = file1_data \n","    \n","    words = question.lower().split()\n","    words = [lemmatize_word(word) for word in words]  # Lemmatize each word\n","    num_words = len(words)\n","\n","    uris = []\n","    \n","    for label in labels:\n","        label_text = label.strip().split(',')[0].lower()  # Lowercase label\n","        uri = label.strip().split(',')[1]\n","        label_words = label_text.split()\n","        label_length = len(label_words)\n","\n","        # Check similarity for single-word labels using Levenshtein distance\n","        if label_length == 1:\n","            for word in words:\n","                similarity = levenshtein_similarity(word, label_text)\n","                if similarity > threshold:\n","                    uris.append(uri)\n","#                     print(f\"{word} matched with {label_text}\")\n","#                     print(uri)\n","\n","        # Check similarity for multi-word labels using Jaro-Winkler similarity\n","        else:\n","            for i in range(num_words - label_length + 1):\n","                word_sequence = \" \".join(words[i:i + label_length])\n","                similarity = jaro_winkler_similarity(word_sequence, label_text)\n","                if similarity > threshold:\n","                    uris.append(uri)\n","#                     print(f\"{word_sequence} matched with {label_text}\")\n","#                     print(uri)\n","    return list(set(uris))\n","\n","question = \"Which bays intersect with county councils that border with County Mayo?\"\n","\n","print(Plain_Concept_Identifier(question))"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:03:21.375084Z","iopub.status.busy":"2024-09-05T19:03:21.374180Z","iopub.status.idle":"2024-09-05T19:03:21.390065Z","shell.execute_reply":"2024-09-05T19:03:21.388616Z","shell.execute_reply.started":"2024-09-05T19:03:21.375045Z"},"trusted":true},"outputs":[],"source":["def read_file_as_string(filepath):\n","    with open(filepath, 'r') as file:\n","        return file.read()\n","\n","# Reading the two files and saving their contents as strings\n","file1_data = read_file_as_string('/kaggle/input/yagoclasses1/YAGO2geoClasses.txt')\n","file2_data = read_file_as_string('/kaggle/input/yagoclasses1/YAGOClasses.txt')\n","\n","concept_identifier_role = f\"\"\"You are an expect concept identifier. You are given a knowledge base of URIs that represent various concepts.\n","Each URI is associated by a descriptive label. The format is \"label,URI\". Your job is to identify concepts within the user-supplied questions and return only the URIs that correspond to them.\n","If a specific concept is not mentioned you do not report it, even if it is semantically relevant.\n","Your answers include only the relevant URIs seperated by commas. If no URI is present you do not answer anything. You do not provide any explanations.\n","\n","Your knowledge base is the following: {file1_data}\n","\n","For example: \n","Q: \"Which bays intersect with county councils that border with County Mayo?\"\n","A: \"y2geoo:OSI_City_and_County_Council,y2geoo:OSM_bay\"\n","Q: \"Is Doolin to the south of Dublin?\"\n","A: \"\"\n","Q: \"Which forests are within baronies in the Republic of Ireland?\"\n","A: \"y2geoo:OSM_forest,y2geoo:OSI_Barony\"\n","\n","Now based on your knowledge base answer the user question.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:03:21.392536Z","iopub.status.busy":"2024-09-05T19:03:21.391988Z","iopub.status.idle":"2024-09-05T19:03:24.281524Z","shell.execute_reply":"2024-09-05T19:03:24.279223Z","shell.execute_reply.started":"2024-09-05T19:03:21.392464Z"},"trusted":true},"outputs":[],"source":["def LLM_Concept_Identifier(question):\n","    result = run_chat_inference(model, tokenizer, concept_identifier_role, question, max_tokens=20)\n","    result = result.replace(\"[/INST]\", \"\").strip()\n","    result = result.replace(\"\\\\\", \"\").strip()\n","    uri_pattern = r'\\b\\w+:[\\w_]+\\b'\n","    \n","    # Find all URIs that match the pattern\n","    uris = re.findall(uri_pattern, result)\n","    \n","    #print(uris)\n","    return uris\n","    \n","LLM_Concept_Identifier(\"Which forests are within baronies in the Republic of Ireland?\")"]},{"cell_type":"markdown","metadata":{},"source":["* GeoQA Instance Identifier system:"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:04:48.314071Z","iopub.status.busy":"2024-09-05T19:04:48.312834Z","iopub.status.idle":"2024-09-05T19:04:48.349690Z","shell.execute_reply":"2024-09-05T19:04:48.346407Z","shell.execute_reply.started":"2024-09-05T19:04:48.313961Z"},"trusted":true},"outputs":[],"source":["import requests, json\n","\n","class WATAnnotation:\n","    # An entity annotated by WAT\n","\n","    def __init__(self, d):\n","\n","        # char offset (included)\n","        self.start = d['start']\n","        # char offset (not included)\n","        self.end = d['end']\n","\n","        # annotation accuracy\n","        self.rho = d['rho']\n","        # spot-entity probability\n","        self.prior_prob = d['explanation']['prior_explanation']['entity_mention_probability']\n","\n","        # annotated text\n","        self.spot = d['spot']\n","\n","        # Wikpedia entity info\n","        self.wiki_id = d['id']\n","        self.wiki_title = d['title']\n","\n","\n","    def json_dict(self):\n","        # Simple dictionary representation\n","        return {'wiki_title': self.wiki_title,\n","                'wiki_id': self.wiki_id,\n","                'start': self.start,\n","                'end': self.end,\n","                'rho': self.rho,\n","                'prior_prob': self.prior_prob\n","                }\n","    \n","def wat_entity_linking(text):\n","    # Main method, text annotation with WAT entity linking system\n","    wat_url = 'https://wat.d4science.org/wat/tag/tag'\n","    payload = [(\"gcube-token\", MY_GCUBE_TOKEN),\n","               (\"text\", text),\n","               (\"lang\", 'en'),\n","               (\"tokenizer\", \"nlp4j\"),\n","               ('debug', 9),\n","               (\"method\",\n","                \"spotter:includeUserHint=true:includeNamedEntity=true:includeNounPhrase=true,prior:k=50,filter-valid,centroid:rescore=true,topk:k=5,voting:relatedness=lm,ranker:model=0046.model,confidence:model=pruner-wiki.linear\")]\n","\n","    response = requests.get(wat_url, params=payload)\n","    return [WATAnnotation(a) for a in response.json()['annotations']]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:04:48.356192Z","iopub.status.busy":"2024-09-05T19:04:48.354773Z","iopub.status.idle":"2024-09-05T19:04:48.382368Z","shell.execute_reply":"2024-09-05T19:04:48.379832Z","shell.execute_reply.started":"2024-09-05T19:04:48.356073Z"},"trusted":true},"outputs":[],"source":["import requests\n","\n","def test_uri(uri, endpoint_url=\"endp\", accept_format='application/sparql-results+json'):\n","    \"\"\"\n","    Sends a SPARQL ASK query to a GraphDB endpoint to check if a URI exists.\n","\n","    :param uri: URI to be checked\n","    :param endpoint_url: URL of the GraphDB SPARQL endpoint\n","    :param accept_format: Desired response format (default is JSON)\n","    :return: Boolean indicating if the URI exists\n","    \"\"\"\n","    query = f\"\"\"ASK WHERE {{\n","  <{uri}> ?p ?o .\n","}}\"\"\"\n","    headers = {\n","        'Accept': accept_format,\n","        'Content-Type': 'application/x-www-form-urlencoded'\n","    }\n","\n","    data = {\n","        'query': query\n","    }\n","    \n","    try:\n","        response = requests.post(endpoint_url, headers=headers, data=data, auth=requests.auth.HTTPBasicAuth('user', 'pass'))\n","\n","        if response.status_code == 200:\n","            if accept_format == 'application/sparql-results+json':\n","                json_response = response.json()\n","                return json_response.get('boolean', False)  # Return True if URI exists, otherwise False\n","            else:\n","                return False  # If not using JSON format, default to False (or handle differently if necessary)\n","        else:\n","            response.raise_for_status()\n","    except requests.exceptions.HTTPError as err:\n","        print(\"HTTP error (most likely invalid query):\", err)\n","        return False\n","    except Exception as err:\n","        print(\"Endpoint error or endpoint is down:\", err)\n","        return False\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:04:50.735228Z","iopub.status.busy":"2024-09-05T19:04:50.734706Z","iopub.status.idle":"2024-09-05T19:04:50.748800Z","shell.execute_reply":"2024-09-05T19:04:50.747032Z","shell.execute_reply.started":"2024-09-05T19:04:50.735189Z"},"trusted":true},"outputs":[],"source":["def GeoQAInstanceIdentifier(question):\n","    # Process the question using the NLP pipeline\n","    doc = nlp(question)\n","    uris = []\n","    \n","    for sentence in doc.sentences:\n","        for word in sentence.words:\n","            # Display the word, lemma, POS tag, and dependency information in CoNLL-U format\n","            #print(f\"{word.id}\\t{word.text}\\t{word.lemma}\\t{word.upos}\\t{word.xpos}\\t_\\t{word.head}\\t{word.deprel}\\t_\\t_\")\n","            \n","            #print (word.upos)\n","            # Check for specific POS tags and map to the geospatial relations\n","            if word.xpos in {\"NN\", \"NNS\", \"NNP\", \"NNPS\"}:\n","                print(word.text)\n","                #lemma = word.lemma.lower()\n","                ann = wat_entity_linking(word.text)\n","                for result in ann: \n","                    title = result.wiki_title\n","\n","                    yago_link = \"http://yago-knowledge.org/resource/\"\n","                    target_uri = yago_link + title\n","                    full_uri = target_uri\n","                    \n","                    # Shorten the uris down to prefixes.\n","                    for uri_map, prefix in prefix_map.items():\n","                        target_uri = target_uri.replace(uri_map, prefix)\n","\n","                    target_uri = target_uri.replace(\"&amp;\", \"&\")\n","                    target_uri = target_uri.replace(\" \", \"_\")\n","                    \n","                    exists = test_uri(full_uri)\n","                    \n","                    if exists == True:\n","                        uris.append(target_uri)\n","#                     else:\n","#                         ##### SEARCH IN KG ##### This performed worse.\n","#                         recognized_uris = graphdb_send_request(title)\n","#                         # Use StringIO to treat the CSV string as a file\n","#                         csv_file = StringIO(recognized_uris)\n","#                         csv_reader = csv.reader(csv_file)\n","#                         # Skip the first row (headers)\n","#                         next(csv_reader)\n","\n","#                         candidate_uris = \"\"\n","#                         relevant_uris = []\n","#                         for row in csv_reader:\n","#                             uri = row[0]\n","#                             name = row[1]\n","#                             count = row[2]\n","#                             # Shorten the uris down to prefixes.\n","#                             for uri_map, prefix in prefix_map.items():\n","#                                 uri = uri.replace(uri_map, prefix)\n","\n","#                             relevant_uris.append((uri, name, count))\n","#                             final_uri = f\"Uri: {uri}, Name: {name}, Count: {count}\"\n","#                             candidate_uris += final_uri\n","\n","#                         # Popular uri choice.\n","#                         if relevant_uris != []:\n","#                             uris.append(relevant_uris[0][0])\n","                  \n","    return uris"]},{"cell_type":"markdown","metadata":{},"source":["* LLM-powered NER Pipeline function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:03:24.292180Z","iopub.status.idle":"2024-09-05T19:03:24.292935Z","shell.execute_reply":"2024-09-05T19:03:24.292613Z","shell.execute_reply.started":"2024-09-05T19:03:24.292584Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def extract_first_integer(s):\n","    match = re.search(r'\\d+', s)\n","    if match:\n","        return int(match.group(0))\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-05T19:03:24.296189Z","iopub.status.idle":"2024-09-05T19:03:24.296904Z","shell.execute_reply":"2024-09-05T19:03:24.296591Z","shell.execute_reply.started":"2024-09-05T19:03:24.296562Z"},"trusted":true},"outputs":[],"source":["import csv\n","from io import StringIO\n","\n","def retrieve_uris(question):\n","    # Extract toponyms from the questions using NER.\n","    result = run_chat_inference(model, tokenizer, ner_system_role, question)\n","    result = ner_cleanup(result)\n","    # Split the toponym string to a list of toponyms.\n","    if result == '':\n","        toponyms = []\n","    else:\n","        toponyms = result.split(',')\n","    \n","    uris = []\n","    # Disambigation with WAT\n","    for toponym in toponyms: \n","        ann = wat_entity_linking(toponym)\n","        for result in ann: \n","            title = result.wiki_title\n","\n","            yago_link = \"http://yago-knowledge.org/resource/\"\n","            target_uri = yago_link + title\n","            full_uri = target_uri\n","\n","            # Shorten the uris down to prefixes.\n","            for uri_map, prefix in prefix_map.items():\n","                target_uri = target_uri.replace(uri_map, prefix)\n","\n","            target_uri = target_uri.replace(\"&amp;\", \"&\")\n","            target_uri = target_uri.replace(\" \", \"_\")\n","\n","            exists = test_uri(full_uri)\n","\n","            if exists == True:\n","                uris.append(target_uri)\n","            else:\n","                ##### SEARCH IN KG #####\n","                recognized_uris = graphdb_send_request(title)\n","                # Use StringIO to treat the CSV string as a file\n","                csv_file = StringIO(recognized_uris)\n","                csv_reader = csv.reader(csv_file)\n","                # Skip the first row (headers)\n","                next(csv_reader)\n","\n","                candidate_uris = \"\"\n","                relevant_uris = []\n","                for row in csv_reader:\n","                    uri = row[0]\n","                    name = row[1]\n","                    count = row[2]\n","                    # Shorten the uris down to prefixes.\n","                    for uri_map, prefix in prefix_map.items():\n","                        uri = uri.replace(uri_map, prefix)\n","\n","                    relevant_uris.append((uri, name, count))\n","                    final_uri = f\"Uri: {uri}, Name: {name}, Count: {count}\"\n","                    candidate_uris += final_uri\n","\n","                # Popular uri choice.\n","                if relevant_uris != []:\n","                    uris.append(relevant_uris[0][0])\n","    return uris\n","        \n","    # List of the uris that will be used.\n","    uris = []\n","    for toponym in toponyms:\n","        recognized_uris = graphdb_send_request(toponym)\n","        #print(recognized_uris)\n","\n","        # Use StringIO to treat the CSV string as a file\n","        csv_file = StringIO(recognized_uris)\n","        csv_reader = csv.reader(csv_file)\n","        # Skip the first row (headers)\n","        next(csv_reader)\n","        \n","        candidate_uris = \"\"\n","        relevant_uris = []\n","        for row in csv_reader:\n","            uri = row[0]\n","            name = row[1]\n","            count = row[2]\n","            # Shorten the uris down to prefixes.\n","            for uri_map, prefix in prefix_map.items():\n","                uri = uri.replace(uri_map, prefix)\n","            \n","            relevant_uris.append((uri, name, count))\n","            final_uri = f\"Uri: {uri}, Name: {name}, Count: {count}\"\n","            candidate_uris += final_uri\n","    \n","        if relevant_uris == []:\n","            return []\n","        \n","        # Limit the model to select from the 3 most frequently used relevant uris (usually the first is the target).\n","        supplied_uris = \"\"\n","        limit = min(3, len(relevant_uris))\n","        for i in range (0, limit):\n","            #supplied_uris += f\"{i}. {relevant_uris[i][0], relevant_uris[i][1]}\"\n","            # THIS APPROACH DOES NOT INCLUDE THE NAME OF EACH URI SUPPLIED TO THE MODEL #\n","            # This works better, not because the model is confused by the amount of info\n","            # but rather because the entities of the KG have sometimes odd and misleading names.\n","            supplied_uris += f\"{i}. {relevant_uris[i][0]}\"\n","            \n","        print(supplied_uris)\n","        # Prompt model again, but this time to choose the correct uri to be used.\n","        uri_select_system_role = select_uri_system_role(question)\n","        \n","        result = run_chat_inference(model, tokenizer, uri_select_system_role, supplied_uris)\n","        #print(f\"hello? {result}\")\n","        # Model might return some characters along the number e.g. \"A: 0\". Simply extract the int from the string.\n","        target = extract_first_integer(result)\n","        \n","        # Instead of using the LLM again to select, naively select the most frequently used URI.\n","#         uris.append(relevant_uris[0][0])\n","        \n","        # \"Intelligent\" uri selection powered by LLM disambiguation.\n","        if target in range(0, limit):\n","            uris.append(relevant_uris[target][0])\n","\n","    return uris"]},{"cell_type":"markdown","metadata":{},"source":["* Evaluation and comparisson"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:04:57.226081Z","iopub.status.busy":"2024-09-05T19:04:57.225456Z","iopub.status.idle":"2024-09-05T19:04:57.235309Z","shell.execute_reply":"2024-09-05T19:04:57.233487Z","shell.execute_reply.started":"2024-09-05T19:04:57.226032Z"},"trusted":true},"outputs":[],"source":["count = 0 \n","\n","for key in original_dataset:\n","    uris = original_dataset[key]['URI']\n","    for uri in uris:\n","        count += 1\n","        \n","print (count)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T19:09:57.408823Z","iopub.status.busy":"2024-09-05T19:09:57.407369Z","iopub.status.idle":"2024-09-05T19:10:16.516144Z","shell.execute_reply":"2024-09-05T19:10:16.514787Z","shell.execute_reply.started":"2024-09-05T19:09:57.408778Z"},"trusted":true},"outputs":[],"source":["scores = []\n","fp_scores = []\n","w_count = 0\n","c_count = 0\n","\n","w_scores = []\n","wfp_scores = []\n","ww_count = 0\n","wc_count = 0\n","\n","for key in original_dataset:\n","    # Get the dataset question and dataset uris.\n","    question = original_dataset[key]['Question']\n","    ground_truth_uris = original_dataset[key]['URI']\n","    \n","    # Generate uris from the question.\n","#     generated_uris = retrieve_uris(question)\n","#     concept_uris = LLM_Concept_Identifier(question)\n","#     if concept_uris != []:\n","#         generated_uris.extend(concept_uris)\n","    generated_uris = []\n","    print(f\"LLM: {generated_uris}\")\n","    # Generate uris with the GeoQA method for comparisson.\n","#     wat_uris = GeoQAInstanceIdentifier(question)\n","    wat_uris = Concept_Identifier(question)\n","#     wat_uris = []\n","    print(f\"generated: {wat_uris}\")\n","    print(f\"gt: {ground_truth_uris}\")\n","\n","    # Save the results to the dataset.\n","    original_dataset[key]['Gen_URI'] = wat_uris\n","    \n","    # Evaluate the generated results compared to the ground truth uris.\n","    correct = 0\n","    wrong = 0\n","    for uri in ground_truth_uris: \n","        if generated_uris: \n","            if uri in generated_uris:\n","                correct += 1\n","            \n","    # Count False positives.\n","    if generated_uris: \n","        for uri in generated_uris: \n","            if uri not in ground_truth_uris: \n","                wrong += 1\n","    \n","    if len(ground_truth_uris) != 0:\n","        accuracy = correct/len(ground_truth_uris)\n","        fp_perc = wrong/len(ground_truth_uris)\n","        print(accuracy)\n","        scores.append(accuracy)\n","        fp_scores.append(fp_perc)\n","        \n","    ###### SAME FOR WAT. ######\n","    w_correct = 0\n","    w_wrong = 0\n","    for uri in ground_truth_uris: \n","        if wat_uris:\n","            if uri in wat_uris:\n","                w_correct += 1\n","            \n","    # Count False positives.\n","    if wat_uris: \n","        for uri in wat_uris: \n","            if uri not in ground_truth_uris: \n","                w_wrong += 1\n","    \n","    if len(ground_truth_uris) != 0:\n","        accuracy = w_correct/len(ground_truth_uris)\n","        fp_perc = w_wrong/len(ground_truth_uris)\n","        print(accuracy)\n","        w_scores.append(accuracy)\n","        wfp_scores.append(fp_perc)\n","    \n","    c_count += correct\n","    w_count += wrong\n","    \n","    wc_count += w_correct\n","    ww_count += w_wrong\n","    \n","# Print average of scores.\n","average_accuracy = sum(scores) / len(scores) if scores else 0\n","fp_perc = sum(fp_scores) / len(fp_scores) if fp_scores else 0\n","print(f\"Average accuracy: {average_accuracy:.2f}. Total corrects: {c_count}\")\n","print(f\"False Positive rate percentage: {fp_perc:.2f}. Total mistakes: {w_count}\")\n","\n","# Print average of scores for wat.\n","average_accuracy = sum(w_scores) / len(w_scores) if w_scores else 0\n","fp_perc = sum(wfp_scores) / len(wfp_scores) if wfp_scores else 0\n","print(f\"GeoQA Average accuracy: {average_accuracy:.2f}. Total corrects: {wc_count}\")\n","print(f\"GeoQA False Positive rate percentage: {fp_perc:.2f}. Total mistakes: {ww_count}\")\n","\n","# Save the JSON data to a file\n","with open('concepts_dataset.json', 'w') as json_file:\n","    json.dump(original_dataset, json_file, indent=4)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5299649,"sourceId":8812575,"sourceType":"datasetVersion"},{"datasetId":5560618,"sourceId":9197625,"sourceType":"datasetVersion"},{"datasetId":5614381,"sourceId":9276358,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
