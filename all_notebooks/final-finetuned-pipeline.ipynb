{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8812575,"sourceType":"datasetVersion","datasetId":5299649},{"sourceId":9288757,"sourceType":"datasetVersion","datasetId":5623080},{"sourceId":9328230,"sourceType":"datasetVersion","datasetId":5651610},{"sourceId":9448558,"sourceType":"datasetVersion","datasetId":5742793}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load 100 question dataset.","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n#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\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:26:37.634357Z","iopub.execute_input":"2024-09-21T10:26:37.635896Z","iopub.status.idle":"2024-09-21T10:26:37.722950Z","shell.execute_reply.started":"2024-09-21T10:26:37.635845Z","shell.execute_reply":"2024-09-21T10:26:37.722029Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/concept-no-rdfs/concepts_dataset_no_rdfs.json', 'r') as file:\n    original_dataset = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:26:37.724843Z","iopub.execute_input":"2024-09-21T10:26:37.725593Z","iopub.status.idle":"2024-09-21T10:26:37.731777Z","shell.execute_reply.started":"2024-09-21T10:26:37.725548Z","shell.execute_reply":"2024-09-21T10:26:37.730967Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# import json\n# import torch\n\n# with open('/kaggle/input/aurii-concepts-instances-dataset/concepts_instances_dataset.json', 'r') as file:\n#     original_dataset = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:26:37.732743Z","iopub.execute_input":"2024-09-21T10:26:37.733026Z","iopub.status.idle":"2024-09-21T10:26:37.742266Z","shell.execute_reply.started":"2024-09-21T10:26:37.732994Z","shell.execute_reply":"2024-09-21T10:26:37.741398Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"markdown","source":"## Model loading and inference.","metadata":{}},{"cell_type":"markdown","source":"* Load model","metadata":{}},{"cell_type":"code","source":"# import torch\n\n# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n\n# model = AutoModelForCausalLM.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\", torch_dtype=torch.float16)\n# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:26:37.744367Z","iopub.execute_input":"2024-09-21T10:26:37.744661Z","iopub.status.idle":"2024-09-21T10:26:37.754129Z","shell.execute_reply.started":"2024-09-21T10:26:37.744619Z","shell.execute_reply":"2024-09-21T10:26:37.753392Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!pip install -q accelerate bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:26:37.755242Z","iopub.execute_input":"2024-09-21T10:26:37.755526Z","iopub.status.idle":"2024-09-21T10:26:50.650537Z","shell.execute_reply.started":"2024-09-21T10:26:37.755495Z","shell.execute_reply":"2024-09-21T10:26:50.649417Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!pip install -q peft","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:26:50.652066Z","iopub.execute_input":"2024-09-21T10:26:50.652387Z","iopub.status.idle":"2024-09-21T10:27:03.583424Z","shell.execute_reply.started":"2024-09-21T10:26:50.652354Z","shell.execute_reply":"2024-09-21T10:27:03.582334Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Stratos-Kakalis/norm_trunc_no_rdfs_8_epoch\", torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:27:03.584918Z","iopub.execute_input":"2024-09-21T10:27:03.585260Z","iopub.status.idle":"2024-09-21T10:29:33.392427Z","shell.execute_reply.started":"2024-09-21T10:27:03.585225Z","shell.execute_reply":"2024-09-21T10:29:33.390531Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f644befd45c441c59e1a1472ef36457c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7299ad8bf9440b2984ae2c461f9a1ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533a80071ee94eeab7d769b3ac123eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e76cad4dbb46b9942634cf01857375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64b8423371a344d58767db3472aa8f7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b3ddadcc89428c82588e2ae8eb7a27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df0434ce5c904adc86c6cd77b965ed46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fba64ccadf740309de2fcf914d2c70d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4122f20d4994d9eb4028226edcd1270"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83c75707adba4c15b51b9aec02937ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/960 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"364d7add37f44a3d9aee08ebe9b7015a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7141424a104d3ba2d1af538f9ea1f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ffca3ad53494bf0a913e83bde6984af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da9fe398ee148fab436a509e165ac36"}},"metadata":{}}]},{"cell_type":"markdown","source":"* Inference function","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef run_chat_inference(model, tokenizer, system_role, user_message):    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_role},\n        {\"role\": \"user\", \"content\": user_message}\n    ]\n\n    tokenizer.apply_chat_template(messages, tokenize=False)\n\n    model_inputs = tokenizer.apply_chat_template(messages, return_tensors = \"pt\").to(device)\n    \n    generated_ids = model.generate(\n        model_inputs,\n        max_new_tokens = 1000,\n        do_sample = True,\n    )\n\n    # Decode generated text\n    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    # Remove the system message\n    if system_role in generated_text:\n        generated_text = generated_text.split(system_role)[-1].strip()\n\n    # Remove the user message from the output to get only the assistant's response\n    if user_message in generated_text:\n        generated_text = generated_text.split(user_message)[-1].strip()\n\n    # Clear model from RAM\n    del model\n    torch.cuda.empty_cache()\n    \n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.394732Z","iopub.execute_input":"2024-09-21T10:29:33.395713Z","iopub.status.idle":"2024-09-21T10:29:33.404345Z","shell.execute_reply.started":"2024-09-21T10:29:33.395664Z","shell.execute_reply":"2024-09-21T10:29:33.403395Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def run_inference(model, tokenizer, prompt):\n    # Move model to GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()  # Set model to evaluation mode\n            \n    # Tokenize prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n            \n    # Generate output\n    with torch.no_grad():\n        outputs = model.generate(**inputs, \n                        max_new_tokens=800,  # Set a maximum length for generated text\n                        #do_sample=True,  # Enable sampling\n                        #top_k=7,        # Top-k sampling\n                        #top_p=0.1,      # Top-p sampling (nucleus sampling)\n                        #num_return_sequences=1,\n                        #repetition_penalty=1, # No penalty for instruction tuned models.\n                        repetition_penalty=1.2, # Penalty on repeating tokens.\n                        eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n                        pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n                        )\n        \n    # Extract generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n   \n    # Remove the prompt text\n    prompt_length = len(prompt)\n    generated_text = generated_text[prompt_length:]\n\n    # Decode and print output\n    #print(\"Prompt:\", prompt)\n    \n    # Clear model from RAM\n    del model\n    torch.cuda.empty_cache()\n    \n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.405454Z","iopub.execute_input":"2024-09-21T10:29:33.405720Z","iopub.status.idle":"2024-09-21T10:29:33.434205Z","shell.execute_reply.started":"2024-09-21T10:29:33.405690Z","shell.execute_reply":"2024-09-21T10:29:33.433271Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"* Quantized inference function for the quantized finetuned models.","metadata":{}},{"cell_type":"code","source":"def Quantized_Inference(model, tokenizer, prompt):\n    results = []\n    \n    # Move model to GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)  # Ensure model is moved to the device\n    model.eval()  # Set model to evaluation mode\n            \n    # Tokenize prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n            \n    # Generate output\n    with torch.no_grad():\n        outputs = model.generate(**inputs, \n                            max_new_tokens=400,  # Set a maximum length for generated text\n                            #do_sample=True,  # Enable sampling\n                            #top_k=7,        # Top-k sampling\n                            #top_p=0.1,      # Top-p sampling (nucleus sampling)\n                            #num_return_sequences=1,\n                            repetition_penalty=1.2, # Penalty on repeating tokens.\n                            eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n                            pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n                            )\n        \n    # Extract generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Remove the prompt text\n    prompt_length = len(prompt)\n    generated_text = generated_text[prompt_length:]\n    \n    # Clear model from RAM\n    del model\n    torch.cuda.empty_cache()\n    \n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.437525Z","iopub.execute_input":"2024-09-21T10:29:33.437868Z","iopub.status.idle":"2024-09-21T10:29:33.448954Z","shell.execute_reply.started":"2024-09-21T10:29:33.437838Z","shell.execute_reply":"2024-09-21T10:29:33.448192Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"* Prompt creation function","metadata":{}},{"cell_type":"code","source":"# def create_prompt(question):\n#     prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the Yago2Geo knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n# Human: Where is Swansea located?\n# Generator: ```select ?geoWKT where {{ yago:Swansea geo:hasGeometry ?o.  ?o geo:asWKT ?geoWKT. }}```\n# Human: Which Greek regions have between 500000 and 1000000 inhabitants?\n# Generator: ```select ?region where {{ ?region a y2geoo:GAG_Region . ?region y2geoo:hasGAG_Population ?pop. filter(?pop < 1000000). filter(?pop > 500000). }}```\n# Human: Is Doolin to the south of Dublin?\n# Generator: ```ASK {{ <http://yago-knowledge.org/resource/Doolin> geo:hasGeometry ?o. ?o geo:asWKT ?geoWKT. <http://yago-knowledge.org/resource/Dublin> geo:hasGeometry ?o1. ?o1 geo:asWKT ?geoWKT1. FILTER(strdf:below(?geoWKT,?geoWKT1)) }}```\n# Human: {question}\n# Generator: ```\"\"\"\n#     return prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.450070Z","iopub.execute_input":"2024-09-21T10:29:33.450393Z","iopub.status.idle":"2024-09-21T10:29:33.464414Z","shell.execute_reply.started":"2024-09-21T10:29:33.450362Z","shell.execute_reply":"2024-09-21T10:29:33.463401Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# def create_prompt(question, uris):\n#     prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n# The generator is logical and creates each query by first explaining its thought process step-by-step.\n# Human: Where is Swansea located?\n# Generator: Let's think step by step. First, we want to find the location of Swansea, so we start with select ?geoWKT to specify that we need the geographic data.\n# Next, we need to identify how Swansea's location is stored in the knowledge graph. We use yago:Swansea geo:hasGeometry ?o. to find the geometric data related to Swansea.\n# Then, we need to extract the specific coordinates. We do this with ?o geo:asWKT ?geoWKT. to get the Well-Known Text (WKT) representation of Swansea's geometry.\n# Finally, we wrap these patterns in a where clause to structure our query properly. The final result: ```select ?geoWKT where {{ yago:Swansea geo:hasGeometry ?o.  ?o geo:asWKT ?geoWKT. }}```\n# Human: Which Greek regions have between 500000 and 1000000 inhabitants?\n# Generator: Let's think step by step. First, we want to find Greek regions with a population between 500,000 and 1,000,000, so we start with select ?region to specify that we need the region names.\n# Next, we need to identify which entities are Greek regions. We use ?region a y2geoo:GAG_Region to find entities classified as Greek regions.\n# Then, we need to get the population of these regions. We do this with ?region y2geoo:hasGAG_Population ?pop to find the population data associated with each region.\n# After that, we need to filter the results to only include regions with populations between 500,000 and 1,000,000. We use filter(?pop < 1000000) to exclude regions with more than 1,000,000 inhabitants and filter(?pop > 500000) to exclude regions with fewer than 500,000 inhabitants.\n# Finally, we wrap these patterns in a where clause to structure our query properly. The final result: ```select ?region where {{ ?region a y2geoo:GAG_Region . ?region y2geoo:hasGAG_Population ?pop. filter(?pop < 1000000). filter(?pop > 500000). }}```\n# Human: Is Doolin to the south of Dublin?\n# Generator: Let's think step by step. Question asks for yes/no answer: Use ASK query\n# Need to compare locations: Retrieve geometric data for both\n# geo:hasGeometry and geo:asWKT predicates for Doolin and Dublin\n# Check if one is south of the other: Use geospatial comparison function\n# FILTER with strdf:below function\n# Steps to build query:\n# a. Get Doolin's geometry: http://yago-knowledge.org/resource/Doolin\n# b. Get Dublin's geometry: http://yago-knowledge.org/resource/Dublin\n# c. Compare using FILTER and strdf:below Resulting query:\n# ```ASK {{ <http://yago-knowledge.org/resource/Doolin> geo:hasGeometry ?o. ?o geo:asWKT ?geoWKT. <http://yago-knowledge.org/resource/Dublin> geo:hasGeometry ?o1. ?o1 geo:asWKT ?geoWKT1. FILTER(strdf:below(?geoWKT,?geoWKT1)) }}```\n# Human: {question}\n# Generator: Let's think step by step.\"\"\"\n#     return prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.465929Z","iopub.execute_input":"2024-09-21T10:29:33.466326Z","iopub.status.idle":"2024-09-21T10:29:33.479083Z","shell.execute_reply.started":"2024-09-21T10:29:33.466283Z","shell.execute_reply":"2024-09-21T10:29:33.478256Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"* 6-shot prompt without CoT","metadata":{}},{"cell_type":"code","source":"def create_prompt(question, uris):\n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n\nHuman: {question}\nGenerator: ```\"\"\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.480323Z","iopub.execute_input":"2024-09-21T10:29:33.480968Z","iopub.status.idle":"2024-09-21T10:29:33.493171Z","shell.execute_reply.started":"2024-09-21T10:29:33.480924Z","shell.execute_reply":"2024-09-21T10:29:33.492299Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def create_prompt(prompt, uris):\n    prompt = f\"\"\"Human: In Breckland district, which forests are south of streams?\nGenerator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\nHuman: How many streams intersect with lakes?\nGenerator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\nHuman: Which Municipalities are on Thessaly's border?\nGenerator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\nHuman: Which is the largest island in Ireland?\nGenerator: ```select distinct ?x (strdf:area(?lWKT) as ?area) where {{ yago:Republic_of_Ireland geo:hasGeometry ?geom . ?geom geo:asWKT ?mWKT . ?lake a y2geoo:OSM_island . ?lake geo:hasGeometry ?geol . ?geol geo:asWKT ?lWKT . FILTER (geof:sfContains(?mWKT, ?lWKT)) }} ORDER BY (?area) LIMIT 1```\nHuman: Is Crete south of Thessaly?\nGenerator: ```ASK {{ <http://yago-knowledge.org/resource/Crete> geo:hasGeometry ?geo1 . <http://yago-knowledge.org/resource/Thessaly> geo:hasGeometry ?geo2 . ?geo1 geo:asWKT ?geoWKT1 . ?geo2 geo:asWKT ?geoWKT2 . FILTER(strdf:below(?geoWKT1, ?geoWKT2)) }}```\nHuman: What is the population of Northern Ireland?\nGenerator: ```SELECT (xsd:integer (?population) as ?pop) WHERE {{ yago:Northern_Ireland yago:'hasPopulation ?population. }}```\nHuman: {prompt}\nGenerator: ```\"\"\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.494444Z","iopub.execute_input":"2024-09-21T10:29:33.495095Z","iopub.status.idle":"2024-09-21T10:29:33.504336Z","shell.execute_reply.started":"2024-09-21T10:29:33.495053Z","shell.execute_reply":"2024-09-21T10:29:33.503493Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"* Same CoT prompt but designed for automatic uri-injection. ","metadata":{}},{"cell_type":"code","source":"# def create_prompt(question, uris):\n#     prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question.\n# The generator may be provided with a list of URIs. Some of these URIs are relevant, while others are not. The generator must carefully identify and use only the correct URIs when they are provided. If the correct URIs are not available, the generator will rely on its understanding to construct the appropriate query.\n# The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n# The generator is logical and creates each query by first explaining its thought process step-by-step.\n\n# Human: Where is Swansea located?\n# Provided URIs: yago:Swansea,y2geoo:OS_UnitaryAuthority\n# Generator: Let's think step by step. First, we want to find the location of Swansea, so we start with select ?geoWKT to specify that we need the geographic data.\n# Next, we need to identify how Swansea's location is stored in the knowledge graph. We use yago:Swansea geo:hasGeometry ?o. to find the geometric data related to Swansea.\n# Then, we need to extract the specific coordinates. We do this with ?o geo:asWKT ?geoWKT. to get the Well-Known Text (WKT) representation of Swansea's geometry.\n# Finally, we wrap these patterns in a where clause to structure our query properly. The final result: ```select ?geoWKT where {{ yago:Swansea geo:hasGeometry ?o.  ?o geo:asWKT ?geoWKT. }}```\n\n# Human: Which Greek regions have between 500000 and 1000000 inhabitants?\n# Provided URIs: yago:Greece\n# Generator: Let's think step by step. First, we want to find Greek regions with a population between 500,000 and 1,000,000, so we start with select ?region to specify that we need the region names.\n# Next, we need to identify which entities are Greek regions. We use ?region a y2geoo:GAG_Region to find entities classified as Greek regions. We do not need the provided URI of Greece.\n# Then, we need to get the population of these regions. We do this with ?region y2geoo:hasGAG_Population ?pop to find the population data associated with each region.\n# After that, we need to filter the results to only include regions with populations between 500,000 and 1,000,000. We use filter(?pop < 1000000) to exclude regions with more than 1,000,000 inhabitants and filter(?pop > 500000) to exclude regions with fewer than 500,000 inhabitants.\n# Finally, we wrap these patterns in a where clause to structure our query properly. The final result: ```select ?region where {{ ?region a y2geoo:GAG_Region . ?region y2geoo:hasGAG_Population ?pop. filter(?pop < 1000000). filter(?pop > 500000). }}```\n\n# Human: Is Doolin to the south of Dublin?\n# Provided URIs: yago:Doolin,yago:Dublin\n# Generator: Let's think step by step. Question asks for yes/no answer: Use ASK query\n# Need to compare locations: Retrieve geometric data for both\n# geo:hasGeometry and geo:asWKT predicates for Doolin and Dublin\n# Check if one is south of the other: Use geospatial comparison function\n# FILTER with strdf:below function\n# Steps to build query:\n# a. Get Doolin's and Dublin's geometry from the provided URIs: yago:Doolin and yago:Dublin\n# b. Compare using FILTER and strdf:below \n# The final result:\n# ```ASK {{ <http://yago-knowledge.org/resource/Doolin> geo:hasGeometry ?o. ?o geo:asWKT ?geoWKT. <http://yago-knowledge.org/resource/Dublin> geo:hasGeometry ?o1. ?o1 geo:asWKT ?geoWKT1. FILTER(strdf:below(?geoWKT,?geoWKT1)) }}```\n\n# Human: {question}\n# Provided URIs: {uris}\n# Generator: Let's think step by step.\"\"\"\n#     return prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.505629Z","iopub.execute_input":"2024-09-21T10:29:33.505916Z","iopub.status.idle":"2024-09-21T10:29:33.521428Z","shell.execute_reply.started":"2024-09-21T10:29:33.505885Z","shell.execute_reply":"2024-09-21T10:29:33.520535Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"* Query cleanup function","metadata":{}},{"cell_type":"code","source":"import re\n\ndef query_cleanup(results):\n    # Search for the pattern in the text\n#     match = re.search(r'```(.*?)```', results, re.DOTALL)\n    match = re.search(r'(.*?)```', results, re.DOTALL)\n    query = results\n    # If a match is found, return the matched text\n    if match:\n        query = match.group(1).strip()\n    \n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.522620Z","iopub.execute_input":"2024-09-21T10:29:33.522987Z","iopub.status.idle":"2024-09-21T10:29:33.535459Z","shell.execute_reply.started":"2024-09-21T10:29:33.522945Z","shell.execute_reply":"2024-09-21T10:29:33.534664Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Endpoint inference functions","metadata":{}},{"cell_type":"markdown","source":"* Gost materialization.","metadata":{}},{"cell_type":"code","source":"def gost_materialize_query(query: str):\n    data = {\n        \"query\": query\n    }\n\n    headers = {\n        'Content-Type': 'application/json'\n    }\n\n    response = requests.post(\"http://195.134.71.116:9090/materialize-api\", headers=headers, data=json.dumps(data))\n    \n    if response.status_code == 200:\n        return response.text\n    else:\n        print(\"Materialize failed:\", response.text)\n        return (query)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.536459Z","iopub.execute_input":"2024-09-21T10:29:33.536763Z","iopub.status.idle":"2024-09-21T10:29:33.549169Z","shell.execute_reply.started":"2024-09-21T10:29:33.536719Z","shell.execute_reply":"2024-09-21T10:29:33.548363Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"* Query fomatting function. This adds the correct prefixes and fixes some endpoint issues with regex.","metadata":{}},{"cell_type":"code","source":"def format_query(query):\n    PREFIXES = \"\"\"PREFIX geo: <http://www.opengis.net/ont/geosparql#>\nPREFIX geof: <http://www.opengis.net/def/function/geosparql/>\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nPREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\nPREFIX yago: <http://yago-knowledge.org/resource/>\nPREFIX y2geor: <http://kr.di.uoa.gr/yago2geo/resource/>\nPREFIX y2geoo: <http://kr.di.uoa.gr/yago2geo/ontology/>\nPREFIX strdf: <http://strdf.di.uoa.gr/ontology#>\nPREFIX uom: <http://www.opengis.net/def/uom/OGC/1.0/>\nPREFIX owl: <http://www.w3.org/2002/07/owl#>\"\"\"\n    \n    query = PREFIXES + ' ' + query\n    \n    query = query.replace('strdf:within', 'geof:sfWithin')\n    query = query.replace('strdf:contains', 'geof:sfContains')\n    query = query.replace('strdf:overlaps', 'geof:sfOverlaps')\n    query = query.replace('strdf:distance', 'geof:sfDistance')\n    \n    # Use regex to find and replace strdf:buffer patterns\n    query = re.sub(r'strdf:buffer\\((\\?\\w+),\\s*\\d+,\\s*uom:\\w+\\)', r'\\1', query)\n    \n    return query","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.550244Z","iopub.execute_input":"2024-09-21T10:29:33.550522Z","iopub.status.idle":"2024-09-21T10:29:33.560031Z","shell.execute_reply.started":"2024-09-21T10:29:33.550491Z","shell.execute_reply":"2024-09-21T10:29:33.559210Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"* Endpoint request function","metadata":{}},{"cell_type":"code","source":"import requests\nimport pandas as pd\nfrom io import StringIO\n\ndef graphdb_send_request(query, endpoint_url=\"http://88.197.53.158:7200/repositories/da4dte_final\", accept_format='application/sparql-results+json'):\n    \"\"\"\n    Sends a SPARQL query to a GraphDB endpoint.\n\n    :param query: SPARQL query to be sent\n    :param endpoint_url: URL of the GraphDB SPARQL endpoint\n    :param accept_format: Desired response format (default is JSON)\n    :return: Response from the endpoint\n    \"\"\"\n    # Format the query, this means add the correct prefixes and fix some endpoint issues with regex.\n    query = format_query(query)\n    original_query = query\n    query = gost_materialize_query(query)\n    \n    headers = {\n        'Accept': accept_format,\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n\n    data = {\n        'query': query\n    }\n    \n    try:\n        response = requests.post(endpoint_url, headers=headers, data=data, auth=requests.auth.HTTPBasicAuth('admin', 'p@sx@'))\n\n        if response.status_code == 200:\n            if accept_format == 'application/sparql-results+json':\n#                 print(response.json())\n                json_response = response.json()\n                return convert_json_to_csv(json_response)\n            else:\n#                 print(response.text)\n                return response.text\n        else:\n            response.raise_for_status()\n    except requests.exceptions.HTTPError as err:\n        print(\"HTTP error (most likely invalid query)\")\n        #print(query)\n        #print(err)\n    except Exception as err:\n        print(err)\n        print(\"Endpoint error ENDPOINT DOWN\")\n        \ndef convert_json_to_csv(json_data):\n    \"\"\"\n    Converts JSON data to CSV format.\n\n    :param json_data: JSON data to be converted\n    :return: CSV formatted data as a string\n    \"\"\"\n    if 'boolean' in json_data:\n        # Handling boolean result\n        headers = ['value']\n        rows = [[json_data['boolean']]]\n    else:\n        # Extracting header and rows from JSON response\n        headers = json_data['head']['vars']\n        rows = [{var: result.get(var, {}).get('value', '') for var in headers} for result in json_data['results']['bindings']]\n    \n    # Creating DataFrame and converting to CSV\n    df = pd.DataFrame(rows, columns=headers)\n    csv_output = StringIO()\n    df.to_csv(csv_output, index=False)\n    \n    return csv_output.getvalue()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.561054Z","iopub.execute_input":"2024-09-21T10:29:33.561378Z","iopub.status.idle":"2024-09-21T10:29:33.576062Z","shell.execute_reply.started":"2024-09-21T10:29:33.561346Z","shell.execute_reply":"2024-09-21T10:29:33.575277Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# # ------------------------\n# # ----- Quantization -----\n# # ------------------------\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True, \n#     bnb_4bit_use_double_quant=True, \n#     bnb_4bit_quant_type=\"nf4\", \n#     bnb_4bit_compute_dtype=torch.bfloat16\n# )\n \n# # -----------------\n# # ----- Model -----\n# # -----------------\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL.model_id,\n#     device_map=\"auto\",\n#     torch_dtype=torch.bfloat16,\n#     quantization_config=bnb_config\n# )\n# for param in model.parameters():\n#     param.requires_grad = False\n#     if param.ndim ==1:\n#         param.data = param.data.to(torch.float32)\n    \n# model.gradient_checkpointing_enable()\n# model.enable_input_require_grads()\n\n# # compute_metrics(None, subset_test_dataset)\n# compute_metrics(None, dataset['test'])\n\n# # ----------------\n# # ----- LoRA -----\n# # ----------------\n# peft_config = LoraConfig(\n#     r=LORA_R,\n#     lora_alpha=LORA_ALPHA,\n#     lora_dropout=0.1,\n#     # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n#     bias = 'none',\n#     # modules_to_save = ['lm_head', 'embed_tokens'],\n#     task_type=\"CAUSAL_LM\"\n# )\n# model = get_peft_model(model, peft_config)\n \n# # --------------------\n# # ----- Training -----\n# # --------------------\n# args = TrainingArguments(\n#     output_dir=\"outputs\",                   # directory to save and repository id\n#     # Training length\n#     max_steps=MAX_STEPS,\n#     num_train_epochs=EPOCHS,\n#     # Important for VRAM\n#     per_device_train_batch_size=6,          # batch size per device during training\n#     gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n#     # Other\n#     gradient_checkpointing=True,            # use gradient checkpointing to save memory\n#     optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n#     bf16=True,                              # use bfloat16 precision\n#     tf32=True,                              # use tf32 precision\n#     max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n#     warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n#     # lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n#     learning_rate=0.0002,\n#     # Logging\n#     logging_dir='logs',\n#     logging_steps=10,\n#     # Evaluation\n#     evaluation_strategy=\"steps\",            # evaluate every 'eval_steps'\n#     eval_steps=500,                         # evaluation step frequency\n#     save_steps=500,                         # save checkpoint every 'save_steps'\n#     load_best_model_at_end=True,            # load the best model at the end of training\n#     metric_for_best_model=\"correct\",        # metric to compare the best model\n#     greater_is_better=True\n# )\n\n# trainer = SFTTrainer(\n#     model=model,\n#     args=args,\n#     train_dataset=dataset['train'],\n#     eval_dataset=subset_test_dataset,\n#     dataset_text_field=\"input\",\n#     compute_metrics=compute_metrics,\n#     tokenizer=tokenizer,\n#     dataset_kwargs={\n#         \"add_special_tokens\": False,  # We template with special tokens\n#         \"append_concat_token\": False, # No need to add additional separator token\n#     }\n# )\n\n# # trainer.evaluate()\n\n# trainer.train(resume_from_checkpoint=True)\n\n# print(\"FULL EVALUATION\")\n# compute_metrics(None, dataset['test'])\n\n# # ---------------------------------\n# # ----- Upload to HuggingFace -----\n# # ---------------------------------\n# model.push_to_hub(\"SKefalidis/\" + MODEL.model_string + \"-\" + DATASET.dataset_string,\n#                   commit_message=str(MAX_STEPS) + \"-steps-\" + str(EPOCHS) + \"-epochs-4bit-r\" + str(LORA_R),\n#                   private=True)\n# tokenizer.push_to_hub(\"SKefalidis/\" + MODEL.model_string + \"-\" + DATASET.dataset_string,\n#                   commit_message=str(MAX_STEPS) + \"-steps-\" + str(EPOCHS) + \"-epochs-4bit-r\" + str(LORA_R),\n#                   private=True) ","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.577228Z","iopub.execute_input":"2024-09-21T10:29:33.577539Z","iopub.status.idle":"2024-09-21T10:29:33.591188Z","shell.execute_reply.started":"2024-09-21T10:29:33.577510Z","shell.execute_reply":"2024-09-21T10:29:33.590456Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# model = PeftModel.from_pretrained(base_model, \"SKefalidis/Mistral-7B-v2-queries\") ","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.592348Z","iopub.execute_input":"2024-09-21T10:29:33.592736Z","iopub.status.idle":"2024-09-21T10:29:33.603931Z","shell.execute_reply.started":"2024-09-21T10:29:33.592696Z","shell.execute_reply":"2024-09-21T10:29:33.603073Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# def __get_mistral_tokenizer() -> AutoTokenizer:\n#     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n#     tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n#     tokenizer.padding_side = 'right'\n#     return tokenizer ","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.604957Z","iopub.execute_input":"2024-09-21T10:29:33.605288Z","iopub.status.idle":"2024-09-21T10:29:33.618198Z","shell.execute_reply.started":"2024-09-21T10:29:33.605254Z","shell.execute_reply":"2024-09-21T10:29:33.617290Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"* General pipeline handler.","metadata":{}},{"cell_type":"code","source":"# Handler, takes a question as input and returns the geometric data that answers it.\ndef ask_pipeline(question, uris=None):\n    # Generate appropriate prompt.\n    prompt = create_prompt(question, uris)\n    # Run inference on the LLM.\n#     generated_query = run_inference(model, tokenizer, prompt)\n    generated_query = Quantized_Inference(model, tokenizer, prompt)\n#     print(generated_query)\n#     generated_query = run_chat_inference(model, tokenizer, prompt, question)\n    # Extract the query alone from the results.\n    cleaned_query = query_cleanup(generated_query)\n    print(\"----\")\n    print(cleaned_query)\n    print(\"----\")\n    # Send the query to the endpoint.\n    results = graphdb_send_request(cleaned_query)\n    \n    # TO DO: visualize the results instead of just printing them.\n    #print (results)\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.619272Z","iopub.execute_input":"2024-09-21T10:29:33.619568Z","iopub.status.idle":"2024-09-21T10:29:33.628914Z","shell.execute_reply.started":"2024-09-21T10:29:33.619537Z","shell.execute_reply":"2024-09-21T10:29:33.628104Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy evaluation ","metadata":{}},{"cell_type":"markdown","source":"* Custom comparisson function","metadata":{}},{"cell_type":"code","source":"def csv_to_columns(csv_data):\n    rows = csv_data.strip().split('\\n')\n    data_rows = [row.split(',') for row in rows[1:]]  # Skip the header row\n    columns = list(zip(*data_rows))  # Transpose rows to columns\n    return columns\n\ndef compare_csv_columns(csv1, csv2):\n    columns1 = csv_to_columns(csv1)\n    columns2 = csv_to_columns(csv2)\n    \n    set_columns1 = {tuple(col) for col in columns1}\n    set_columns2 = {tuple(col) for col in columns2}\n    \n    #return not set_columns1.isdisjoint(set_columns2)\n    \n    common_columns = set_columns1.intersection(set_columns2)\n    \n    if common_columns:\n        return True\n    else:\n        return False","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.629998Z","iopub.execute_input":"2024-09-21T10:29:33.630300Z","iopub.status.idle":"2024-09-21T10:29:33.640629Z","shell.execute_reply.started":"2024-09-21T10:29:33.630269Z","shell.execute_reply":"2024-09-21T10:29:33.639828Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"* The evaluation on the 100 question dataset.","metadata":{}},{"cell_type":"code","source":"detailed_comp = 0\n\ngt_results = []\ngen_results = []\n\ni = 0\nfor key in original_dataset:\n    print(i)\n    i+=1\n#     if i < 12:\n#         continue\n    #2 at 12\n    query = original_dataset[key]['Query']\n    \n    gt_result = graphdb_send_request(query)\n    gt_results.append(gt_result)\n    \n    question = original_dataset[key]['Question']\n#     uris = original_dataset[key]['Gen_URI']\n#     uris_string = ','.join(uris)\n    uris_string = ''\n    \n    gen_result = ask_pipeline(question, uris_string)\n    gen_results.append(gen_result)\n    \n    if gen_result and gt_result:\n        comparisson = compare_csv_columns(gt_result, gen_result)\n        if comparisson == True:  \n            detailed_comp += 1\n        print(detailed_comp)\n        \nprint (detailed_comp/100)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T10:29:33.641633Z","iopub.execute_input":"2024-09-21T10:29:33.641883Z","iopub.status.idle":"2024-09-21T10:29:41.479531Z","shell.execute_reply.started":"2024-09-21T10:29:33.641855Z","shell.execute_reply":"2024-09-21T10:29:41.478277Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     uris = original_dataset[key]['Gen_URI']\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     uris_string = ','.join(uris)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     uris_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 23\u001b[0m     gen_result \u001b[38;5;241m=\u001b[39m \u001b[43mask_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     gen_results\u001b[38;5;241m.\u001b[39mappend(gen_result)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gen_result \u001b[38;5;129;01mand\u001b[39;00m gt_result:\n","Cell \u001b[0;32mIn[47], line 7\u001b[0m, in \u001b[0;36mask_pipeline\u001b[0;34m(question, uris)\u001b[0m\n\u001b[1;32m      4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m create_prompt(question, uris)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Run inference on the LLM.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     generated_query = run_inference(model, tokenizer, prompt)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     generated_query \u001b[38;5;241m=\u001b[39m \u001b[43mQuantized_Inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     print(generated_query)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     generated_query = run_chat_inference(model, tokenizer, prompt, question)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Extract the query alone from the results.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     cleaned_query \u001b[38;5;241m=\u001b[39m query_cleanup(generated_query)\n","Cell \u001b[0;32mIn[34], line 10\u001b[0m, in \u001b[0;36mQuantized_Inference\u001b[0;34m(model, tokenizer, prompt)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Tokenize prompt\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate output\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3049\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3047\u001b[0m all_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3049\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either `text` or `text_target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3051\u001b[0m     \u001b[38;5;66;03m# The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the\u001b[39;00m\n\u001b[1;32m   3052\u001b[0m     \u001b[38;5;66;03m# input mode in this case.\u001b[39;00m\n\u001b[1;32m   3053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n","\u001b[0;31mValueError\u001b[0m: You need to specify either `text` or `text_target`."],"ename":"ValueError","evalue":"You need to specify either `text` or `text_target`.","output_type":"error"}]}]}