{"cells":[{"cell_type":"markdown","metadata":{},"source":["Define the model and the inference function"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:32:21.789789Z","iopub.status.busy":"2024-07-23T17:32:21.789509Z","iopub.status.idle":"2024-07-23T17:32:25.592473Z","shell.execute_reply":"2024-07-23T17:32:25.591445Z","shell.execute_reply.started":"2024-07-23T17:32:21.789756Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def run_inference(model, tokenizer, prompt):\n","    results = []\n","    \n","    if tokenizer == None:\n","        # Generate output\n","        with torch.no_grad():\n","            outputs = model(prompt)\n","            \n","        # Decode and print output\n","        #print(\"Prompt:\", prompt)\n","        #print(\"Generated text:\" + outputs + \"\\n\")\n","        results.append(\"Generated text:\" + outputs)\n","    else:\n","        # Move model to GPU\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","        model.eval()  # Set model to evaluation mode\n","            \n","        # Tokenize prompt\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","            \n","        # Generate output\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, \n","                            max_length=1800,  # Set a maximum length for generated text\n","                            #do_sample=True,  # Enable sampling\n","                            #top_k=7,        # Top-k sampling\n","                            #top_p=0.1,      # Top-p sampling (nucleus sampling)\n","                            #num_return_sequences=1,\n","                            #repetition_penalty=1, # No penalty for instruction tuned models.\n","                            repetition_penalty=1.2, # Penalty on repeating tokens.\n","                            eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n","                            pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n","                            )\n","        \n","        # Extract generated text\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Remove the prompt text\n","        prompt_length = len(prompt)\n","        generated_text = generated_text[prompt_length:]\n","\n","        # Decode and print output\n","        #print(\"Prompt:\", prompt)\n","        #print(generated_text)\n","        results.append(generated_text)\n","    \n","    # Clear model from RAM\n","    del model\n","    torch.cuda.empty_cache()\n","    \n","    return results"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:32:25.594684Z","iopub.status.busy":"2024-07-23T17:32:25.594278Z","iopub.status.idle":"2024-07-23T17:32:25.602871Z","shell.execute_reply":"2024-07-23T17:32:25.602004Z","shell.execute_reply.started":"2024-07-23T17:32:25.594656Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def Quantized_Inference(model, tokenizer, prompt):\n","    results = []\n","    \n","    # Move model to GPU\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()  # Set model to evaluation mode\n","            \n","    # Tokenize prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","            \n","    # Generate output\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, \n","                            max_new_tokens=350,  # Set a maximum length for generated text\n","                            #do_sample=True,  # Enable sampling\n","                            #top_k=7,        # Top-k sampling\n","                            #top_p=0.1,      # Top-p sampling (nucleus sampling)\n","                            #num_return_sequences=1,\n","                            repetition_penalty=1.2, # Penalty on repeating tokens.\n","                            eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n","                            pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n","                            )\n","        \n","    # Extract generated text\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Remove the prompt text\n","    prompt_length = len(prompt)\n","    generated_text = generated_text[prompt_length:]\n","\n","    # Decode and print output\n","    #print(\"Prompt:\", prompt)\n","    #print(\"Generated text:\" + generated_text + \"\\n\")\n","    results.append(\"Generated text:\" + generated_text)\n","    \n","    # Clear model from RAM\n","    del model\n","    torch.cuda.empty_cache()\n","    \n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["Define two functions for extracting URIs from the dataset queries, one with direct extraction and the other with the ability to expand prefixes."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:32:25.604289Z","iopub.status.busy":"2024-07-23T17:32:25.603990Z","iopub.status.idle":"2024-07-23T17:32:25.618122Z","shell.execute_reply":"2024-07-23T17:32:25.617023Z","shell.execute_reply.started":"2024-07-23T17:32:25.604267Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def extract_uris(query):\n","    # Regular expression to match both fully expanded and prefixed URIs\n","    uri_pattern = r'<([^>]+)>|(\\b[a-zA-Z0-9_]+):([a-zA-Z0-9_]+)'\n","    \n","    uris = []\n","    matches = re.findall(uri_pattern, query)\n","    for match in matches:\n","        if match[0]:  # Fully expanded URI\n","            uris.append(match[0])\n","        else:  # Prefixed URI\n","            uris.append(f\"{match[1]}:{match[2]}\")\n","    return uris\n","\n","def expand_uris(query, prefix_dict):\n","    prefixed_pattern = r'(\\b[a-zA-Z0-9_]+):([a-zA-Z0-9_]+)'\n","    expanded_pattern = r'<([^>]+)>'\n","    \n","    expanded_uris = []\n","    \n","    # Find and expand prefixed URIs\n","    matches = re.findall(prefixed_pattern, query)\n","    for prefix, suffix in matches:\n","        if prefix in prefix_dict:\n","            expanded_uris.append(f\"{prefix_dict[prefix]}{suffix}\")\n","        else:\n","            expanded_uris.append(f\"{prefix}:{suffix}\")\n","    \n","    # Find and add already expanded URIs\n","    matches = re.findall(expanded_pattern, query)\n","    for uri in matches:\n","        expanded_uris.append(uri)\n","    \n","    return expanded_uris\n","\n","prefix_dict = {\n","    'geo': 'http://www.opengis.net/ont/geosparql#',\n","    'osm': 'http://www.openstreetmap.org/ontology#',\n","    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n","    'geof': 'http://www.opengis.net/def/function/geosparql/',\n","    'uom': 'http://www.opengis.net/def/uom/OGC/1.0/'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:32:25.621347Z","iopub.status.busy":"2024-07-23T17:32:25.620563Z","iopub.status.idle":"2024-07-23T17:32:25.980948Z","shell.execute_reply":"2024-07-23T17:32:25.979983Z","shell.execute_reply.started":"2024-07-23T17:32:25.621321Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:32:25.982434Z","iopub.status.busy":"2024-07-23T17:32:25.982157Z","iopub.status.idle":"2024-07-23T17:32:55.479666Z","shell.execute_reply":"2024-07-23T17:32:55.478420Z","shell.execute_reply.started":"2024-07-23T17:32:25.982410Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:32:55.481683Z","iopub.status.busy":"2024-07-23T17:32:55.481343Z","iopub.status.idle":"2024-07-23T17:32:59.299645Z","shell.execute_reply":"2024-07-23T17:32:59.295883Z","shell.execute_reply.started":"2024-07-23T17:32:55.481652Z"},"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n","model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", load_in_8bit=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.300497Z","iopub.status.idle":"2024-07-23T17:32:59.300887Z","shell.execute_reply":"2024-07-23T17:32:59.300710Z","shell.execute_reply.started":"2024-07-23T17:32:59.300695Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","\n","model = AutoModelForCausalLM.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\", torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.302158Z","iopub.status.idle":"2024-07-23T17:32:59.302472Z","shell.execute_reply":"2024-07-23T17:32:59.302332Z","shell.execute_reply.started":"2024-07-23T17:32:59.302320Z"},"trusted":true},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n","model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", torch_dtype=torch.float32)"]},{"cell_type":"markdown","metadata":{},"source":["Run inference on the entire dataset and store them for evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.304205Z","iopub.status.idle":"2024-07-23T17:32:59.304529Z","shell.execute_reply":"2024-07-23T17:32:59.304377Z","shell.execute_reply.started":"2024-07-23T17:32:59.304364Z"},"trusted":true},"outputs":[],"source":["# Run inference and cleanup.\n","def generate_query(model, tokenizer, user_prompt):\n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: {user_prompt}\n","Generator: ```\"\"\"\n","    \n","    results = run_inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.306449Z","iopub.status.idle":"2024-07-23T17:32:59.306803Z","shell.execute_reply":"2024-07-23T17:32:59.306651Z","shell.execute_reply.started":"2024-07-23T17:32:59.306636Z"},"trusted":true},"outputs":[],"source":["# Run inference and cleanup.\n","def generate_query(model, tokenizer, user_prompt):\n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: In Breckland district, which forests are south of streams?\n","Generator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\n","Human: How many streams intersect with lakes?\n","Generator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\n","Human: Which Municipalities are on Thessaly's border?\n","Generator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\n","Human: {user_prompt}\n","Generator: ```\"\"\"\n","    \n","    results = run_inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.308549Z","iopub.status.idle":"2024-07-23T17:32:59.308936Z","shell.execute_reply":"2024-07-23T17:32:59.308750Z","shell.execute_reply.started":"2024-07-23T17:32:59.308730Z"},"trusted":true},"outputs":[],"source":["# Run llama 3 inference and cleanup.\n","def generate_quantized_query(model, tokenizer, user_prompt):\n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: In Breckland district, which forests are south of streams?\n","Generator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\n","Human: How many streams intersect with lakes?\n","Generator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\n","Human: Which Municipalities are on Thessaly's border?\n","Generator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\n","Human: {user_prompt}\n","Generator: ```\"\"\"\n","\n","    results = Quantized_Inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"markdown","metadata":{},"source":["Save the results as a json file of the same format as the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.310217Z","iopub.status.idle":"2024-07-23T17:32:59.310535Z","shell.execute_reply":"2024-07-23T17:32:59.310388Z","shell.execute_reply.started":"2024-07-23T17:32:59.310375Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import json\n","\n","with open('/kaggle/input/geoqa200/200_Sub_Dataset.json', 'r') as file:\n","    original_dataset = json.load(file)\n","\n","# Create a new dataset with questions and generated queries\n","new_dataset = {}\n","i = 0\n","for key, item in original_dataset.items():\n","    i += 1\n","    question = item['Question']\n","    query = generate_query(model, tokenizer, question)\n","    new_dataset[key] = {'Question': question, 'Query': query}\n","    print (f\"{i}/{len(original_dataset.items())}\")\n","    \n","# Save the new dataset to a JSON file\n","with open('/kaggle/working/generated_dataset1.json', 'w') as file:\n","    json.dump(new_dataset, file, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.311602Z","iopub.status.idle":"2024-07-23T17:32:59.311997Z","shell.execute_reply":"2024-07-23T17:32:59.311781Z","shell.execute_reply.started":"2024-07-23T17:32:59.311766Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import json\n","\n","with open('/kaggle/input/80geoqa/80_Sub_Dataset.json', 'r') as file:\n","    original_dataset = json.load(file)\n","\n","# Create a new dataset with questions and generated queries\n","new_dataset = {}\n","i = 0\n","for key, item in original_dataset.items():\n","    i += 1\n","    question = item['Question']\n","    query = generate_quantized_query(model, tokenizer, question)\n","    new_dataset[key] = {'Question': question, 'Query': query}\n","    print (f\"{i}/{len(original_dataset.items())}\")\n","\n","# Save the new dataset to a JSON file\n","with open('/kaggle/working/generated_dataset1.json', 'w') as file:\n","    json.dump(new_dataset, file, indent=4)"]},{"cell_type":"markdown","metadata":{},"source":["Alternatively, run inference and URI extraction and injection into the prompt."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.315235Z","iopub.status.idle":"2024-07-23T17:32:59.315742Z","shell.execute_reply":"2024-07-23T17:32:59.315523Z","shell.execute_reply.started":"2024-07-23T17:32:59.315490Z"},"trusted":true},"outputs":[],"source":["# Run inference and cleanup.\n","def generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n","    # Direct extraction, NOTE: try expanded extraction.\n","    uris = extract_uris(gt_query)\n","    \n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: {user_prompt}\n","The generator must use these URIs to answer the question: {uris}\n","Generator: ```\"\"\"\n","    \n","    results = run_inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"markdown","metadata":{},"source":["Try as well with supplying the model with GeoSPARQL ontology description."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.316919Z","iopub.status.idle":"2024-07-23T17:32:59.317359Z","shell.execute_reply":"2024-07-23T17:32:59.317149Z","shell.execute_reply.started":"2024-07-23T17:32:59.317130Z"},"trusted":true},"outputs":[],"source":["# Run inference and cleanup.\n","def generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n","    # Direct extraction, NOTE: try expanded extraction.\n","    uris = extract_uris(gt_query)\n","    \n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","The resulting query may have to be in GeoSPARQL. The GeoSPARQL ontology is defined by:\n","URI: http://www.opengis.net/ont/geosparql\n","Classes: Feature, Feature Collection, Geometry, Geometry Collection, Spatial Object, Spatial Object Collection\n","Object Properties: default geometry, contains, covered by, covers, disjoint, equals, inside, meet, overlap, has area, has bounding box, has centroid, has default geometry, has geometry, has length, has perimeter length, has size, has spatial accuracy, has spatial resolution, has volume, disconnected, externally connected, equals, non-tangential proper part, non-tangential proper part inverse, partially overlapping, tangential proper part, tangential proper part inverse, contains, crosses, disjoint, equals, intersects, overlaps, touches, within\n","Datatype Properties: as DGGS, as GML, as GeoJSON, as KML, as WKT, coordinate dimension, dimension, has area in square meters, has length in meters, has perimeter length in meters, has metric size, has spatial accuracy in meters, has spatial resolution in meters, has volume in cubic meters, has serialization, is empty, is simple, spatial dimension\n","Human: {user_prompt}\n","The generator must use these URIs to answer the question: {uris}\n","Generator: ```\"\"\"\n","    \n","    results = run_inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"markdown","metadata":{},"source":["Try few shot training."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.319268Z","iopub.status.idle":"2024-07-23T17:32:59.319729Z","shell.execute_reply":"2024-07-23T17:32:59.319516Z","shell.execute_reply.started":"2024-07-23T17:32:59.319496Z"},"trusted":true},"outputs":[],"source":["# Run inference and cleanup.\n","def generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n","    # Direct extraction, NOTE: try expanded extraction.\n","    uris = extract_uris(gt_query)\n","    \n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: In Breckland district, which forests are south of streams?\n","The generator must use these URIs to answer the question: ['yago:Breckland_District', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within', 'strdf:within', 'strdf:below']\n","Generator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\n","Human: How many streams intersect with lakes?\n","The generator must use these URIs to answer the question: ['rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects']\n","Generator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\n","Human: Which Municipalities are on Thessaly's border?\n","The generator must use these URIs to answer the question: ['yago:Thessaly', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'strdf:touches']\n","Generator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\n","Human: {user_prompt}\n","The generator must use these URIs to answer the question: {uris}\n","Generator: ```\"\"\"\n","    \n","    results = run_inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"markdown","metadata":{},"source":["Let's try with even more examples."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.321728Z","iopub.status.idle":"2024-07-23T17:32:59.322098Z","shell.execute_reply":"2024-07-23T17:32:59.321947Z","shell.execute_reply.started":"2024-07-23T17:32:59.321931Z"},"trusted":true},"outputs":[],"source":["# Run inference and cleanup.\n","def generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n","    # Direct extraction, NOTE: try expanded extraction.\n","    uris = extract_uris(gt_query)\n","    \n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: In Breckland district, which forests are south of streams?\n","The generator must use these URIs to answer the question: ['yago:Breckland_District', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within', 'strdf:within', 'strdf:below']\n","Generator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\n","Human: How many streams intersect with lakes?\n","The generator must use these URIs to answer the question: ['rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects']\n","Generator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\n","Human: Which Municipalities are on Thessaly's border?\n","The generator must use these URIs to answer the question: ['yago:Thessaly', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'strdf:touches']\n","Generator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\n","Human: Which is the largest island in Ireland?\n","The generator must use these URIs to answer the question: ['strdf:area', 'yago:Republic_of_Ireland', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_island', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains']\n","Generator: ```select distinct ?x (strdf:area(?lWKT) as ?area) where {{ yago:Republic_of_Ireland geo:hasGeometry ?geom . ?geom geo:asWKT ?mWKT . ?lake a y2geoo:OSM_island . ?lake geo:hasGeometry ?geol . ?geol geo:asWKT ?lWKT . FILTER (geof:sfContains(?mWKT, ?lWKT)) }} ORDER BY (?area) LIMIT 1```\n","Human: Is Crete south of Thessaly?\n","The generator must use these URIs to answer the question: ['http://yago-knowledge.org/resource/Crete', 'geo:hasGeometry', 'http://yago-knowledge.org/resource/Thessaly', 'geo:hasGeometry', 'geo:asWKT', 'geo:asWKT', 'strdf:below']\n","Generator: ```ASK {{ <http://yago-knowledge.org/resource/Crete> geo:hasGeometry ?geo1 . <http://yago-knowledge.org/resource/Thessaly> geo:hasGeometry ?geo2 . ?geo1 geo:asWKT ?geoWKT1 . ?geo2 geo:asWKT ?geoWKT2 . FILTER(strdf:below(?geoWKT1, ?geoWKT2)) }}```\n","Human: What is the population of Northern Ireland?\n","The generator must use these URIs to answer the question: ['xsd:integer', 'yago:Northern_Ireland', 'yago:hasPopulation']\n","Generator: ```SELECT (xsd:integer (?population) as ?pop) WHERE {{ yago:Northern_Ireland yago:hasPopulation ?population. }}```\n","Human: {user_prompt}\n","The generator must use these URIs to answer the question: {uris}\n","Generator: ```\"\"\"\n","    \n","    results = run_inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"markdown","metadata":{},"source":["How is it without URI injection?"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.323439Z","iopub.status.idle":"2024-07-23T17:32:59.323915Z","shell.execute_reply":"2024-07-23T17:32:59.323682Z","shell.execute_reply.started":"2024-07-23T17:32:59.323664Z"},"trusted":true},"outputs":[],"source":["# Run inference and cleanup.\n","def generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","Human: In Breckland district, which forests are south of streams?\n","Generator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\n","Human: How many streams intersect with lakes?\n","Generator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\n","Human: Which Municipalities are on Thessaly's border?\n","Generator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\n","Human: {user_prompt}\n","Generator: ```\"\"\"\n","    \n","    results = run_inference(model, tokenizer, prompt)\n","\n","    end_index = results[0].find(\"```\")\n","\n","    # Extract the substring from the start of the string up to the first occurrence of ```\n","    if end_index != -1:\n","        query = results[0][:end_index]\n","    else:\n","        # If ``` is not found, keep the original string\n","        query = results[0]\n","\n","    # Now remove the SPARQL prefix that the model adds.\n","    start_index = query.find(\"SPARQL\")\n","    if start_index == 0:\n","        # Remove the prefix and all characters leading up to it\n","        query = query[start_index + len(\"SPARQL\"):]\n","\n","    return query"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.325085Z","iopub.status.idle":"2024-07-23T17:32:59.325527Z","shell.execute_reply":"2024-07-23T17:32:59.325313Z","shell.execute_reply.started":"2024-07-23T17:32:59.325295Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import json\n","\n","with open('/kaggle/input/geoqa200/200_Sub_Dataset.json', 'r') as file:\n","    original_dataset = json.load(file)\n","\n","# Create a new dataset with questions and generated queries\n","new_dataset = {}\n","for key, item in original_dataset.items():\n","    question = item['Question']\n","    gt_query = item['Query']\n","    query = generate_URI_injected_query(model, tokenizer, question, gt_query)\n","    new_dataset[key] = {'Question': question, 'Query': query}\n","    #print (f\"{key}/{len(original_dataset.items())}\")\n","\n","# Save the new dataset to a JSON file\n","with open('/kaggle/working/generated_dataset1.json', 'w') as file:\n","    json.dump(new_dataset, file, indent=4)"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate the generated SPARQL queries."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.327181Z","iopub.status.idle":"2024-07-23T17:32:59.327515Z","shell.execute_reply":"2024-07-23T17:32:59.327365Z","shell.execute_reply.started":"2024-07-23T17:32:59.327351Z"},"trusted":true},"outputs":[],"source":["!pip install rdflib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.328773Z","iopub.status.idle":"2024-07-23T17:32:59.329112Z","shell.execute_reply":"2024-07-23T17:32:59.328971Z","shell.execute_reply.started":"2024-07-23T17:32:59.328957Z"},"trusted":true},"outputs":[],"source":["import re\n","from collections import Counter\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rdflib.plugins.sparql.parser import parseQuery\n","\n","def normalize_variables(query):\n","    if query is None:\n","        return \"\"\n","    variable_pattern = re.compile(r\"\\?\\w+\")\n","    variables = variable_pattern.findall(query)\n","    normalized_query = query\n","    for i, var in enumerate(variables):\n","        normalized_query = normalized_query.replace(var, f\"?var{i}\")\n","    return normalized_query\n","\n","def is_parsable(query):\n","    try:\n","        parsed_query = parseQuery(query)\n","        return True\n","    except Exception as e:\n","        print(f\"Error parsing query: {e}\")\n","        return False\n","\n","def extract_tokens(query):\n","    if query is None:\n","        return []\n","    # Tokenize by splitting on non-word characters\n","    tokens = re.findall(r'\\b\\w+\\b', query)\n","    return tokens\n","\n","def mod_jaccard_similarity(list1, list2):\n","    generated_freq = Counter(list1)\n","    reference_freq = Counter(list2)\n","\n","    intersection = sum((generated_freq & reference_freq).values())\n","    union = sum((generated_freq | reference_freq).values())\n","\n","    if not union:\n","        return 0.0\n","\n","    return intersection / union\n","\n","def cosine_similarity_score(generated_query, reference_query):\n","    # Normalize and extract tokens\n","    normalized_generated_query = normalize_variables(generated_query)\n","    normalized_reference_query = normalize_variables(reference_query)\n","\n","    generated_tokens = ' '.join(extract_tokens(normalized_generated_query))\n","    reference_tokens = ' '.join(extract_tokens(normalized_reference_query))\n","\n","    # Vectorize the tokens\n","    vectorizer = CountVectorizer().fit_transform([generated_tokens, reference_tokens])\n","    vectors = vectorizer.toarray()\n","\n","    # Calculate cosine similarity\n","    cosine_sim = cosine_similarity(vectors)\n","    return cosine_sim[0, 1]\n","\n","def bleu_score(generated_query, reference_query):\n","    # Normalize and extract tokens\n","    normalized_generated_query = normalize_variables(generated_query)\n","    normalized_reference_query = normalize_variables(reference_query)\n","    \n","    generated_tokens = extract_tokens(normalized_generated_query)\n","    reference_tokens = extract_tokens(normalized_reference_query)\n","\n","    # Calculate BLEU score\n","    smoothie = SmoothingFunction().method4\n","    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothie)\n","\n","def calculate_similarity_score(generated_query, reference_query):\n","    # Normalize and extract tokens\n","    normalized_generated_query = normalize_variables(generated_query)\n","    normalized_reference_query = normalize_variables(reference_query)\n","    \n","    generated_tokens = extract_tokens(normalized_generated_query)\n","    reference_tokens = extract_tokens(normalized_reference_query)\n","    \n","    # Calculate Jaccard similarity\n","    m_j_s = mod_jaccard_similarity(generated_tokens, reference_tokens)\n","    cos_s = cosine_similarity_score(generated_query, reference_query)\n","    bleu = bleu_score(generated_query, reference_query)\n","    parsability_score = 1 if is_parsable(generated_query) else 0\n","    \n","    hybrid_bleu = 0.2 * m_j_s + 0.2 * cos_s + 0.05 * parsability_score + 0.55 * bleu\n","    \n","    return bleu, hybrid_bleu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.331144Z","iopub.status.idle":"2024-07-23T17:32:59.331588Z","shell.execute_reply":"2024-07-23T17:32:59.331373Z","shell.execute_reply.started":"2024-07-23T17:32:59.331355Z"},"trusted":true},"outputs":[],"source":["def evaluate_generations(dataset, generated_queries):\n","    blues = []\n","    hybrid_blues = []\n","    \n","    for key in dataset:\n","        query = dataset[key]['Query']\n","        generated_query = generated_queries[key]['Query']\n","        \n","        blue, hbleu = calculate_similarity_score(query, generated_query)\n","        \n","        blues.append(blue)\n","        hybrid_blues.append(hbleu)\n","        \n","    average_bleu = sum(blues) / len(blues) if blues else 0\n","    average_hybrid_bleu = sum(hybrid_blues) / len(hybrid_blues) if hybrid_blues else 0\n","    \n","    return average_bleu, average_hybrid_bleu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.332688Z","iopub.status.idle":"2024-07-23T17:32:59.333142Z","shell.execute_reply":"2024-07-23T17:32:59.332932Z","shell.execute_reply.started":"2024-07-23T17:32:59.332914Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/80geoqa/80_Sub_Dataset.json', 'r') as file:\n","    original_dataset = json.load(file)\n","\n","avg_bleu, avg_hbleu = evaluate_generations(original_dataset, new_dataset)\n","print(avg_bleu, avg_hbleu)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.334877Z","iopub.status.idle":"2024-07-23T17:32:59.335189Z","shell.execute_reply":"2024-07-23T17:32:59.335049Z","shell.execute_reply.started":"2024-07-23T17:32:59.335036Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/geoqa200/200_Sub_Dataset.json', 'r') as file:\n","    original_dataset = json.load(file)\n","\n","avg_bleu, avg_hbleu = evaluate_generations(original_dataset, new_dataset)\n","print(avg_bleu, avg_hbleu)"]},{"cell_type":"markdown","metadata":{},"source":["Now evaluate the model's not based on their query generation abilities but by comparing the results of the original and generated queries. This means that this test is the final accuracy score of the models."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.336145Z","iopub.status.idle":"2024-07-23T17:32:59.336450Z","shell.execute_reply":"2024-07-23T17:32:59.336307Z","shell.execute_reply.started":"2024-07-23T17:32:59.336294Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def format_query(query):\n","    PREFIXES = \"\"\"PREFIX geo: <http://www.opengis.net/ont/geosparql#>\n","PREFIX geof: <http://www.opengis.net/def/function/geosparql/>\n","PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n","PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n","PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n","PREFIX yago: <http://yago-knowledge.org/resource/>\n","PREFIX y2geor: <http://kr.di.uoa.gr/yago2geo/resource/>\n","PREFIX y2geoo: <http://kr.di.uoa.gr/yago2geo/ontology/>\n","PREFIX strdf: <http://strdf.di.uoa.gr/ontology#>\n","PREFIX uom: <http://www.opengis.net/def/uom/OGC/1.0/>\n","PREFIX owl: <http://www.w3.org/2002/07/owl#>\"\"\"\n","    \n","    query = PREFIXES + ' ' + query\n","    \n","    query = query.replace('strdf:within', 'geof:sfWithin')\n","    query = query.replace('strdf:contains', 'geof:sfContains')\n","    query = query.replace('strdf:overlaps', 'geof:sfOverlaps')\n","    query = query.replace('strdf:distance', 'geof:sfDistance')\n","    \n","    # Use regex to find and replace strdf:buffer patterns\n","    query = re.sub(r'strdf:buffer\\((\\?\\w+),\\s*\\d+,\\s*uom:\\w+\\)', r'\\1', query)\n","    \n","    return query"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.337994Z","iopub.status.idle":"2024-07-23T17:32:59.338308Z","shell.execute_reply":"2024-07-23T17:32:59.338167Z","shell.execute_reply.started":"2024-07-23T17:32:59.338155Z"},"trusted":true},"outputs":[],"source":["def gost_materialize_query(query: str):\n","    data = {\n","        \"query\": query\n","    }\n","\n","    headers = {\n","        'Content-Type': 'application/json'\n","    }\n","\n","    response = requests.post(\"materialize-api\", headers=headers, data=json.dumps(data))\n","    \n","    if response.status_code == 200:\n","        return response.text\n","    else:\n","        print(\"Materialize failed:\", response.text)\n","        return (query)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.340020Z","iopub.status.idle":"2024-07-23T17:32:59.340459Z","shell.execute_reply":"2024-07-23T17:32:59.340252Z","shell.execute_reply.started":"2024-07-23T17:32:59.340233Z"},"trusted":true},"outputs":[],"source":["def graphdb_send_request(query, endpoint_url=\"endp_url\", accept_format='application/sparql-results+json'):\n","    # Format the query, this means add the correct prefixes and fix some endpoint issues with regex.\n","    query = format_query(query)\n","    original_query = query\n","    query = gost_materialize_query(query)\n","    \n","    headers = {\n","        'Accept': accept_format\n","    }\n","    \n","    params = {\n","        'query': query,\n","        'infer': 'true',\n","        'sameAs': 'true'\n","    }\n","    \n","    try:\n","        response = requests.get(endpoint_url, headers=headers, params=params, auth=requests.auth.HTTPBasicAuth('username', 'password'))\n","        response.raise_for_status()\n","        \n","        if accept_format == 'application/sparql-results+json':\n","            return response.json()\n","        else:\n","            return response.text\n","    except requests.exceptions.HTTPError as err:\n","        print(f\"HTTP error occurred: {err}\")\n","        print(f\"Original query: {original_query} \\n\\n gost: {query}\")\n","    except Exception as err:\n","        print(f\"An error occurred: {err}\")\n","        print(f\"Original query: {original_query} \\n\\n gost: {query}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.341729Z","iopub.status.idle":"2024-07-23T17:32:59.342209Z","shell.execute_reply":"2024-07-23T17:32:59.342006Z","shell.execute_reply.started":"2024-07-23T17:32:59.341988Z"},"trusted":true},"outputs":[],"source":["import json, requests\n","\n","with open('/kaggle/input/100geoquestions/100_Sub_Dataset.json', 'r') as file:\n","    original_dataset = json.load(file)\n","    \n","for key in original_dataset:\n","    query = original_dataset[key]['Query']\n","    #print(query)\n","    graphdb_send_request(query)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5299649,"sourceId":8812575,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
