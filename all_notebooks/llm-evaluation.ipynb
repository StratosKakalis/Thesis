{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8812575,"sourceType":"datasetVersion","datasetId":5299649}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Define the model and the inference function","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef run_inference(model, tokenizer, prompt):\n    results = []\n    \n    if tokenizer == None:\n        # Generate output\n        with torch.no_grad():\n            outputs = model(prompt)\n            \n        # Decode and print output\n        #print(\"Prompt:\", prompt)\n        #print(\"Generated text:\" + outputs + \"\\n\")\n        results.append(\"Generated text:\" + outputs)\n    else:\n        # Move model to GPU\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model.to(device)\n        model.eval()  # Set model to evaluation mode\n            \n        # Tokenize prompt\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n            \n        # Generate output\n        with torch.no_grad():\n            outputs = model.generate(**inputs, \n                            max_length=1800,  # Set a maximum length for generated text\n                            #do_sample=True,  # Enable sampling\n                            #top_k=7,        # Top-k sampling\n                            #top_p=0.1,      # Top-p sampling (nucleus sampling)\n                            #num_return_sequences=1,\n                            #repetition_penalty=1, # No penalty for instruction tuned models.\n                            repetition_penalty=1.2, # Penalty on repeating tokens.\n                            eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n                            pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n                            )\n        \n        # Extract generated text\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Remove the prompt text\n        prompt_length = len(prompt)\n        generated_text = generated_text[prompt_length:]\n\n        # Decode and print output\n        #print(\"Prompt:\", prompt)\n        #print(generated_text)\n        results.append(generated_text)\n    \n    # Clear model from RAM\n    del model\n    torch.cuda.empty_cache()\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:21.789509Z","iopub.execute_input":"2024-07-23T17:32:21.789789Z","iopub.status.idle":"2024-07-23T17:32:25.592473Z","shell.execute_reply.started":"2024-07-23T17:32:21.789756Z","shell.execute_reply":"2024-07-23T17:32:25.591445Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef Quantized_Inference(model, tokenizer, prompt):\n    results = []\n    \n    # Move model to GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()  # Set model to evaluation mode\n            \n    # Tokenize prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n            \n    # Generate output\n    with torch.no_grad():\n        outputs = model.generate(**inputs, \n                            max_new_tokens=350,  # Set a maximum length for generated text\n                            #do_sample=True,  # Enable sampling\n                            #top_k=7,        # Top-k sampling\n                            #top_p=0.1,      # Top-p sampling (nucleus sampling)\n                            #num_return_sequences=1,\n                            repetition_penalty=1.2, # Penalty on repeating tokens.\n                            eos_token_id=tokenizer.eos_token_id,  # Specify EOS token ID\n                            pad_token_id=tokenizer.pad_token_id  # Specify PAD token ID\n                            )\n        \n    # Extract generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Remove the prompt text\n    prompt_length = len(prompt)\n    generated_text = generated_text[prompt_length:]\n\n    # Decode and print output\n    #print(\"Prompt:\", prompt)\n    #print(\"Generated text:\" + generated_text + \"\\n\")\n    results.append(\"Generated text:\" + generated_text)\n    \n    # Clear model from RAM\n    del model\n    torch.cuda.empty_cache()\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:25.594278Z","iopub.execute_input":"2024-07-23T17:32:25.594684Z","iopub.status.idle":"2024-07-23T17:32:25.602871Z","shell.execute_reply.started":"2024-07-23T17:32:25.594656Z","shell.execute_reply":"2024-07-23T17:32:25.602004Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Define two functions for extracting URIs from the dataset queries, one with direct extraction and the other with the ability to expand prefixes.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef extract_uris(query):\n    # Regular expression to match both fully expanded and prefixed URIs\n    uri_pattern = r'<([^>]+)>|(\\b[a-zA-Z0-9_]+):([a-zA-Z0-9_]+)'\n    \n    uris = []\n    matches = re.findall(uri_pattern, query)\n    for match in matches:\n        if match[0]:  # Fully expanded URI\n            uris.append(match[0])\n        else:  # Prefixed URI\n            uris.append(f\"{match[1]}:{match[2]}\")\n    return uris\n\ndef expand_uris(query, prefix_dict):\n    prefixed_pattern = r'(\\b[a-zA-Z0-9_]+):([a-zA-Z0-9_]+)'\n    expanded_pattern = r'<([^>]+)>'\n    \n    expanded_uris = []\n    \n    # Find and expand prefixed URIs\n    matches = re.findall(prefixed_pattern, query)\n    for prefix, suffix in matches:\n        if prefix in prefix_dict:\n            expanded_uris.append(f\"{prefix_dict[prefix]}{suffix}\")\n        else:\n            expanded_uris.append(f\"{prefix}:{suffix}\")\n    \n    # Find and add already expanded URIs\n    matches = re.findall(expanded_pattern, query)\n    for uri in matches:\n        expanded_uris.append(uri)\n    \n    return expanded_uris\n\nprefix_dict = {\n    'geo': 'http://www.opengis.net/ont/geosparql#',\n    'osm': 'http://www.openstreetmap.org/ontology#',\n    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n    'geof': 'http://www.opengis.net/def/function/geosparql/',\n    'uom': 'http://www.opengis.net/def/uom/OGC/1.0/'\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:25.603990Z","iopub.execute_input":"2024-07-23T17:32:25.604289Z","iopub.status.idle":"2024-07-23T17:32:25.618122Z","shell.execute_reply.started":"2024-07-23T17:32:25.604267Z","shell.execute_reply":"2024-07-23T17:32:25.617023Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n#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","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:25.620563Z","iopub.execute_input":"2024-07-23T17:32:25.621347Z","iopub.status.idle":"2024-07-23T17:32:25.980948Z","shell.execute_reply.started":"2024-07-23T17:32:25.621321Z","shell.execute_reply":"2024-07-23T17:32:25.979983Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec5d8d1d407c44f49ecd1d37f6bc69be"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-23T17:32:25.982157Z","iopub.execute_input":"2024-07-23T17:32:25.982434Z","iopub.status.idle":"2024-07-23T17:32:55.479666Z","shell.execute_reply.started":"2024-07-23T17:32:25.982410Z","shell.execute_reply":"2024-07-23T17:32:55.478420Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", load_in_8bit=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:55.481343Z","iopub.execute_input":"2024-07-23T17:32:55.481683Z","iopub.status.idle":"2024-07-23T17:32:59.299645Z","shell.execute_reply.started":"2024-07-23T17:32:55.481652Z","shell.execute_reply":"2024-07-23T17:32:59.295883Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 393\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GatedRepoError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-669fe949-780d28a00f47785613dd2d17;2a2f6e50-1f11-4f3a-9daa-540c79e6bc72)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:794\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 794\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1138\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1136\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1138\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1140\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:416\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n401 Client Error. (Request ID: Root=1-669fe949-780d28a00f47785613dd2d17;2a2f6e50-1f11-4f3a-9daa-540c79e6bc72)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it."],"ename":"OSError","evalue":"You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n401 Client Error. (Request ID: Root=1-669fe949-780d28a00f47785613dd2d17;2a2f6e50-1f11-4f3a-9daa-540c79e6bc72)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.","output_type":"error"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n\nmodel = AutoModelForCausalLM.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\", torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.300497Z","iopub.status.idle":"2024-07-23T17:32:59.300887Z","shell.execute_reply.started":"2024-07-23T17:32:59.300695Z","shell.execute_reply":"2024-07-23T17:32:59.300710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", torch_dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.302158Z","iopub.status.idle":"2024-07-23T17:32:59.302472Z","shell.execute_reply.started":"2024-07-23T17:32:59.302320Z","shell.execute_reply":"2024-07-23T17:32:59.302332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run inference on the entire dataset and store them for evaluation","metadata":{}},{"cell_type":"code","source":"# Run inference and cleanup.\ndef generate_query(model, tokenizer, user_prompt):\n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nHuman: {user_prompt}\nGenerator: ```\"\"\"\n    \n    results = run_inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.304205Z","iopub.status.idle":"2024-07-23T17:32:59.304529Z","shell.execute_reply.started":"2024-07-23T17:32:59.304364Z","shell.execute_reply":"2024-07-23T17:32:59.304377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run inference and cleanup.\ndef generate_query(model, tokenizer, user_prompt):\n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nHuman: In Breckland district, which forests are south of streams?\nGenerator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\nHuman: How many streams intersect with lakes?\nGenerator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\nHuman: Which Municipalities are on Thessaly's border?\nGenerator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\nHuman: {user_prompt}\nGenerator: ```\"\"\"\n    \n    results = run_inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.306449Z","iopub.status.idle":"2024-07-23T17:32:59.306803Z","shell.execute_reply.started":"2024-07-23T17:32:59.306636Z","shell.execute_reply":"2024-07-23T17:32:59.306651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run llama 3 inference and cleanup.\ndef generate_quantized_query(model, tokenizer, user_prompt):\n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nHuman: In Breckland district, which forests are south of streams?\nGenerator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\nHuman: How many streams intersect with lakes?\nGenerator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\nHuman: Which Municipalities are on Thessaly's border?\nGenerator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\nHuman: {user_prompt}\nGenerator: ```\"\"\"\n\n    results = Quantized_Inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.308549Z","iopub.status.idle":"2024-07-23T17:32:59.308936Z","shell.execute_reply.started":"2024-07-23T17:32:59.308730Z","shell.execute_reply":"2024-07-23T17:32:59.308750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the results as a json file of the same format as the dataset.","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/geoqa200/200_Sub_Dataset.json', 'r') as file:\n    original_dataset = json.load(file)\n\n# Create a new dataset with questions and generated queries\nnew_dataset = {}\ni = 0\nfor key, item in original_dataset.items():\n    i += 1\n    question = item['Question']\n    query = generate_query(model, tokenizer, question)\n    new_dataset[key] = {'Question': question, 'Query': query}\n    print (f\"{i}/{len(original_dataset.items())}\")\n    \n# Save the new dataset to a JSON file\nwith open('/kaggle/working/generated_dataset1.json', 'w') as file:\n    json.dump(new_dataset, file, indent=4)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-23T17:32:59.310217Z","iopub.status.idle":"2024-07-23T17:32:59.310535Z","shell.execute_reply.started":"2024-07-23T17:32:59.310375Z","shell.execute_reply":"2024-07-23T17:32:59.310388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/80geoqa/80_Sub_Dataset.json', 'r') as file:\n    original_dataset = json.load(file)\n\n# Create a new dataset with questions and generated queries\nnew_dataset = {}\ni = 0\nfor key, item in original_dataset.items():\n    i += 1\n    question = item['Question']\n    query = generate_quantized_query(model, tokenizer, question)\n    new_dataset[key] = {'Question': question, 'Query': query}\n    print (f\"{i}/{len(original_dataset.items())}\")\n\n# Save the new dataset to a JSON file\nwith open('/kaggle/working/generated_dataset1.json', 'w') as file:\n    json.dump(new_dataset, file, indent=4)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-23T17:32:59.311602Z","iopub.status.idle":"2024-07-23T17:32:59.311997Z","shell.execute_reply.started":"2024-07-23T17:32:59.311766Z","shell.execute_reply":"2024-07-23T17:32:59.311781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alternatively, run inference and URI extraction and injection into the prompt.","metadata":{}},{"cell_type":"code","source":"# Run inference and cleanup.\ndef generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n    # Direct extraction, NOTE: try expanded extraction.\n    uris = extract_uris(gt_query)\n    \n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nHuman: {user_prompt}\nThe generator must use these URIs to answer the question: {uris}\nGenerator: ```\"\"\"\n    \n    results = run_inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.315235Z","iopub.status.idle":"2024-07-23T17:32:59.315742Z","shell.execute_reply.started":"2024-07-23T17:32:59.315490Z","shell.execute_reply":"2024-07-23T17:32:59.315523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try as well with supplying the model with GeoSPARQL ontology description.","metadata":{}},{"cell_type":"code","source":"# Run inference and cleanup.\ndef generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n    # Direct extraction, NOTE: try expanded extraction.\n    uris = extract_uris(gt_query)\n    \n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nThe resulting query may have to be in GeoSPARQL. The GeoSPARQL ontology is defined by:\nURI: http://www.opengis.net/ont/geosparql\nClasses: Feature, Feature Collection, Geometry, Geometry Collection, Spatial Object, Spatial Object Collection\nObject Properties: default geometry, contains, covered by, covers, disjoint, equals, inside, meet, overlap, has area, has bounding box, has centroid, has default geometry, has geometry, has length, has perimeter length, has size, has spatial accuracy, has spatial resolution, has volume, disconnected, externally connected, equals, non-tangential proper part, non-tangential proper part inverse, partially overlapping, tangential proper part, tangential proper part inverse, contains, crosses, disjoint, equals, intersects, overlaps, touches, within\nDatatype Properties: as DGGS, as GML, as GeoJSON, as KML, as WKT, coordinate dimension, dimension, has area in square meters, has length in meters, has perimeter length in meters, has metric size, has spatial accuracy in meters, has spatial resolution in meters, has volume in cubic meters, has serialization, is empty, is simple, spatial dimension\nHuman: {user_prompt}\nThe generator must use these URIs to answer the question: {uris}\nGenerator: ```\"\"\"\n    \n    results = run_inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.316919Z","iopub.status.idle":"2024-07-23T17:32:59.317359Z","shell.execute_reply.started":"2024-07-23T17:32:59.317130Z","shell.execute_reply":"2024-07-23T17:32:59.317149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try few shot training.","metadata":{}},{"cell_type":"code","source":"# Run inference and cleanup.\ndef generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n    # Direct extraction, NOTE: try expanded extraction.\n    uris = extract_uris(gt_query)\n    \n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nHuman: In Breckland district, which forests are south of streams?\nThe generator must use these URIs to answer the question: ['yago:Breckland_District', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within', 'strdf:within', 'strdf:below']\nGenerator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\nHuman: How many streams intersect with lakes?\nThe generator must use these URIs to answer the question: ['rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects']\nGenerator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\nHuman: Which Municipalities are on Thessaly's border?\nThe generator must use these URIs to answer the question: ['yago:Thessaly', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'strdf:touches']\nGenerator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\nHuman: {user_prompt}\nThe generator must use these URIs to answer the question: {uris}\nGenerator: ```\"\"\"\n    \n    results = run_inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.319268Z","iopub.status.idle":"2024-07-23T17:32:59.319729Z","shell.execute_reply.started":"2024-07-23T17:32:59.319496Z","shell.execute_reply":"2024-07-23T17:32:59.319516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try with even more examples.","metadata":{}},{"cell_type":"code","source":"# Run inference and cleanup.\ndef generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n    # Direct extraction, NOTE: try expanded extraction.\n    uris = extract_uris(gt_query)\n    \n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nHuman: In Breckland district, which forests are south of streams?\nThe generator must use these URIs to answer the question: ['yago:Breckland_District', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_forest', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'strdf:within', 'strdf:within', 'strdf:below']\nGenerator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\nHuman: How many streams intersect with lakes?\nThe generator must use these URIs to answer the question: ['rdf:type', 'y2geoo:OSM_stream', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:OSM_lake', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfIntersects']\nGenerator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\nHuman: Which Municipalities are on Thessaly's border?\nThe generator must use these URIs to answer the question: ['yago:Thessaly', 'geo:hasGeometry', 'geo:asWKT', 'rdf:type', 'y2geoo:GAG_Municipality', 'geo:hasGeometry', 'geo:asWKT', 'strdf:touches']\nGenerator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\nHuman: Which is the largest island in Ireland?\nThe generator must use these URIs to answer the question: ['strdf:area', 'yago:Republic_of_Ireland', 'geo:hasGeometry', 'geo:asWKT', 'y2geoo:OSM_island', 'geo:hasGeometry', 'geo:asWKT', 'geof:sfContains']\nGenerator: ```select distinct ?x (strdf:area(?lWKT) as ?area) where {{ yago:Republic_of_Ireland geo:hasGeometry ?geom . ?geom geo:asWKT ?mWKT . ?lake a y2geoo:OSM_island . ?lake geo:hasGeometry ?geol . ?geol geo:asWKT ?lWKT . FILTER (geof:sfContains(?mWKT, ?lWKT)) }} ORDER BY (?area) LIMIT 1```\nHuman: Is Crete south of Thessaly?\nThe generator must use these URIs to answer the question: ['http://yago-knowledge.org/resource/Crete', 'geo:hasGeometry', 'http://yago-knowledge.org/resource/Thessaly', 'geo:hasGeometry', 'geo:asWKT', 'geo:asWKT', 'strdf:below']\nGenerator: ```ASK {{ <http://yago-knowledge.org/resource/Crete> geo:hasGeometry ?geo1 . <http://yago-knowledge.org/resource/Thessaly> geo:hasGeometry ?geo2 . ?geo1 geo:asWKT ?geoWKT1 . ?geo2 geo:asWKT ?geoWKT2 . FILTER(strdf:below(?geoWKT1, ?geoWKT2)) }}```\nHuman: What is the population of Northern Ireland?\nThe generator must use these URIs to answer the question: ['xsd:integer', 'yago:Northern_Ireland', 'yago:hasPopulation']\nGenerator: ```SELECT (xsd:integer (?population) as ?pop) WHERE {{ yago:Northern_Ireland yago:hasPopulation ?population. }}```\nHuman: {user_prompt}\nThe generator must use these URIs to answer the question: {uris}\nGenerator: ```\"\"\"\n    \n    results = run_inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.321728Z","iopub.status.idle":"2024-07-23T17:32:59.322098Z","shell.execute_reply.started":"2024-07-23T17:32:59.321931Z","shell.execute_reply":"2024-07-23T17:32:59.321947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How is it without URI injection?","metadata":{}},{"cell_type":"code","source":"# Run inference and cleanup.\ndef generate_URI_injected_query(model, tokenizer, user_prompt, gt_query):\n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query will be based on the DBpedia knowledge graph. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\nHuman: In Breckland district, which forests are south of streams?\nGenerator: ```SELECT DISTINCT ?forest WHERE {{ yago:Breckland_District geo:hasGeometry ?o1 . ?o1 geo:asWKT ?geoWKT1 . ?forest rdf:type y2geoo:OSM_forest . ?forest geo:hasGeometry ?o2 . ?o2 geo:asWKT ?geoWKT2 . ?stream rdf:type y2geoo:OSM_stream . ?stream geo:hasGeometry ?o3 . ?o3 geo:asWKT ?geoWKT3 . FILTER (strdf:within(?geoWKT2, ?geoWKT1) && strdf:within(?geoWKT3, ?geoWKT1) && strdf:below(?geoWKT2, ?geoWKT3)) }}```\nHuman: How many streams intersect with lakes?\nGenerator: ```SELECT (COUNT (DISTINCT ?p1) as ?streams) WHERE {{ ?p1 rdf:type y2geoo:OSM_stream; geo:hasGeometry ?p1geo. ?p1geo geo:asWKT ?p1WKT. ?p2 rdf:type y2geoo:OSM_lake; geo:hasGeometry ?p2geo. ?p2geo geo:asWKT ?p2WKT. FILTER(geof:sfIntersects(?p1WKT, ?p2WKT)) }}```\nHuman: Which Municipalities are on Thessaly's border?\nGenerator: ```SELECT distinct ?rg where {{ yago:Thessaly geo:hasGeometry ?tgeo . ?tgeo geo:asWKT ?tgWKT . ?rg rdf:type y2geoo:GAG_Municipality . ?rg geo:hasGeometry ?rggeo . ?rggeo geo:asWKT ?rgWKT . FILTER (strdf:touches(?tgWKT,?rgWKT)) . }}```\nHuman: {user_prompt}\nGenerator: ```\"\"\"\n    \n    results = run_inference(model, tokenizer, prompt)\n\n    end_index = results[0].find(\"```\")\n\n    # Extract the substring from the start of the string up to the first occurrence of ```\n    if end_index != -1:\n        query = results[0][:end_index]\n    else:\n        # If ``` is not found, keep the original string\n        query = results[0]\n\n    # Now remove the SPARQL prefix that the model adds.\n    start_index = query.find(\"SPARQL\")\n    if start_index == 0:\n        # Remove the prefix and all characters leading up to it\n        query = query[start_index + len(\"SPARQL\"):]\n\n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.323439Z","iopub.status.idle":"2024-07-23T17:32:59.323915Z","shell.execute_reply.started":"2024-07-23T17:32:59.323664Z","shell.execute_reply":"2024-07-23T17:32:59.323682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/geoqa200/200_Sub_Dataset.json', 'r') as file:\n    original_dataset = json.load(file)\n\n# Create a new dataset with questions and generated queries\nnew_dataset = {}\nfor key, item in original_dataset.items():\n    question = item['Question']\n    gt_query = item['Query']\n    query = generate_URI_injected_query(model, tokenizer, question, gt_query)\n    new_dataset[key] = {'Question': question, 'Query': query}\n    #print (f\"{key}/{len(original_dataset.items())}\")\n\n# Save the new dataset to a JSON file\nwith open('/kaggle/working/generated_dataset1.json', 'w') as file:\n    json.dump(new_dataset, file, indent=4)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-23T17:32:59.325085Z","iopub.status.idle":"2024-07-23T17:32:59.325527Z","shell.execute_reply.started":"2024-07-23T17:32:59.325295Z","shell.execute_reply":"2024-07-23T17:32:59.325313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the generated SPARQL queries.","metadata":{}},{"cell_type":"code","source":"!pip install rdflib","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.327181Z","iopub.status.idle":"2024-07-23T17:32:59.327515Z","shell.execute_reply.started":"2024-07-23T17:32:59.327351Z","shell.execute_reply":"2024-07-23T17:32:59.327365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rdflib.plugins.sparql.parser import parseQuery\n\ndef normalize_variables(query):\n    if query is None:\n        return \"\"\n    variable_pattern = re.compile(r\"\\?\\w+\")\n    variables = variable_pattern.findall(query)\n    normalized_query = query\n    for i, var in enumerate(variables):\n        normalized_query = normalized_query.replace(var, f\"?var{i}\")\n    return normalized_query\n\ndef is_parsable(query):\n    try:\n        parsed_query = parseQuery(query)\n        return True\n    except Exception as e:\n        print(f\"Error parsing query: {e}\")\n        return False\n\ndef extract_tokens(query):\n    if query is None:\n        return []\n    # Tokenize by splitting on non-word characters\n    tokens = re.findall(r'\\b\\w+\\b', query)\n    return tokens\n\ndef mod_jaccard_similarity(list1, list2):\n    generated_freq = Counter(list1)\n    reference_freq = Counter(list2)\n\n    intersection = sum((generated_freq & reference_freq).values())\n    union = sum((generated_freq | reference_freq).values())\n\n    if not union:\n        return 0.0\n\n    return intersection / union\n\ndef cosine_similarity_score(generated_query, reference_query):\n    # Normalize and extract tokens\n    normalized_generated_query = normalize_variables(generated_query)\n    normalized_reference_query = normalize_variables(reference_query)\n\n    generated_tokens = ' '.join(extract_tokens(normalized_generated_query))\n    reference_tokens = ' '.join(extract_tokens(normalized_reference_query))\n\n    # Vectorize the tokens\n    vectorizer = CountVectorizer().fit_transform([generated_tokens, reference_tokens])\n    vectors = vectorizer.toarray()\n\n    # Calculate cosine similarity\n    cosine_sim = cosine_similarity(vectors)\n    return cosine_sim[0, 1]\n\ndef bleu_score(generated_query, reference_query):\n    # Normalize and extract tokens\n    normalized_generated_query = normalize_variables(generated_query)\n    normalized_reference_query = normalize_variables(reference_query)\n    \n    generated_tokens = extract_tokens(normalized_generated_query)\n    reference_tokens = extract_tokens(normalized_reference_query)\n\n    # Calculate BLEU score\n    smoothie = SmoothingFunction().method4\n    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothie)\n\ndef calculate_similarity_score(generated_query, reference_query):\n    # Normalize and extract tokens\n    normalized_generated_query = normalize_variables(generated_query)\n    normalized_reference_query = normalize_variables(reference_query)\n    \n    generated_tokens = extract_tokens(normalized_generated_query)\n    reference_tokens = extract_tokens(normalized_reference_query)\n    \n    # Calculate Jaccard similarity\n    m_j_s = mod_jaccard_similarity(generated_tokens, reference_tokens)\n    cos_s = cosine_similarity_score(generated_query, reference_query)\n    bleu = bleu_score(generated_query, reference_query)\n    parsability_score = 1 if is_parsable(generated_query) else 0\n    \n    hybrid_bleu = 0.2 * m_j_s + 0.2 * cos_s + 0.05 * parsability_score + 0.55 * bleu\n    \n    return bleu, hybrid_bleu","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.328773Z","iopub.status.idle":"2024-07-23T17:32:59.329112Z","shell.execute_reply.started":"2024-07-23T17:32:59.328957Z","shell.execute_reply":"2024-07-23T17:32:59.328971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_generations(dataset, generated_queries):\n    blues = []\n    hybrid_blues = []\n    \n    for key in dataset:\n        query = dataset[key]['Query']\n        generated_query = generated_queries[key]['Query']\n        \n        blue, hbleu = calculate_similarity_score(query, generated_query)\n        \n        blues.append(blue)\n        hybrid_blues.append(hbleu)\n        \n    average_bleu = sum(blues) / len(blues) if blues else 0\n    average_hybrid_bleu = sum(hybrid_blues) / len(hybrid_blues) if hybrid_blues else 0\n    \n    return average_bleu, average_hybrid_bleu","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.331144Z","iopub.status.idle":"2024-07-23T17:32:59.331588Z","shell.execute_reply.started":"2024-07-23T17:32:59.331355Z","shell.execute_reply":"2024-07-23T17:32:59.331373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/80geoqa/80_Sub_Dataset.json', 'r') as file:\n    original_dataset = json.load(file)\n\navg_bleu, avg_hbleu = evaluate_generations(original_dataset, new_dataset)\nprint(avg_bleu, avg_hbleu)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.332688Z","iopub.status.idle":"2024-07-23T17:32:59.333142Z","shell.execute_reply.started":"2024-07-23T17:32:59.332914Z","shell.execute_reply":"2024-07-23T17:32:59.332932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/geoqa200/200_Sub_Dataset.json', 'r') as file:\n    original_dataset = json.load(file)\n\navg_bleu, avg_hbleu = evaluate_generations(original_dataset, new_dataset)\nprint(avg_bleu, avg_hbleu)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.334877Z","iopub.status.idle":"2024-07-23T17:32:59.335189Z","shell.execute_reply.started":"2024-07-23T17:32:59.335036Z","shell.execute_reply":"2024-07-23T17:32:59.335049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now evaluate the model's not based on their query generation abilities but by comparing the results of the original and generated queries. This means that this test is the final accuracy score of the models.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef format_query(query):\n    PREFIXES = \"\"\"PREFIX geo: <http://www.opengis.net/ont/geosparql#>\nPREFIX geof: <http://www.opengis.net/def/function/geosparql/>\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nPREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\nPREFIX yago: <http://yago-knowledge.org/resource/>\nPREFIX y2geor: <http://kr.di.uoa.gr/yago2geo/resource/>\nPREFIX y2geoo: <http://kr.di.uoa.gr/yago2geo/ontology/>\nPREFIX strdf: <http://strdf.di.uoa.gr/ontology#>\nPREFIX uom: <http://www.opengis.net/def/uom/OGC/1.0/>\nPREFIX owl: <http://www.w3.org/2002/07/owl#>\"\"\"\n    \n    query = PREFIXES + ' ' + query\n    \n    query = query.replace('strdf:within', 'geof:sfWithin')\n    query = query.replace('strdf:contains', 'geof:sfContains')\n    query = query.replace('strdf:overlaps', 'geof:sfOverlaps')\n    query = query.replace('strdf:distance', 'geof:sfDistance')\n    \n    # Use regex to find and replace strdf:buffer patterns\n    query = re.sub(r'strdf:buffer\\((\\?\\w+),\\s*\\d+,\\s*uom:\\w+\\)', r'\\1', query)\n    \n    return query","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.336145Z","iopub.status.idle":"2024-07-23T17:32:59.336450Z","shell.execute_reply.started":"2024-07-23T17:32:59.336294Z","shell.execute_reply":"2024-07-23T17:32:59.336307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gost_materialize_query(query: str):\n    data = {\n        \"query\": query\n    }\n\n    headers = {\n        'Content-Type': 'application/json'\n    }\n\n    response = requests.post(\"http://195.134.71.116:9090/materialize-api\", headers=headers, data=json.dumps(data))\n    \n    if response.status_code == 200:\n        return response.text\n    else:\n        print(\"Materialize failed:\", response.text)\n        return (query)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.337994Z","iopub.status.idle":"2024-07-23T17:32:59.338308Z","shell.execute_reply.started":"2024-07-23T17:32:59.338155Z","shell.execute_reply":"2024-07-23T17:32:59.338167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def graphdb_send_request(query, endpoint_url=\"http://88.197.53.158:7200/repositories/da4dte_final\", accept_format='application/sparql-results+json'):\n    # Format the query, this means add the correct prefixes and fix some endpoint issues with regex.\n    query = format_query(query)\n    original_query = query\n    query = gost_materialize_query(query)\n    \n    headers = {\n        'Accept': accept_format\n    }\n    \n    params = {\n        'query': query,\n        'infer': 'true',\n        'sameAs': 'true'\n    }\n    \n    try:\n        response = requests.get(endpoint_url, headers=headers, params=params, auth=requests.auth.HTTPBasicAuth('admin', 'p@sx@'))\n        response.raise_for_status()\n        \n        if accept_format == 'application/sparql-results+json':\n            return response.json()\n        else:\n            return response.text\n    except requests.exceptions.HTTPError as err:\n        print(f\"HTTP error occurred: {err}\")\n        print(f\"Original query: {original_query} \\n\\n gost: {query}\")\n    except Exception as err:\n        print(f\"An error occurred: {err}\")\n        print(f\"Original query: {original_query} \\n\\n gost: {query}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.340020Z","iopub.status.idle":"2024-07-23T17:32:59.340459Z","shell.execute_reply.started":"2024-07-23T17:32:59.340233Z","shell.execute_reply":"2024-07-23T17:32:59.340252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json, requests\n\nwith open('/kaggle/input/100geoquestions/100_Sub_Dataset.json', 'r') as file:\n    original_dataset = json.load(file)\n    \nfor key in original_dataset:\n    query = original_dataset[key]['Query']\n    #print(query)\n    graphdb_send_request(query)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:32:59.341729Z","iopub.status.idle":"2024-07-23T17:32:59.342209Z","shell.execute_reply.started":"2024-07-23T17:32:59.341988Z","shell.execute_reply":"2024-07-23T17:32:59.342006Z"},"trusted":true},"execution_count":null,"outputs":[]}]}