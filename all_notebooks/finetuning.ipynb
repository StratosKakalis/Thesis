{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8812575,"sourceType":"datasetVersion","datasetId":5299649},{"sourceId":9296326,"sourceType":"datasetVersion","datasetId":5628448},{"sourceId":9429401,"sourceType":"datasetVersion","datasetId":5728478}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n#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\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:02.057391Z","iopub.execute_input":"2024-09-20T16:16:02.057954Z","iopub.status.idle":"2024-09-20T16:16:02.610463Z","shell.execute_reply.started":"2024-09-20T16:16:02.057919Z","shell.execute_reply":"2024-09-20T16:16:02.609480Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q datasets accelerate evaluate trl accelerate bitsandbytes peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-20T16:16:02.612522Z","iopub.execute_input":"2024-09-20T16:16:02.612927Z","iopub.status.idle":"2024-09-20T16:16:22.975558Z","shell.execute_reply.started":"2024-09-20T16:16:02.612881Z","shell.execute_reply":"2024-09-20T16:16:22.974369Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def create_prompt(question, answer):\n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n\nHuman: {question}\nGenerator: '''{answer}'''\"\"\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:22.976874Z","iopub.execute_input":"2024-09-20T16:16:22.977212Z","iopub.status.idle":"2024-09-20T16:16:22.983017Z","shell.execute_reply.started":"2024-09-20T16:16:22.977178Z","shell.execute_reply":"2024-09-20T16:16:22.981980Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import re \n\ndef normalize_variables(query):\n    if query is None:\n        return \"\"\n    variable_pattern = re.compile(r\"\\?\\w+\")\n    variables = variable_pattern.findall(query)\n    normalized_query = query\n    for i, var in enumerate(variables):\n        normalized_query = normalized_query.replace(var, f\"?var{i}\")\n    return normalized_query","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:22.985017Z","iopub.execute_input":"2024-09-20T16:16:22.985425Z","iopub.status.idle":"2024-09-20T16:16:22.994599Z","shell.execute_reply.started":"2024-09-20T16:16:22.985392Z","shell.execute_reply":"2024-09-20T16:16:22.993814Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# This prefix map will be used to shrink the uri's down to the prefix level, to help the model better understand them and decrease mistakes.\nprefix_map = {\"http://www.opengis.net/ont/geosparql#\" : \"geo:\",\n               \"http://www.opengis.net/def/function/geosparql/\" : \"geof:\",\n               \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" : \"rdf:\",\n               \"http://www.w3.org/2000/01/rdf-schema#\" : \"rdfs:\",\n               \"http://www.w3.org/2001/XMLSchema#\" : \"xsd:\",\n               \"http://yago-knowledge.org/resource/\" : \"yago:\",\n               \"http://kr.di.uoa.gr/yago2geo/resource/\" : \"y2geor:\",\n               \"http://kr.di.uoa.gr/yago2geo/ontology/\" : \"y2geoo:\",\n               \"http://strdf.di.uoa.gr/ontology#\" : \"strdf:\",\n               \"http://www.opengis.net/def/uom/OGC/1.0/\" : \"uom:\",\n               \"http://www.w3.org/2002/07/owl#\" : \"owl:\"}","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:22.995629Z","iopub.execute_input":"2024-09-20T16:16:22.995929Z","iopub.status.idle":"2024-09-20T16:16:23.004667Z","shell.execute_reply.started":"2024-09-20T16:16:22.995889Z","shell.execute_reply":"2024-09-20T16:16:23.003809Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport json\n\ndef flatten_dataset(original_dataset):\n    flattened_data = []\n    \n    # Iterate through each numbered key in the dataset\n    for key in original_dataset:\n        if key.isdigit():  # Ensure we're only processing the numbered keys\n            item = original_dataset[key]  # Access the single dictionary in the list\n            # Create a new entry combining question and answer\n            query = normalize_variables(item['Query'])\n            # Shorten the uris down to prefixes.\n            for uri_map, prefix in prefix_map.items():\n                query = query.replace(uri_map, prefix)\n            prompt = create_prompt(item['Question'], query)\n            flattened_data.append(prompt)\n    \n    return flattened_data\n\nwith open('/kaggle/input/finetuning-no-rdfs/training_set_no_rdfs.json', 'r') as file:\n    original_dataset = json.load(file)\n    \n# Assuming original_dataset is your original dictionary\nflattened_texts = flatten_dataset(original_dataset)\n\n# Create a Dataset object from the flattened data\ndataset = Dataset.from_dict({\"text\": flattened_texts})","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:23.005628Z","iopub.execute_input":"2024-09-20T16:16:23.005913Z","iopub.status.idle":"2024-09-20T16:16:24.225370Z","shell.execute_reply.started":"2024-09-20T16:16:23.005869Z","shell.execute_reply":"2024-09-20T16:16:24.224401Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/finetuning-no-rdfs/validation_set_no_rdfs - Copy.json', 'r') as file:\n    val_dataset = json.load(file)\n    \n# Assuming original_dataset is your original dictionary\nval_flat = flatten_dataset(val_dataset)\n\n# Create a Dataset object from the flattened data\nval_set = Dataset.from_dict({\"text\": val_flat})","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.226478Z","iopub.execute_input":"2024-09-20T16:16:24.226756Z","iopub.status.idle":"2024-09-20T16:16:24.240584Z","shell.execute_reply.started":"2024-09-20T16:16:24.226724Z","shell.execute_reply":"2024-09-20T16:16:24.239861Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.241597Z","iopub.execute_input":"2024-09-20T16:16:24.241912Z","iopub.status.idle":"2024-09-20T16:16:24.249040Z","shell.execute_reply.started":"2024-09-20T16:16:24.241880Z","shell.execute_reply":"2024-09-20T16:16:24.248120Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: What is the population of Central Greece?\\nGenerator: '''SELECT DISTINCT ?var0 WHERE { <yago:Central_Greece_(region)> y2geoo:hasGAG_Population ?var0 }'''\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(val_set[4])","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.250076Z","iopub.execute_input":"2024-09-20T16:16:24.250390Z","iopub.status.idle":"2024-09-20T16:16:24.260071Z","shell.execute_reply.started":"2024-09-20T16:16:24.250356Z","shell.execute_reply":"2024-09-20T16:16:24.259135Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: Is Ierapetra south of Athens?\\nGenerator: '''ASK {     <yago:Ierapetra> geo:hasGeometry ?var0. ?var0 geo:asWKT ?var2.      <yago:geoentity_Dimos_Athens_8133876> geo:hasGeometry ?var3.     ?var3 geo:asWKT ?var5.   FILTER (strdf:below(?var2, ?var5)) }'''\"}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n## Step 2: Set up the model and tokenizer","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.263416Z","iopub.execute_input":"2024-09-20T16:16:24.263736Z","iopub.status.idle":"2024-09-20T16:16:37.445053Z","shell.execute_reply.started":"2024-09-20T16:16:24.263704Z","shell.execute_reply":"2024-09-20T16:16:37.444039Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\ndef __get_mistral_tokenizer() -> AutoTokenizer:\n    tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")\n    tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n    tokenizer.padding_side = 'right'\n    return tokenizer ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:37.446383Z","iopub.execute_input":"2024-09-20T16:16:37.446692Z","iopub.status.idle":"2024-09-20T16:16:41.348013Z","shell.execute_reply.started":"2024-09-20T16:16:37.446660Z","shell.execute_reply":"2024-09-20T16:16:41.347013Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nfrom peft import prepare_model_for_kbit_training\n\nmodel_id = \"alpindale/Mistral-7B-v0.2-hf\"\ntokenizer = __get_mistral_tokenizer()\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:41.349276Z","iopub.execute_input":"2024-09-20T16:16:41.349671Z","iopub.status.idle":"2024-09-20T16:18:03.962875Z","shell.execute_reply.started":"2024-09-20T16:16:41.349623Z","shell.execute_reply":"2024-09-20T16:18:03.961842Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/960 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d677ed90f4a14140a99639da5e9088c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f91d82202d4b58b382888b8887529f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf721806bc048b6b6725e2d01cdfc53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e8c8c879f84f2e84b9313bcb7ce507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c95ddd3c33b148ec81b2452ff951a49f"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358f573626144aca9bcbbce160977ef3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac40d4da949463dad9f0a48da01b76c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8617a83661e745e2b3511c819a87c7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85274c2f770452b9307e6055cc8548f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c6b318add148169321401ecbe76683"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369a4314e819477bbf6dace1d98dfbb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b0f869dcf34f208734c58d88425864"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Step 3: Set up PEFT (Parameter-Efficient Fine-Tuning)","metadata":{}},{"cell_type":"code","source":"# from peft import LoraConfig, get_peft_model\n\n# config = LoraConfig(\n#     r=32,\n#     lora_alpha=64,\n#     bias=\"none\",\n#     lora_dropout=0.05,\n#     task_type=\"CAUSAL_LM\",\n# )\n\n# model = get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:03.964479Z","iopub.execute_input":"2024-09-20T16:18:03.965462Z","iopub.status.idle":"2024-09-20T16:18:03.973163Z","shell.execute_reply.started":"2024-09-20T16:18:03.965405Z","shell.execute_reply":"2024-09-20T16:18:03.972014Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Set up the training arguments","metadata":{}},{"cell_type":"code","source":"# from transformers import TrainingArguments\n\n# args = TrainingArguments(\n#     output_dir=\"/kaggle/working/\",\n#     num_train_epochs=3, \n#     per_device_train_batch_size=8,\n#     learning_rate=1e-5,\n#     optim=\"sgd\",\n#     lr_scheduler_type='linear',\n#     logging_steps=50\n# )","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:03.978182Z","iopub.execute_input":"2024-09-20T16:18:03.978846Z","iopub.status.idle":"2024-09-20T16:18:06.710362Z","shell.execute_reply.started":"2024-09-20T16:18:03.978812Z","shell.execute_reply":"2024-09-20T16:18:06.709390Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Initialize the trainer and fine-tune the model","metadata":{}},{"cell_type":"code","source":"# import os\n\n# # Set the PYTORCH_CUDA_ALLOC_CONF environment variable\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# #API: ae46119707898af6a020629f5b1db3cb63c0dc29\n\n# from trl import SFTTrainer\n\n# trainer = SFTTrainer(\n#     model=model,\n#     args=args,\n#     train_dataset=dataset,\n#     dataset_text_field='text',\n#     max_seq_length=1024,\n# )\n\n# trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:06.711533Z","iopub.execute_input":"2024-09-20T16:18:06.711846Z","iopub.status.idle":"2024-09-20T16:18:06.719890Z","shell.execute_reply.started":"2024-09-20T16:18:06.711804Z","shell.execute_reply":"2024-09-20T16:18:06.718972Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\n# # ------------------------\n# # ----- Quantization -----\n# # ------------------------\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True, \n#     bnb_4bit_use_double_quant=True, \n#     bnb_4bit_quant_type=\"nf4\", \n#     bnb_4bit_compute_dtype=torch.bfloat16\n# )\n \n# # -----------------\n# # ----- Model -----\n# # -----------------\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL.model_id,\n#     device_map=\"auto\",\n#     torch_dtype=torch.bfloat16,\n#     quantization_config=bnb_config\n# )\nfor param in model.parameters():\n    param.requires_grad = False\n    if param.ndim ==1:\n        param.data = param.data.to(torch.float32)\n    \nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# compute_metrics(None, subset_test_dataset)\n# compute_metrics(None, dataset['test'])\n\n# ----------------\n# ----- LoRA -----\n# ----------------\npeft_config = LoraConfig(\n    lora_dropout=0.1,\n    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n    bias = 'none',\n    # modules_to_save = ['lm_head', 'embed_tokens'],\n    task_type=\"CAUSAL_LM\"\n)\n\npeft_config.save_pretrained(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:06.720988Z","iopub.execute_input":"2024-09-20T16:18:06.721377Z","iopub.status.idle":"2024-09-20T16:18:20.016910Z","shell.execute_reply.started":"2024-09-20T16:18:06.721333Z","shell.execute_reply":"2024-09-20T16:18:20.016121Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Upload the adapter config to the model repo\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/adapter_config.json\",\n    path_in_repo=\"adapter_config.json\",\n    repo_id=\"Stratos-Kakalis/New_FT_Weights\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:20.018126Z","iopub.execute_input":"2024-09-20T16:18:20.018739Z","iopub.status.idle":"2024-09-20T16:18:20.243931Z","shell.execute_reply.started":"2024-09-20T16:18:20.018704Z","shell.execute_reply":"2024-09-20T16:18:20.242977Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/New_FT_Weights/commit/4e4b309cde515910a27c57fdc8fcd74295c8bb64', commit_message='Upload adapter_config.json with huggingface_hub', commit_description='', oid='4e4b309cde515910a27c57fdc8fcd74295c8bb64', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\n# # ------------------------\n# # ----- Quantization -----\n# # ------------------------\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True, \n#     bnb_4bit_use_double_quant=True, \n#     bnb_4bit_quant_type=\"nf4\", \n#     bnb_4bit_compute_dtype=torch.bfloat16\n# )\n \n# # -----------------\n# # ----- Model -----\n# # -----------------\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL.model_id,\n#     device_map=\"auto\",\n#     torch_dtype=torch.bfloat16,\n#     quantization_config=bnb_config\n# )\nfor param in model.parameters():\n    param.requires_grad = False\n    if param.ndim ==1:\n        param.data = param.data.to(torch.float32)\n    \nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# compute_metrics(None, subset_test_dataset)\n# compute_metrics(None, dataset['test'])\n\n# ----------------\n# ----- LoRA -----\n# ----------------\npeft_config = LoraConfig(\n    lora_dropout=0.1,\n    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n    bias = 'none',\n    # modules_to_save = ['lm_head', 'embed_tokens'],\n    task_type=\"CAUSAL_LM\"\n)\n\npeft_config.save_pretrained(\"path/to/save/adapter_config\")\nmodel = get_peft_model(model, peft_config)\n \n# --------------------\n# ----- Training -----\n# --------------------\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    # Training length\n    num_train_epochs=8,\n    # Important for VRAM\n    per_device_train_batch_size=8,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    # Other\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    bf16=True,                              # use bfloat16 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    # lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    learning_rate=0.0002,\n    # Logging\n    logging_dir=\"/kaggle/working/logs/\",\n    logging_steps=10,\n    # Evaluation\n    evaluation_strategy=\"steps\",            # evaluate every 'eval_steps'\n    eval_steps=10,                         # evaluation step frequency\n    load_best_model_at_end=True,            # load the best model at the end of training\n    metric_for_best_model=\"eval_loss\",        # metric to compare the best model\n    greater_is_better=False\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n#     train_dataset=dataset['train'],\n    train_dataset=dataset,              ## ????????????\n    dataset_text_field='text',\n    eval_dataset=val_set,\n#     dataset_text_field=\"input\",\n#     compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n\ntrainer.train()\n\ntrainer.evaluate()\n\n# print(\"FULL EVALUATION\")\n# compute_metrics(None, dataset['test'])\n\n# ---------------------------------\n# ----- Upload to HuggingFace -----\n# ---------------------------------\nmodel.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\", private=True)\ntokenizer.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\",private=True) ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:20.245431Z","iopub.execute_input":"2024-09-20T16:18:20.245750Z","iopub.status.idle":"2024-09-20T21:14:53.858731Z","shell.execute_reply.started":"2024-09-20T16:18:20.245717Z","shell.execute_reply":"2024-09-20T21:14:53.857604Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, dataset_kwargs. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/754 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2692e33d988e4ac1adbc5cc58c9a5771"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/92 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"440dc8c118354015800891fa468fc461"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240920_161831-1w3pte5w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">/kaggle/working/</a></strong> to <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [376/376 4:54:01, Epoch 7/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.739600</td>\n      <td>1.559345</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.139500</td>\n      <td>0.718919</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.529900</td>\n      <td>0.432377</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.399900</td>\n      <td>0.362587</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.352200</td>\n      <td>0.328678</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.301300</td>\n      <td>0.313414</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.287600</td>\n      <td>0.303795</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.296000</td>\n      <td>0.292512</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.281900</td>\n      <td>0.283960</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.265400</td>\n      <td>0.279635</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.265900</td>\n      <td>0.271931</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.236800</td>\n      <td>0.266753</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.253300</td>\n      <td>0.264406</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.246000</td>\n      <td>0.259101</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.223500</td>\n      <td>0.261006</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.215600</td>\n      <td>0.256545</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.232300</td>\n      <td>0.253065</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.225400</td>\n      <td>0.253050</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.220300</td>\n      <td>0.247555</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.202300</td>\n      <td>0.251507</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.206300</td>\n      <td>0.246796</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.213500</td>\n      <td>0.243952</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.202500</td>\n      <td>0.245366</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.192500</td>\n      <td>0.243025</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.190400</td>\n      <td>0.244651</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.197500</td>\n      <td>0.248072</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.189000</td>\n      <td>0.240951</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.189900</td>\n      <td>0.241467</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.179500</td>\n      <td>0.244546</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.178400</td>\n      <td>0.242785</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.172000</td>\n      <td>0.241941</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.182800</td>\n      <td>0.239014</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.180800</td>\n      <td>0.240981</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.162400</td>\n      <td>0.242569</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.172300</td>\n      <td>0.242886</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.172600</td>\n      <td>0.243584</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.168300</td>\n      <td>0.242901</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 01:08]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784be7f652834bf3abbfd279500a61b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257c27890fed4a3ca64c01f8e8e95794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45653f21e03e444e8099c41b753cd231"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/norm_trunc_no_rdfs_8_epoch/commit/a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', commit_message='Upload tokenizer', commit_description='', oid='a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"api key: ae46119707898af6a020629f5b1db3cb63c0dc29","metadata":{}},{"cell_type":"code","source":"# model = PeftModel.from_pretrained(base_model, \"SKefalidis/Mistral-7B-v2-queries\") ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.860149Z","iopub.execute_input":"2024-09-20T21:14:53.860561Z","iopub.status.idle":"2024-09-20T21:14:53.866161Z","shell.execute_reply.started":"2024-09-20T21:14:53.860515Z","shell.execute_reply":"2024-09-20T21:14:53.865131Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# def __get_mistral_tokenizer() -> AutoTokenizer:\n#     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n#     tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n#     tokenizer.padding_side = 'right'\n#     return tokenizer ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.867322Z","iopub.execute_input":"2024-09-20T21:14:53.867642Z","iopub.status.idle":"2024-09-20T21:14:53.877391Z","shell.execute_reply.started":"2024-09-20T21:14:53.867601Z","shell.execute_reply":"2024-09-20T21:14:53.876581Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Merge the adapter and model back together","metadata":{}},{"cell_type":"code","source":"# adapter_model = trainer.model\n# merged_model = adapter_model.merge_and_unload()\n\n# trained_tokenizer = trainer.tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.878364Z","iopub.execute_input":"2024-09-20T21:14:53.878644Z","iopub.status.idle":"2024-09-20T21:14:53.887310Z","shell.execute_reply.started":"2024-09-20T21:14:53.878613Z","shell.execute_reply":"2024-09-20T21:14:53.886452Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Push the fine-tuned model to the Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"# repo_id = \"normalized_sparql_linear_scheduler\"\n\n# merged_model.push_to_hub(repo_id)\n# trained_tokenizer.push_to_hub(repo_id)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.888251Z","iopub.execute_input":"2024-09-20T21:14:53.888503Z","iopub.status.idle":"2024-09-20T21:14:53.897980Z","shell.execute_reply.started":"2024-09-20T21:14:53.888475Z","shell.execute_reply":"2024-09-20T21:14:53.896827Z"},"trusted":true},"execution_count":22,"outputs":[]}]}