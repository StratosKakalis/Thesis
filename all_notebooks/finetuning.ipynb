{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:02.057954Z","iopub.status.busy":"2024-09-20T16:16:02.057391Z","iopub.status.idle":"2024-09-20T16:16:02.610463Z","shell.execute_reply":"2024-09-20T16:16:02.609480Z","shell.execute_reply.started":"2024-09-20T16:16:02.057919Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["import os\n","from huggingface_hub import login\n","#token\n","login(token='token')"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-20T16:16:02.612927Z","iopub.status.busy":"2024-09-20T16:16:02.612522Z","iopub.status.idle":"2024-09-20T16:16:22.975558Z","shell.execute_reply":"2024-09-20T16:16:22.974369Z","shell.execute_reply.started":"2024-09-20T16:16:02.612881Z"},"trusted":true},"outputs":[],"source":["!pip install -q datasets accelerate evaluate trl accelerate bitsandbytes peft"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.977212Z","iopub.status.busy":"2024-09-20T16:16:22.976874Z","iopub.status.idle":"2024-09-20T16:16:22.983017Z","shell.execute_reply":"2024-09-20T16:16:22.981980Z","shell.execute_reply.started":"2024-09-20T16:16:22.977178Z"},"trusted":true},"outputs":[],"source":["def create_prompt(question, answer):\n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","\n","Human: {question}\n","Generator: '''{answer}'''\"\"\"\n","    return prompt"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.985425Z","iopub.status.busy":"2024-09-20T16:16:22.985017Z","iopub.status.idle":"2024-09-20T16:16:22.994599Z","shell.execute_reply":"2024-09-20T16:16:22.993814Z","shell.execute_reply.started":"2024-09-20T16:16:22.985392Z"},"trusted":true},"outputs":[],"source":["import re \n","\n","def normalize_variables(query):\n","    if query is None:\n","        return \"\"\n","    variable_pattern = re.compile(r\"\\?\\w+\")\n","    variables = variable_pattern.findall(query)\n","    normalized_query = query\n","    for i, var in enumerate(variables):\n","        normalized_query = normalized_query.replace(var, f\"?var{i}\")\n","    return normalized_query"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.995929Z","iopub.status.busy":"2024-09-20T16:16:22.995629Z","iopub.status.idle":"2024-09-20T16:16:23.004667Z","shell.execute_reply":"2024-09-20T16:16:23.003809Z","shell.execute_reply.started":"2024-09-20T16:16:22.995889Z"},"trusted":true},"outputs":[],"source":["# This prefix map will be used to shrink the uri's down to the prefix level, to help the model better understand them and decrease mistakes.\n","prefix_map = {\"http://www.opengis.net/ont/geosparql#\" : \"geo:\",\n","               \"http://www.opengis.net/def/function/geosparql/\" : \"geof:\",\n","               \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" : \"rdf:\",\n","               \"http://www.w3.org/2000/01/rdf-schema#\" : \"rdfs:\",\n","               \"http://www.w3.org/2001/XMLSchema#\" : \"xsd:\",\n","               \"http://yago-knowledge.org/resource/\" : \"yago:\",\n","               \"http://kr.di.uoa.gr/yago2geo/resource/\" : \"y2geor:\",\n","               \"http://kr.di.uoa.gr/yago2geo/ontology/\" : \"y2geoo:\",\n","               \"http://strdf.di.uoa.gr/ontology#\" : \"strdf:\",\n","               \"http://www.opengis.net/def/uom/OGC/1.0/\" : \"uom:\",\n","               \"http://www.w3.org/2002/07/owl#\" : \"owl:\"}"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:23.005913Z","iopub.status.busy":"2024-09-20T16:16:23.005628Z","iopub.status.idle":"2024-09-20T16:16:24.225370Z","shell.execute_reply":"2024-09-20T16:16:24.224401Z","shell.execute_reply.started":"2024-09-20T16:16:23.005869Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","import json\n","\n","def flatten_dataset(original_dataset):\n","    flattened_data = []\n","    \n","    # Iterate through each numbered key in the dataset\n","    for key in original_dataset:\n","        if key.isdigit():  # Ensure we're only processing the numbered keys\n","            item = original_dataset[key]  # Access the single dictionary in the list\n","            # Create a new entry combining question and answer\n","            query = normalize_variables(item['Query'])\n","            # Shorten the uris down to prefixes.\n","            for uri_map, prefix in prefix_map.items():\n","                query = query.replace(uri_map, prefix)\n","            prompt = create_prompt(item['Question'], query)\n","            flattened_data.append(prompt)\n","    \n","    return flattened_data\n","\n","with open('/kaggle/input/finetuning-no-rdfs/training_set_no_rdfs.json', 'r') as file:\n","    original_dataset = json.load(file)\n","    \n","# Assuming original_dataset is your original dictionary\n","flattened_texts = flatten_dataset(original_dataset)\n","\n","# Create a Dataset object from the flattened data\n","dataset = Dataset.from_dict({\"text\": flattened_texts})"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.226756Z","iopub.status.busy":"2024-09-20T16:16:24.226478Z","iopub.status.idle":"2024-09-20T16:16:24.240584Z","shell.execute_reply":"2024-09-20T16:16:24.239861Z","shell.execute_reply.started":"2024-09-20T16:16:24.226724Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/finetuning-no-rdfs/validation_set_no_rdfs - Copy.json', 'r') as file:\n","    val_dataset = json.load(file)\n","    \n","# Assuming original_dataset is your original dictionary\n","val_flat = flatten_dataset(val_dataset)\n","\n","# Create a Dataset object from the flattened data\n","val_set = Dataset.from_dict({\"text\": val_flat})"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.241912Z","iopub.status.busy":"2024-09-20T16:16:24.241597Z","iopub.status.idle":"2024-09-20T16:16:24.249040Z","shell.execute_reply":"2024-09-20T16:16:24.248120Z","shell.execute_reply.started":"2024-09-20T16:16:24.241880Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: What is the population of Central Greece?\\nGenerator: '''SELECT DISTINCT ?var0 WHERE { <yago:Central_Greece_(region)> y2geoo:hasGAG_Population ?var0 }'''\"}\n"]}],"source":["print(dataset[0])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.250390Z","iopub.status.busy":"2024-09-20T16:16:24.250076Z","iopub.status.idle":"2024-09-20T16:16:24.260071Z","shell.execute_reply":"2024-09-20T16:16:24.259135Z","shell.execute_reply.started":"2024-09-20T16:16:24.250356Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: Is Ierapetra south of Athens?\\nGenerator: '''ASK {     <yago:Ierapetra> geo:hasGeometry ?var0. ?var0 geo:asWKT ?var2.      <yago:geoentity_Dimos_Athens_8133876> geo:hasGeometry ?var3.     ?var3 geo:asWKT ?var5.   FILTER (strdf:below(?var2, ?var5)) }'''\"}\n"]}],"source":["print(val_set[4])"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Step 2: Set up the model and tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.263736Z","iopub.status.busy":"2024-09-20T16:16:24.263416Z","iopub.status.idle":"2024-09-20T16:16:37.445053Z","shell.execute_reply":"2024-09-20T16:16:37.444039Z","shell.execute_reply.started":"2024-09-20T16:16:24.263704Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.12.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"]}],"source":["!pip install peft"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:37.446692Z","iopub.status.busy":"2024-09-20T16:16:37.446383Z","iopub.status.idle":"2024-09-20T16:16:41.348013Z","shell.execute_reply":"2024-09-20T16:16:41.347013Z","shell.execute_reply.started":"2024-09-20T16:16:37.446660Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","def __get_mistral_tokenizer() -> AutoTokenizer:\n","    tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")\n","    tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n","    tokenizer.padding_side = 'right'\n","    return tokenizer "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:41.349671Z","iopub.status.busy":"2024-09-20T16:16:41.349276Z","iopub.status.idle":"2024-09-20T16:18:03.962875Z","shell.execute_reply":"2024-09-20T16:18:03.961842Z","shell.execute_reply.started":"2024-09-20T16:16:41.349623Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d677ed90f4a14140a99639da5e9088c6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/960 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0f91d82202d4b58b382888b8887529f","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecf721806bc048b6b6725e2d01cdfc53","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10e8c8c879f84f2e84b9313bcb7ce507","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c95ddd3c33b148ec81b2452ff951a49f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"358f573626144aca9bcbbce160977ef3","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fac40d4da949463dad9f0a48da01b76c","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8617a83661e745e2b3511c819a87c7bd","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f85274c2f770452b9307e6055cc8548f","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74c6b318add148169321401ecbe76683","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"369a4314e819477bbf6dace1d98dfbb1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0b0f869dcf34f208734c58d88425864","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","from peft import prepare_model_for_kbit_training\n","\n","model_id = \"alpindale/Mistral-7B-v0.2-hf\"\n","tokenizer = __get_mistral_tokenizer()\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Set up PEFT (Parameter-Efficient Fine-Tuning)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:06.721377Z","iopub.status.busy":"2024-09-20T16:18:06.720988Z","iopub.status.idle":"2024-09-20T16:18:20.016910Z","shell.execute_reply":"2024-09-20T16:18:20.016121Z","shell.execute_reply.started":"2024-09-20T16:18:06.721333Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","# # ------------------------\n","# # ----- Quantization -----\n","# # ------------------------\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True, \n","#     bnb_4bit_use_double_quant=True, \n","#     bnb_4bit_quant_type=\"nf4\", \n","#     bnb_4bit_compute_dtype=torch.bfloat16\n","# )\n"," \n","# # -----------------\n","# # ----- Model -----\n","# # -----------------\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL.model_id,\n","#     device_map=\"auto\",\n","#     torch_dtype=torch.bfloat16,\n","#     quantization_config=bnb_config\n","# )\n","for param in model.parameters():\n","    param.requires_grad = False\n","    if param.ndim ==1:\n","        param.data = param.data.to(torch.float32)\n","    \n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","# compute_metrics(None, subset_test_dataset)\n","# compute_metrics(None, dataset['test'])\n","\n","# ----------------\n","# ----- LoRA -----\n","# ----------------\n","peft_config = LoraConfig(\n","    lora_dropout=0.1,\n","    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n","    bias = 'none',\n","    # modules_to_save = ['lm_head', 'embed_tokens'],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","peft_config.save_pretrained(\"/kaggle/working\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:20.018739Z","iopub.status.busy":"2024-09-20T16:18:20.018126Z","iopub.status.idle":"2024-09-20T16:18:20.243931Z","shell.execute_reply":"2024-09-20T16:18:20.242977Z","shell.execute_reply.started":"2024-09-20T16:18:20.018704Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/New_FT_Weights/commit/4e4b309cde515910a27c57fdc8fcd74295c8bb64', commit_message='Upload adapter_config.json with huggingface_hub', commit_description='', oid='4e4b309cde515910a27c57fdc8fcd74295c8bb64', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi\n","\n","# Upload the adapter config to the model repo\n","api = HfApi()\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/adapter_config.json\",\n","    path_in_repo=\"adapter_config.json\",\n","    repo_id=\"Stratos-Kakalis/New_FT_Weights\",\n","    repo_type=\"model\",\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:20.245750Z","iopub.status.busy":"2024-09-20T16:18:20.245431Z","iopub.status.idle":"2024-09-20T21:14:53.858731Z","shell.execute_reply":"2024-09-20T21:14:53.857604Z","shell.execute_reply.started":"2024-09-20T16:18:20.245717Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, dataset_kwargs. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2692e33d988e4ac1adbc5cc58c9a5771","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/754 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"440dc8c118354015800891fa468fc461","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/92 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.18.1 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240920_161831-1w3pte5w</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">/kaggle/working/</a></strong> to <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [376/376 4:54:01, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.739600</td>\n","      <td>1.559345</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.139500</td>\n","      <td>0.718919</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.529900</td>\n","      <td>0.432377</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.399900</td>\n","      <td>0.362587</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.352200</td>\n","      <td>0.328678</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.301300</td>\n","      <td>0.313414</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.287600</td>\n","      <td>0.303795</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.296000</td>\n","      <td>0.292512</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.281900</td>\n","      <td>0.283960</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.265400</td>\n","      <td>0.279635</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.265900</td>\n","      <td>0.271931</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.236800</td>\n","      <td>0.266753</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.253300</td>\n","      <td>0.264406</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.246000</td>\n","      <td>0.259101</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.223500</td>\n","      <td>0.261006</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.215600</td>\n","      <td>0.256545</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.232300</td>\n","      <td>0.253065</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.225400</td>\n","      <td>0.253050</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.220300</td>\n","      <td>0.247555</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.202300</td>\n","      <td>0.251507</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.206300</td>\n","      <td>0.246796</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.213500</td>\n","      <td>0.243952</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.202500</td>\n","      <td>0.245366</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.192500</td>\n","      <td>0.243025</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.190400</td>\n","      <td>0.244651</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.197500</td>\n","      <td>0.248072</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.189000</td>\n","      <td>0.240951</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.189900</td>\n","      <td>0.241467</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.179500</td>\n","      <td>0.244546</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.178400</td>\n","      <td>0.242785</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.172000</td>\n","      <td>0.241941</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.182800</td>\n","      <td>0.239014</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.180800</td>\n","      <td>0.240981</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.162400</td>\n","      <td>0.242569</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.172300</td>\n","      <td>0.242886</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.172600</td>\n","      <td>0.243584</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.168300</td>\n","      <td>0.242901</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 01:08]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"784be7f652834bf3abbfd279500a61b1","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"257c27890fed4a3ca64c01f8e8e95794","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45653f21e03e444e8099c41b753cd231","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/norm_trunc_no_rdfs_8_epoch/commit/a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', commit_message='Upload tokenizer', commit_description='', oid='a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from peft import LoraConfig, get_peft_model\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","# # ------------------------\n","# # ----- Quantization -----\n","# # ------------------------\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True, \n","#     bnb_4bit_use_double_quant=True, \n","#     bnb_4bit_quant_type=\"nf4\", \n","#     bnb_4bit_compute_dtype=torch.bfloat16\n","# )\n"," \n","# # -----------------\n","# # ----- Model -----\n","# # -----------------\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL.model_id,\n","#     device_map=\"auto\",\n","#     torch_dtype=torch.bfloat16,\n","#     quantization_config=bnb_config\n","# )\n","for param in model.parameters():\n","    param.requires_grad = False\n","    if param.ndim ==1:\n","        param.data = param.data.to(torch.float32)\n","    \n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","# compute_metrics(None, subset_test_dataset)\n","# compute_metrics(None, dataset['test'])\n","\n","# ----------------\n","# ----- LoRA -----\n","# ----------------\n","peft_config = LoraConfig(\n","    lora_dropout=0.1,\n","    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n","    bias = 'none',\n","    # modules_to_save = ['lm_head', 'embed_tokens'],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","peft_config.save_pretrained(\"path/to/save/adapter_config\")\n","model = get_peft_model(model, peft_config)\n"," \n","# --------------------\n","# ----- Training -----\n","# --------------------\n","args = TrainingArguments(\n","    output_dir=\"/kaggle/working/\",\n","    # Training length\n","    num_train_epochs=8,\n","    # Important for VRAM\n","    per_device_train_batch_size=8,          # batch size per device during training\n","    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n","    # Other\n","    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n","    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n","    bf16=True,                              # use bfloat16 precision\n","    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n","    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n","    # lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n","    learning_rate=0.0002,\n","    # Logging\n","    logging_dir=\"/kaggle/working/logs/\",\n","    logging_steps=10,\n","    # Evaluation\n","    evaluation_strategy=\"steps\",            # evaluate every 'eval_steps'\n","    eval_steps=10,                         # evaluation step frequency\n","    load_best_model_at_end=True,            # load the best model at the end of training\n","    metric_for_best_model=\"eval_loss\",        # metric to compare the best model\n","    greater_is_better=False\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","#     train_dataset=dataset['train'],\n","    train_dataset=dataset,              ## ????????????\n","    dataset_text_field='text',\n","    eval_dataset=val_set,\n","#     dataset_text_field=\"input\",\n","#     compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,  # We template with special tokens\n","        \"append_concat_token\": False, # No need to add additional separator token\n","    }\n",")\n","\n","trainer.train()\n","\n","trainer.evaluate()\n","\n","# print(\"FULL EVALUATION\")\n","# compute_metrics(None, dataset['test'])\n","\n","# ---------------------------------\n","# ----- Upload to HuggingFace -----\n","# ---------------------------------\n","model.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\", private=True)\n","tokenizer.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\",private=True) "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5299649,"sourceId":8812575,"sourceType":"datasetVersion"},{"datasetId":5628448,"sourceId":9296326,"sourceType":"datasetVersion"},{"datasetId":5728478,"sourceId":9429401,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
