{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8812575,"sourceType":"datasetVersion","datasetId":5299649},{"sourceId":9296326,"sourceType":"datasetVersion","datasetId":5628448},{"sourceId":9429401,"sourceType":"datasetVersion","datasetId":5728478}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n#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\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:02.057391Z","iopub.execute_input":"2024-09-20T16:16:02.057954Z","iopub.status.idle":"2024-09-20T16:16:02.610463Z","shell.execute_reply.started":"2024-09-20T16:16:02.057919Z","shell.execute_reply":"2024-09-20T16:16:02.609480Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q datasets accelerate evaluate trl accelerate bitsandbytes peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-20T16:16:02.612522Z","iopub.execute_input":"2024-09-20T16:16:02.612927Z","iopub.status.idle":"2024-09-20T16:16:22.975558Z","shell.execute_reply.started":"2024-09-20T16:16:02.612881Z","shell.execute_reply":"2024-09-20T16:16:22.974369Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def create_prompt(question, answer):\n    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n\nHuman: {question}\nGenerator: '''{answer}'''\"\"\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:22.976874Z","iopub.execute_input":"2024-09-20T16:16:22.977212Z","iopub.status.idle":"2024-09-20T16:16:22.983017Z","shell.execute_reply.started":"2024-09-20T16:16:22.977178Z","shell.execute_reply":"2024-09-20T16:16:22.981980Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import re \n\ndef normalize_variables(query):\n    if query is None:\n        return \"\"\n    variable_pattern = re.compile(r\"\\?\\w+\")\n    variables = variable_pattern.findall(query)\n    normalized_query = query\n    for i, var in enumerate(variables):\n        normalized_query = normalized_query.replace(var, f\"?var{i}\")\n    return normalized_query","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:22.985017Z","iopub.execute_input":"2024-09-20T16:16:22.985425Z","iopub.status.idle":"2024-09-20T16:16:22.994599Z","shell.execute_reply.started":"2024-09-20T16:16:22.985392Z","shell.execute_reply":"2024-09-20T16:16:22.993814Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# This prefix map will be used to shrink the uri's down to the prefix level, to help the model better understand them and decrease mistakes.\nprefix_map = {\"http://www.opengis.net/ont/geosparql#\" : \"geo:\",\n               \"http://www.opengis.net/def/function/geosparql/\" : \"geof:\",\n               \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" : \"rdf:\",\n               \"http://www.w3.org/2000/01/rdf-schema#\" : \"rdfs:\",\n               \"http://www.w3.org/2001/XMLSchema#\" : \"xsd:\",\n               \"http://yago-knowledge.org/resource/\" : \"yago:\",\n               \"http://kr.di.uoa.gr/yago2geo/resource/\" : \"y2geor:\",\n               \"http://kr.di.uoa.gr/yago2geo/ontology/\" : \"y2geoo:\",\n               \"http://strdf.di.uoa.gr/ontology#\" : \"strdf:\",\n               \"http://www.opengis.net/def/uom/OGC/1.0/\" : \"uom:\",\n               \"http://www.w3.org/2002/07/owl#\" : \"owl:\"}","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:22.995629Z","iopub.execute_input":"2024-09-20T16:16:22.995929Z","iopub.status.idle":"2024-09-20T16:16:23.004667Z","shell.execute_reply.started":"2024-09-20T16:16:22.995889Z","shell.execute_reply":"2024-09-20T16:16:23.003809Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport json\n\ndef flatten_dataset(original_dataset):\n    flattened_data = []\n    \n    # Iterate through each numbered key in the dataset\n    for key in original_dataset:\n        if key.isdigit():  # Ensure we're only processing the numbered keys\n            item = original_dataset[key]  # Access the single dictionary in the list\n            # Create a new entry combining question and answer\n            query = normalize_variables(item['Query'])\n            # Shorten the uris down to prefixes.\n            for uri_map, prefix in prefix_map.items():\n                query = query.replace(uri_map, prefix)\n            prompt = create_prompt(item['Question'], query)\n            flattened_data.append(prompt)\n    \n    return flattened_data\n\nwith open('/kaggle/input/finetuning-no-rdfs/training_set_no_rdfs.json', 'r') as file:\n    original_dataset = json.load(file)\n    \n# Assuming original_dataset is your original dictionary\nflattened_texts = flatten_dataset(original_dataset)\n\n# Create a Dataset object from the flattened data\ndataset = Dataset.from_dict({\"text\": flattened_texts})","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:23.005628Z","iopub.execute_input":"2024-09-20T16:16:23.005913Z","iopub.status.idle":"2024-09-20T16:16:24.225370Z","shell.execute_reply.started":"2024-09-20T16:16:23.005869Z","shell.execute_reply":"2024-09-20T16:16:24.224401Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/finetuning-no-rdfs/validation_set_no_rdfs - Copy.json', 'r') as file:\n    val_dataset = json.load(file)\n    \n# Assuming original_dataset is your original dictionary\nval_flat = flatten_dataset(val_dataset)\n\n# Create a Dataset object from the flattened data\nval_set = Dataset.from_dict({\"text\": val_flat})","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.226478Z","iopub.execute_input":"2024-09-20T16:16:24.226756Z","iopub.status.idle":"2024-09-20T16:16:24.240584Z","shell.execute_reply.started":"2024-09-20T16:16:24.226724Z","shell.execute_reply":"2024-09-20T16:16:24.239861Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.241597Z","iopub.execute_input":"2024-09-20T16:16:24.241912Z","iopub.status.idle":"2024-09-20T16:16:24.249040Z","shell.execute_reply.started":"2024-09-20T16:16:24.241880Z","shell.execute_reply":"2024-09-20T16:16:24.248120Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: What is the population of Central Greece?\\nGenerator: '''SELECT DISTINCT ?var0 WHERE { <yago:Central_Greece_(region)> y2geoo:hasGAG_Population ?var0 }'''\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(val_set[4])","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.250076Z","iopub.execute_input":"2024-09-20T16:16:24.250390Z","iopub.status.idle":"2024-09-20T16:16:24.260071Z","shell.execute_reply.started":"2024-09-20T16:16:24.250356Z","shell.execute_reply":"2024-09-20T16:16:24.259135Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: Is Ierapetra south of Athens?\\nGenerator: '''ASK {     <yago:Ierapetra> geo:hasGeometry ?var0. ?var0 geo:asWKT ?var2.      <yago:geoentity_Dimos_Athens_8133876> geo:hasGeometry ?var3.     ?var3 geo:asWKT ?var5.   FILTER (strdf:below(?var2, ?var5)) }'''\"}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n## Step 2: Set up the model and tokenizer","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:24.263416Z","iopub.execute_input":"2024-09-20T16:16:24.263736Z","iopub.status.idle":"2024-09-20T16:16:37.445053Z","shell.execute_reply.started":"2024-09-20T16:16:24.263704Z","shell.execute_reply":"2024-09-20T16:16:37.444039Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\ndef __get_mistral_tokenizer() -> AutoTokenizer:\n    tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")\n    tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n    tokenizer.padding_side = 'right'\n    return tokenizer ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:37.446383Z","iopub.execute_input":"2024-09-20T16:16:37.446692Z","iopub.status.idle":"2024-09-20T16:16:41.348013Z","shell.execute_reply.started":"2024-09-20T16:16:37.446660Z","shell.execute_reply":"2024-09-20T16:16:41.347013Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nfrom peft import prepare_model_for_kbit_training\n\nmodel_id = \"alpindale/Mistral-7B-v0.2-hf\"\ntokenizer = __get_mistral_tokenizer()\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:16:41.349276Z","iopub.execute_input":"2024-09-20T16:16:41.349671Z","iopub.status.idle":"2024-09-20T16:18:03.962875Z","shell.execute_reply.started":"2024-09-20T16:16:41.349623Z","shell.execute_reply":"2024-09-20T16:18:03.961842Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/960 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d677ed90f4a14140a99639da5e9088c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f91d82202d4b58b382888b8887529f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf721806bc048b6b6725e2d01cdfc53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e8c8c879f84f2e84b9313bcb7ce507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c95ddd3c33b148ec81b2452ff951a49f"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358f573626144aca9bcbbce160977ef3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac40d4da949463dad9f0a48da01b76c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8617a83661e745e2b3511c819a87c7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85274c2f770452b9307e6055cc8548f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c6b318add148169321401ecbe76683"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369a4314e819477bbf6dace1d98dfbb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b0f869dcf34f208734c58d88425864"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Step 3: Set up PEFT (Parameter-Efficient Fine-Tuning)","metadata":{}},{"cell_type":"code","source":"# from peft import LoraConfig, get_peft_model\n\n# config = LoraConfig(\n#     r=32,\n#     lora_alpha=64,\n#     bias=\"none\",\n#     lora_dropout=0.05,\n#     task_type=\"CAUSAL_LM\",\n# )\n\n# model = get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:03.964479Z","iopub.execute_input":"2024-09-20T16:18:03.965462Z","iopub.status.idle":"2024-09-20T16:18:03.973163Z","shell.execute_reply.started":"2024-09-20T16:18:03.965405Z","shell.execute_reply":"2024-09-20T16:18:03.972014Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Set up the training arguments","metadata":{}},{"cell_type":"code","source":"# from transformers import TrainingArguments\n\n# args = TrainingArguments(\n#     output_dir=\"/kaggle/working/\",\n#     num_train_epochs=3, \n#     per_device_train_batch_size=8,\n#     learning_rate=1e-5,\n#     optim=\"sgd\",\n#     lr_scheduler_type='linear',\n#     logging_steps=50\n# )","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:03.978182Z","iopub.execute_input":"2024-09-20T16:18:03.978846Z","iopub.status.idle":"2024-09-20T16:18:06.710362Z","shell.execute_reply.started":"2024-09-20T16:18:03.978812Z","shell.execute_reply":"2024-09-20T16:18:06.709390Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Initialize the trainer and fine-tune the model","metadata":{}},{"cell_type":"code","source":"# import os\n\n# # Set the PYTORCH_CUDA_ALLOC_CONF environment variable\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# #API: ae46119707898af6a020629f5b1db3cb63c0dc29\n\n# from trl import SFTTrainer\n\n# trainer = SFTTrainer(\n#     model=model,\n#     args=args,\n#     train_dataset=dataset,\n#     dataset_text_field='text',\n#     max_seq_length=1024,\n# )\n\n# trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:06.711533Z","iopub.execute_input":"2024-09-20T16:18:06.711846Z","iopub.status.idle":"2024-09-20T16:18:06.719890Z","shell.execute_reply.started":"2024-09-20T16:18:06.711804Z","shell.execute_reply":"2024-09-20T16:18:06.718972Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\n# # ------------------------\n# # ----- Quantization -----\n# # ------------------------\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True, \n#     bnb_4bit_use_double_quant=True, \n#     bnb_4bit_quant_type=\"nf4\", \n#     bnb_4bit_compute_dtype=torch.bfloat16\n# )\n \n# # -----------------\n# # ----- Model -----\n# # -----------------\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL.model_id,\n#     device_map=\"auto\",\n#     torch_dtype=torch.bfloat16,\n#     quantization_config=bnb_config\n# )\nfor param in model.parameters():\n    param.requires_grad = False\n    if param.ndim ==1:\n        param.data = param.data.to(torch.float32)\n    \nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# compute_metrics(None, subset_test_dataset)\n# compute_metrics(None, dataset['test'])\n\n# ----------------\n# ----- LoRA -----\n# ----------------\npeft_config = LoraConfig(\n    lora_dropout=0.1,\n    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n    bias = 'none',\n    # modules_to_save = ['lm_head', 'embed_tokens'],\n    task_type=\"CAUSAL_LM\"\n)\n\npeft_config.save_pretrained(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:06.720988Z","iopub.execute_input":"2024-09-20T16:18:06.721377Z","iopub.status.idle":"2024-09-20T16:18:20.016910Z","shell.execute_reply.started":"2024-09-20T16:18:06.721333Z","shell.execute_reply":"2024-09-20T16:18:20.016121Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Upload the adapter config to the model repo\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/adapter_config.json\",\n    path_in_repo=\"adapter_config.json\",\n    repo_id=\"Stratos-Kakalis/New_FT_Weights\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:20.018126Z","iopub.execute_input":"2024-09-20T16:18:20.018739Z","iopub.status.idle":"2024-09-20T16:18:20.243931Z","shell.execute_reply.started":"2024-09-20T16:18:20.018704Z","shell.execute_reply":"2024-09-20T16:18:20.242977Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/New_FT_Weights/commit/4e4b309cde515910a27c57fdc8fcd74295c8bb64', commit_message='Upload adapter_config.json with huggingface_hub', commit_description='', oid='4e4b309cde515910a27c57fdc8fcd74295c8bb64', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\n# # ------------------------\n# # ----- Quantization -----\n# # ------------------------\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True, \n#     bnb_4bit_use_double_quant=True, \n#     bnb_4bit_quant_type=\"nf4\", \n#     bnb_4bit_compute_dtype=torch.bfloat16\n# )\n \n# # -----------------\n# # ----- Model -----\n# # -----------------\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL.model_id,\n#     device_map=\"auto\",\n#     torch_dtype=torch.bfloat16,\n#     quantization_config=bnb_config\n# )\nfor param in model.parameters():\n    param.requires_grad = False\n    if param.ndim ==1:\n        param.data = param.data.to(torch.float32)\n    \nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# compute_metrics(None, subset_test_dataset)\n# compute_metrics(None, dataset['test'])\n\n# ----------------\n# ----- LoRA -----\n# ----------------\npeft_config = LoraConfig(\n    lora_dropout=0.1,\n    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n    bias = 'none',\n    # modules_to_save = ['lm_head', 'embed_tokens'],\n    task_type=\"CAUSAL_LM\"\n)\n\npeft_config.save_pretrained(\"path/to/save/adapter_config\")\nmodel = get_peft_model(model, peft_config)\n \n# --------------------\n# ----- Training -----\n# --------------------\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    # Training length\n    num_train_epochs=8,\n    # Important for VRAM\n    per_device_train_batch_size=8,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    # Other\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    bf16=True,                              # use bfloat16 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    # lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    learning_rate=0.0002,\n    # Logging\n    logging_dir=\"/kaggle/working/logs/\",\n    logging_steps=10,\n    # Evaluation\n    evaluation_strategy=\"steps\",            # evaluate every 'eval_steps'\n    eval_steps=10,                         # evaluation step frequency\n    load_best_model_at_end=True,            # load the best model at the end of training\n    metric_for_best_model=\"eval_loss\",        # metric to compare the best model\n    greater_is_better=False\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n#     train_dataset=dataset['train'],\n    train_dataset=dataset,              ## ????????????\n    dataset_text_field='text',\n    eval_dataset=val_set,\n#     dataset_text_field=\"input\",\n#     compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n\ntrainer.train()\n\ntrainer.evaluate()\n\n# print(\"FULL EVALUATION\")\n# compute_metrics(None, dataset['test'])\n\n# ---------------------------------\n# ----- Upload to HuggingFace -----\n# ---------------------------------\nmodel.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\", private=True)\ntokenizer.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\",private=True) ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:18:20.245431Z","iopub.execute_input":"2024-09-20T16:18:20.245750Z","iopub.status.idle":"2024-09-20T21:14:53.858731Z","shell.execute_reply.started":"2024-09-20T16:18:20.245717Z","shell.execute_reply":"2024-09-20T21:14:53.857604Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, dataset_kwargs. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/754 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2692e33d988e4ac1adbc5cc58c9a5771"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/92 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"440dc8c118354015800891fa468fc461"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240920_161831-1w3pte5w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">/kaggle/working/</a></strong> to <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [376/376 4:54:01, Epoch 7/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.739600</td>\n      <td>1.559345</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.139500</td>\n      <td>0.718919</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.529900</td>\n      <td>0.432377</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.399900</td>\n      <td>0.362587</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.352200</td>\n      <td>0.328678</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.301300</td>\n      <td>0.313414</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.287600</td>\n      <td>0.303795</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.296000</td>\n      <td>0.292512</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.281900</td>\n      <td>0.283960</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.265400</td>\n      <td>0.279635</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.265900</td>\n      <td>0.271931</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.236800</td>\n      <td>0.266753</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.253300</td>\n      <td>0.264406</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.246000</td>\n      <td>0.259101</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.223500</td>\n      <td>0.261006</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.215600</td>\n      <td>0.256545</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.232300</td>\n      <td>0.253065</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.225400</td>\n      <td>0.253050</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.220300</td>\n      <td>0.247555</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.202300</td>\n      <td>0.251507</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.206300</td>\n      <td>0.246796</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.213500</td>\n      <td>0.243952</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.202500</td>\n      <td>0.245366</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.192500</td>\n      <td>0.243025</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.190400</td>\n      <td>0.244651</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.197500</td>\n      <td>0.248072</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.189000</td>\n      <td>0.240951</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.189900</td>\n      <td>0.241467</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.179500</td>\n      <td>0.244546</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.178400</td>\n      <td>0.242785</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.172000</td>\n      <td>0.241941</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.182800</td>\n      <td>0.239014</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.180800</td>\n      <td>0.240981</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.162400</td>\n      <td>0.242569</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.172300</td>\n      <td>0.242886</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.172600</td>\n      <td>0.243584</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.168300</td>\n      <td>0.242901</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 01:08]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784be7f652834bf3abbfd279500a61b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257c27890fed4a3ca64c01f8e8e95794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45653f21e03e444e8099c41b753cd231"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/norm_trunc_no_rdfs_8_epoch/commit/a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', commit_message='Upload tokenizer', commit_description='', oid='a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"api key: ae46119707898af6a020629f5b1db3cb63c0dc29","metadata":{}},{"cell_type":"code","source":"# model = PeftModel.from_pretrained(base_model, \"SKefalidis/Mistral-7B-v2-queries\") ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.860149Z","iopub.execute_input":"2024-09-20T21:14:53.860561Z","iopub.status.idle":"2024-09-20T21:14:53.866161Z","shell.execute_reply.started":"2024-09-20T21:14:53.860515Z","shell.execute_reply":"2024-09-20T21:14:53.865131Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# def __get_mistral_tokenizer() -> AutoTokenizer:\n#     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n#     tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n#     tokenizer.padding_side = 'right'\n#     return tokenizer ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.867322Z","iopub.execute_input":"2024-09-20T21:14:53.867642Z","iopub.status.idle":"2024-09-20T21:14:53.877391Z","shell.execute_reply.started":"2024-09-20T21:14:53.867601Z","shell.execute_reply":"2024-09-20T21:14:53.876581Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Merge the adapter and model back together","metadata":{}},{"cell_type":"code","source":"# adapter_model = trainer.model\n# merged_model = adapter_model.merge_and_unload()\n\n# trained_tokenizer = trainer.tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.878364Z","iopub.execute_input":"2024-09-20T21:14:53.878644Z","iopub.status.idle":"2024-09-20T21:14:53.887310Z","shell.execute_reply.started":"2024-09-20T21:14:53.878613Z","shell.execute_reply":"2024-09-20T21:14:53.886452Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Push the fine-tuned model to the Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"# repo_id = \"normalized_sparql_linear_scheduler\"\n\n# merged_model.push_to_hub(repo_id)\n# trained_tokenizer.push_to_hub(repo_id)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:14:53.888251Z","iopub.execute_input":"2024-09-20T21:14:53.888503Z","iopub.status.idle":"2024-09-20T21:14:53.897980Z","shell.execute_reply.started":"2024-09-20T21:14:53.888475Z","shell.execute_reply":"2024-09-20T21:14:53.896827Z"},"trusted":true},"execution_count":22,"outputs":[]}]}