{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:02.057954Z","iopub.status.busy":"2024-09-20T16:16:02.057391Z","iopub.status.idle":"2024-09-20T16:16:02.610463Z","shell.execute_reply":"2024-09-20T16:16:02.609480Z","shell.execute_reply.started":"2024-09-20T16:16:02.057919Z"},"trusted":true},"outputs":[],"source":["import os\n","from huggingface_hub import login\n","#token\n","login(token='token')"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-20T16:16:02.612927Z","iopub.status.busy":"2024-09-20T16:16:02.612522Z","iopub.status.idle":"2024-09-20T16:16:22.975558Z","shell.execute_reply":"2024-09-20T16:16:22.974369Z","shell.execute_reply.started":"2024-09-20T16:16:02.612881Z"},"trusted":true},"outputs":[],"source":["!pip install -q datasets accelerate evaluate trl accelerate bitsandbytes peft"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.977212Z","iopub.status.busy":"2024-09-20T16:16:22.976874Z","iopub.status.idle":"2024-09-20T16:16:22.983017Z","shell.execute_reply":"2024-09-20T16:16:22.981980Z","shell.execute_reply.started":"2024-09-20T16:16:22.977178Z"},"trusted":true},"outputs":[],"source":["def create_prompt(question, answer):\n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","\n","Human: {question}\n","Generator: '''{answer}'''\"\"\"\n","    return prompt"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.985425Z","iopub.status.busy":"2024-09-20T16:16:22.985017Z","iopub.status.idle":"2024-09-20T16:16:22.994599Z","shell.execute_reply":"2024-09-20T16:16:22.993814Z","shell.execute_reply.started":"2024-09-20T16:16:22.985392Z"},"trusted":true},"outputs":[],"source":["import re \n","\n","def normalize_variables(query):\n","    if query is None:\n","        return \"\"\n","    variable_pattern = re.compile(r\"\\?\\w+\")\n","    variables = variable_pattern.findall(query)\n","    normalized_query = query\n","    for i, var in enumerate(variables):\n","        normalized_query = normalized_query.replace(var, f\"?var{i}\")\n","    return normalized_query"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.995929Z","iopub.status.busy":"2024-09-20T16:16:22.995629Z","iopub.status.idle":"2024-09-20T16:16:23.004667Z","shell.execute_reply":"2024-09-20T16:16:23.003809Z","shell.execute_reply.started":"2024-09-20T16:16:22.995889Z"},"trusted":true},"outputs":[],"source":["# This prefix map will be used to shrink the uri's down to the prefix level, to help the model better understand them and decrease mistakes.\n","prefix_map = {\"http://www.opengis.net/ont/geosparql#\" : \"geo:\",\n","               \"http://www.opengis.net/def/function/geosparql/\" : \"geof:\",\n","               \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" : \"rdf:\",\n","               \"http://www.w3.org/2000/01/rdf-schema#\" : \"rdfs:\",\n","               \"http://www.w3.org/2001/XMLSchema#\" : \"xsd:\",\n","               \"http://yago-knowledge.org/resource/\" : \"yago:\",\n","               \"http://kr.di.uoa.gr/yago2geo/resource/\" : \"y2geor:\",\n","               \"http://kr.di.uoa.gr/yago2geo/ontology/\" : \"y2geoo:\",\n","               \"http://strdf.di.uoa.gr/ontology#\" : \"strdf:\",\n","               \"http://www.opengis.net/def/uom/OGC/1.0/\" : \"uom:\",\n","               \"http://www.w3.org/2002/07/owl#\" : \"owl:\"}"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:23.005913Z","iopub.status.busy":"2024-09-20T16:16:23.005628Z","iopub.status.idle":"2024-09-20T16:16:24.225370Z","shell.execute_reply":"2024-09-20T16:16:24.224401Z","shell.execute_reply.started":"2024-09-20T16:16:23.005869Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","import json\n","\n","def flatten_dataset(original_dataset):\n","    flattened_data = []\n","    \n","    # Iterate through each numbered key in the dataset\n","    for key in original_dataset:\n","        if key.isdigit():  # Ensure we're only processing the numbered keys\n","            item = original_dataset[key]  # Access the single dictionary in the list\n","            # Create a new entry combining question and answer\n","            query = normalize_variables(item['Query'])\n","            # Shorten the uris down to prefixes.\n","            for uri_map, prefix in prefix_map.items():\n","                query = query.replace(uri_map, prefix)\n","            prompt = create_prompt(item['Question'], query)\n","            flattened_data.append(prompt)\n","    \n","    return flattened_data\n","\n","with open('/kaggle/input/finetuning-no-rdfs/training_set_no_rdfs.json', 'r') as file:\n","    original_dataset = json.load(file)\n","    \n","# Assuming original_dataset is your original dictionary\n","flattened_texts = flatten_dataset(original_dataset)\n","\n","# Create a Dataset object from the flattened data\n","dataset = Dataset.from_dict({\"text\": flattened_texts})"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.226756Z","iopub.status.busy":"2024-09-20T16:16:24.226478Z","iopub.status.idle":"2024-09-20T16:16:24.240584Z","shell.execute_reply":"2024-09-20T16:16:24.239861Z","shell.execute_reply.started":"2024-09-20T16:16:24.226724Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/finetuning-no-rdfs/validation_set_no_rdfs - Copy.json', 'r') as file:\n","    val_dataset = json.load(file)\n","    \n","# Assuming original_dataset is your original dictionary\n","val_flat = flatten_dataset(val_dataset)\n","\n","# Create a Dataset object from the flattened data\n","val_set = Dataset.from_dict({\"text\": val_flat})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.241912Z","iopub.status.busy":"2024-09-20T16:16:24.241597Z","iopub.status.idle":"2024-09-20T16:16:24.249040Z","shell.execute_reply":"2024-09-20T16:16:24.248120Z","shell.execute_reply.started":"2024-09-20T16:16:24.241880Z"},"trusted":true},"outputs":[],"source":["print(dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.250390Z","iopub.status.busy":"2024-09-20T16:16:24.250076Z","iopub.status.idle":"2024-09-20T16:16:24.260071Z","shell.execute_reply":"2024-09-20T16:16:24.259135Z","shell.execute_reply.started":"2024-09-20T16:16:24.250356Z"},"trusted":true},"outputs":[],"source":["print(val_set[4])"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Step 2: Set up the model and tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.263736Z","iopub.status.busy":"2024-09-20T16:16:24.263416Z","iopub.status.idle":"2024-09-20T16:16:37.445053Z","shell.execute_reply":"2024-09-20T16:16:37.444039Z","shell.execute_reply.started":"2024-09-20T16:16:24.263704Z"},"trusted":true},"outputs":[],"source":["!pip install peft"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:37.446692Z","iopub.status.busy":"2024-09-20T16:16:37.446383Z","iopub.status.idle":"2024-09-20T16:16:41.348013Z","shell.execute_reply":"2024-09-20T16:16:41.347013Z","shell.execute_reply.started":"2024-09-20T16:16:37.446660Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","def __get_mistral_tokenizer() -> AutoTokenizer:\n","    tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")\n","    tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n","    tokenizer.padding_side = 'right'\n","    return tokenizer "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:41.349671Z","iopub.status.busy":"2024-09-20T16:16:41.349276Z","iopub.status.idle":"2024-09-20T16:18:03.962875Z","shell.execute_reply":"2024-09-20T16:18:03.961842Z","shell.execute_reply.started":"2024-09-20T16:16:41.349623Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","from peft import prepare_model_for_kbit_training\n","\n","model_id = \"alpindale/Mistral-7B-v0.2-hf\"\n","tokenizer = __get_mistral_tokenizer()\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Set up PEFT (Parameter-Efficient Fine-Tuning)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:06.721377Z","iopub.status.busy":"2024-09-20T16:18:06.720988Z","iopub.status.idle":"2024-09-20T16:18:20.016910Z","shell.execute_reply":"2024-09-20T16:18:20.016121Z","shell.execute_reply.started":"2024-09-20T16:18:06.721333Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","# # ------------------------\n","# # ----- Quantization -----\n","# # ------------------------\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True, \n","#     bnb_4bit_use_double_quant=True, \n","#     bnb_4bit_quant_type=\"nf4\", \n","#     bnb_4bit_compute_dtype=torch.bfloat16\n","# )\n"," \n","# # -----------------\n","# # ----- Model -----\n","# # -----------------\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL.model_id,\n","#     device_map=\"auto\",\n","#     torch_dtype=torch.bfloat16,\n","#     quantization_config=bnb_config\n","# )\n","for param in model.parameters():\n","    param.requires_grad = False\n","    if param.ndim ==1:\n","        param.data = param.data.to(torch.float32)\n","    \n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","# compute_metrics(None, subset_test_dataset)\n","# compute_metrics(None, dataset['test'])\n","\n","# ----------------\n","# ----- LoRA -----\n","# ----------------\n","peft_config = LoraConfig(\n","    lora_dropout=0.1,\n","    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n","    bias = 'none',\n","    # modules_to_save = ['lm_head', 'embed_tokens'],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","peft_config.save_pretrained(\"/kaggle/working\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:20.018739Z","iopub.status.busy":"2024-09-20T16:18:20.018126Z","iopub.status.idle":"2024-09-20T16:18:20.243931Z","shell.execute_reply":"2024-09-20T16:18:20.242977Z","shell.execute_reply.started":"2024-09-20T16:18:20.018704Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import HfApi\n","\n","# Upload the adapter config to the model repo\n","api = HfApi()\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/adapter_config.json\",\n","    path_in_repo=\"adapter_config.json\",\n","    repo_id=\"Stratos-Kakalis/New_FT_Weights\",\n","    repo_type=\"model\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:20.245750Z","iopub.status.busy":"2024-09-20T16:18:20.245431Z","iopub.status.idle":"2024-09-20T21:14:53.858731Z","shell.execute_reply":"2024-09-20T21:14:53.857604Z","shell.execute_reply.started":"2024-09-20T16:18:20.245717Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","# # ------------------------\n","# # ----- Quantization -----\n","# # ------------------------\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True, \n","#     bnb_4bit_use_double_quant=True, \n","#     bnb_4bit_quant_type=\"nf4\", \n","#     bnb_4bit_compute_dtype=torch.bfloat16\n","# )\n"," \n","# # -----------------\n","# # ----- Model -----\n","# # -----------------\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL.model_id,\n","#     device_map=\"auto\",\n","#     torch_dtype=torch.bfloat16,\n","#     quantization_config=bnb_config\n","# )\n","for param in model.parameters():\n","    param.requires_grad = False\n","    if param.ndim ==1:\n","        param.data = param.data.to(torch.float32)\n","    \n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","# compute_metrics(None, subset_test_dataset)\n","# compute_metrics(None, dataset['test'])\n","\n","# ----------------\n","# ----- LoRA -----\n","# ----------------\n","peft_config = LoraConfig(\n","    lora_dropout=0.1,\n","    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n","    bias = 'none',\n","    # modules_to_save = ['lm_head', 'embed_tokens'],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","peft_config.save_pretrained(\"path/to/save/adapter_config\")\n","model = get_peft_model(model, peft_config)\n"," \n","# --------------------\n","# ----- Training -----\n","# --------------------\n","args = TrainingArguments(\n","    output_dir=\"/kaggle/working/\",\n","    # Training length\n","    num_train_epochs=8,\n","    # Important for VRAM\n","    per_device_train_batch_size=8,          # batch size per device during training\n","    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n","    # Other\n","    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n","    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n","    bf16=True,                              # use bfloat16 precision\n","    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n","    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n","    # lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n","    learning_rate=0.0002,\n","    # Logging\n","    logging_dir=\"/kaggle/working/logs/\",\n","    logging_steps=10,\n","    # Evaluation\n","    evaluation_strategy=\"steps\",            # evaluate every 'eval_steps'\n","    eval_steps=10,                         # evaluation step frequency\n","    load_best_model_at_end=True,            # load the best model at the end of training\n","    metric_for_best_model=\"eval_loss\",        # metric to compare the best model\n","    greater_is_better=False\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","#     train_dataset=dataset['train'],\n","    train_dataset=dataset,              ## ????????????\n","    dataset_text_field='text',\n","    eval_dataset=val_set,\n","#     dataset_text_field=\"input\",\n","#     compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,  # We template with special tokens\n","        \"append_concat_token\": False, # No need to add additional separator token\n","    }\n",")\n","\n","trainer.train()\n","\n","trainer.evaluate()\n","\n","# print(\"FULL EVALUATION\")\n","# compute_metrics(None, dataset['test'])\n","\n","# ---------------------------------\n","# ----- Upload to HuggingFace -----\n","# ---------------------------------\n","model.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\", private=True)\n","tokenizer.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\",private=True) "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5299649,"sourceId":8812575,"sourceType":"datasetVersion"},{"datasetId":5628448,"sourceId":9296326,"sourceType":"datasetVersion"},{"datasetId":5728478,"sourceId":9429401,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
