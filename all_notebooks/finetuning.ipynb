{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:02.057954Z","iopub.status.busy":"2024-09-20T16:16:02.057391Z","iopub.status.idle":"2024-09-20T16:16:02.610463Z","shell.execute_reply":"2024-09-20T16:16:02.609480Z","shell.execute_reply.started":"2024-09-20T16:16:02.057919Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["import os\n","from huggingface_hub import login\n","#token\n","login(token='token')"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-20T16:16:02.612927Z","iopub.status.busy":"2024-09-20T16:16:02.612522Z","iopub.status.idle":"2024-09-20T16:16:22.975558Z","shell.execute_reply":"2024-09-20T16:16:22.974369Z","shell.execute_reply.started":"2024-09-20T16:16:02.612881Z"},"trusted":true},"outputs":[],"source":["!pip install -q datasets accelerate evaluate trl accelerate bitsandbytes peft"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.977212Z","iopub.status.busy":"2024-09-20T16:16:22.976874Z","iopub.status.idle":"2024-09-20T16:16:22.983017Z","shell.execute_reply":"2024-09-20T16:16:22.981980Z","shell.execute_reply.started":"2024-09-20T16:16:22.977178Z"},"trusted":true},"outputs":[],"source":["def create_prompt(question, answer):\n","    prompt = f\"\"\"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\n","\n","Human: {question}\n","Generator: '''{answer}'''\"\"\"\n","    return prompt"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.985425Z","iopub.status.busy":"2024-09-20T16:16:22.985017Z","iopub.status.idle":"2024-09-20T16:16:22.994599Z","shell.execute_reply":"2024-09-20T16:16:22.993814Z","shell.execute_reply.started":"2024-09-20T16:16:22.985392Z"},"trusted":true},"outputs":[],"source":["import re \n","\n","def normalize_variables(query):\n","    if query is None:\n","        return \"\"\n","    variable_pattern = re.compile(r\"\\?\\w+\")\n","    variables = variable_pattern.findall(query)\n","    normalized_query = query\n","    for i, var in enumerate(variables):\n","        normalized_query = normalized_query.replace(var, f\"?var{i}\")\n","    return normalized_query"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:22.995929Z","iopub.status.busy":"2024-09-20T16:16:22.995629Z","iopub.status.idle":"2024-09-20T16:16:23.004667Z","shell.execute_reply":"2024-09-20T16:16:23.003809Z","shell.execute_reply.started":"2024-09-20T16:16:22.995889Z"},"trusted":true},"outputs":[],"source":["# This prefix map will be used to shrink the uri's down to the prefix level, to help the model better understand them and decrease mistakes.\n","prefix_map = {\"http://www.opengis.net/ont/geosparql#\" : \"geo:\",\n","               \"http://www.opengis.net/def/function/geosparql/\" : \"geof:\",\n","               \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" : \"rdf:\",\n","               \"http://www.w3.org/2000/01/rdf-schema#\" : \"rdfs:\",\n","               \"http://www.w3.org/2001/XMLSchema#\" : \"xsd:\",\n","               \"http://yago-knowledge.org/resource/\" : \"yago:\",\n","               \"http://kr.di.uoa.gr/yago2geo/resource/\" : \"y2geor:\",\n","               \"http://kr.di.uoa.gr/yago2geo/ontology/\" : \"y2geoo:\",\n","               \"http://strdf.di.uoa.gr/ontology#\" : \"strdf:\",\n","               \"http://www.opengis.net/def/uom/OGC/1.0/\" : \"uom:\",\n","               \"http://www.w3.org/2002/07/owl#\" : \"owl:\"}"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:23.005913Z","iopub.status.busy":"2024-09-20T16:16:23.005628Z","iopub.status.idle":"2024-09-20T16:16:24.225370Z","shell.execute_reply":"2024-09-20T16:16:24.224401Z","shell.execute_reply.started":"2024-09-20T16:16:23.005869Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","import json\n","\n","def flatten_dataset(original_dataset):\n","    flattened_data = []\n","    \n","    # Iterate through each numbered key in the dataset\n","    for key in original_dataset:\n","        if key.isdigit():  # Ensure we're only processing the numbered keys\n","            item = original_dataset[key]  # Access the single dictionary in the list\n","            # Create a new entry combining question and answer\n","            query = normalize_variables(item['Query'])\n","            # Shorten the uris down to prefixes.\n","            for uri_map, prefix in prefix_map.items():\n","                query = query.replace(uri_map, prefix)\n","            prompt = create_prompt(item['Question'], query)\n","            flattened_data.append(prompt)\n","    \n","    return flattened_data\n","\n","with open('/kaggle/input/finetuning-no-rdfs/training_set_no_rdfs.json', 'r') as file:\n","    original_dataset = json.load(file)\n","    \n","# Assuming original_dataset is your original dictionary\n","flattened_texts = flatten_dataset(original_dataset)\n","\n","# Create a Dataset object from the flattened data\n","dataset = Dataset.from_dict({\"text\": flattened_texts})"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.226756Z","iopub.status.busy":"2024-09-20T16:16:24.226478Z","iopub.status.idle":"2024-09-20T16:16:24.240584Z","shell.execute_reply":"2024-09-20T16:16:24.239861Z","shell.execute_reply.started":"2024-09-20T16:16:24.226724Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/finetuning-no-rdfs/validation_set_no_rdfs - Copy.json', 'r') as file:\n","    val_dataset = json.load(file)\n","    \n","# Assuming original_dataset is your original dictionary\n","val_flat = flatten_dataset(val_dataset)\n","\n","# Create a Dataset object from the flattened data\n","val_set = Dataset.from_dict({\"text\": val_flat})"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.241912Z","iopub.status.busy":"2024-09-20T16:16:24.241597Z","iopub.status.idle":"2024-09-20T16:16:24.249040Z","shell.execute_reply":"2024-09-20T16:16:24.248120Z","shell.execute_reply.started":"2024-09-20T16:16:24.241880Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: What is the population of Central Greece?\\nGenerator: '''SELECT DISTINCT ?var0 WHERE { <yago:Central_Greece_(region)> y2geoo:hasGAG_Population ?var0 }'''\"}\n"]}],"source":["print(dataset[0])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.250390Z","iopub.status.busy":"2024-09-20T16:16:24.250076Z","iopub.status.idle":"2024-09-20T16:16:24.260071Z","shell.execute_reply":"2024-09-20T16:16:24.259135Z","shell.execute_reply.started":"2024-09-20T16:16:24.250356Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': \"Generator is an expert SPARQL query generator. For each question that the user supplies, the generator will convert it into a valid SPARQL query that can be used to answer the question. The query should be enclosed by three backticks on new lines, denoting that it is a code block.\\n\\nHuman: Is Ierapetra south of Athens?\\nGenerator: '''ASK {     <yago:Ierapetra> geo:hasGeometry ?var0. ?var0 geo:asWKT ?var2.      <yago:geoentity_Dimos_Athens_8133876> geo:hasGeometry ?var3.     ?var3 geo:asWKT ?var5.   FILTER (strdf:below(?var2, ?var5)) }'''\"}\n"]}],"source":["print(val_set[4])"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Step 2: Set up the model and tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:24.263736Z","iopub.status.busy":"2024-09-20T16:16:24.263416Z","iopub.status.idle":"2024-09-20T16:16:37.445053Z","shell.execute_reply":"2024-09-20T16:16:37.444039Z","shell.execute_reply.started":"2024-09-20T16:16:24.263704Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.12.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"]}],"source":["!pip install peft"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:37.446692Z","iopub.status.busy":"2024-09-20T16:16:37.446383Z","iopub.status.idle":"2024-09-20T16:16:41.348013Z","shell.execute_reply":"2024-09-20T16:16:41.347013Z","shell.execute_reply.started":"2024-09-20T16:16:37.446660Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","def __get_mistral_tokenizer() -> AutoTokenizer:\n","    tokenizer = AutoTokenizer.from_pretrained(\"alpindale/Mistral-7B-v0.2-hf\")\n","    tokenizer.pad_token = tokenizer.unk_token#\"<PAD>\"\n","    tokenizer.padding_side = 'right'\n","    return tokenizer "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:16:41.349671Z","iopub.status.busy":"2024-09-20T16:16:41.349276Z","iopub.status.idle":"2024-09-20T16:18:03.962875Z","shell.execute_reply":"2024-09-20T16:18:03.961842Z","shell.execute_reply.started":"2024-09-20T16:16:41.349623Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d677ed90f4a14140a99639da5e9088c6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/960 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0f91d82202d4b58b382888b8887529f","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecf721806bc048b6b6725e2d01cdfc53","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10e8c8c879f84f2e84b9313bcb7ce507","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c95ddd3c33b148ec81b2452ff951a49f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"358f573626144aca9bcbbce160977ef3","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fac40d4da949463dad9f0a48da01b76c","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8617a83661e745e2b3511c819a87c7bd","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f85274c2f770452b9307e6055cc8548f","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74c6b318add148169321401ecbe76683","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"369a4314e819477bbf6dace1d98dfbb1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0b0f869dcf34f208734c58d88425864","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","from peft import prepare_model_for_kbit_training\n","\n","model_id = \"alpindale/Mistral-7B-v0.2-hf\"\n","tokenizer = __get_mistral_tokenizer()\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Set up PEFT (Parameter-Efficient Fine-Tuning)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:06.721377Z","iopub.status.busy":"2024-09-20T16:18:06.720988Z","iopub.status.idle":"2024-09-20T16:18:20.016910Z","shell.execute_reply":"2024-09-20T16:18:20.016121Z","shell.execute_reply.started":"2024-09-20T16:18:06.721333Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","# # ------------------------\n","# # ----- Quantization -----\n","# # ------------------------\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True, \n","#     bnb_4bit_use_double_quant=True, \n","#     bnb_4bit_quant_type=\"nf4\", \n","#     bnb_4bit_compute_dtype=torch.bfloat16\n","# )\n"," \n","# # -----------------\n","# # ----- Model -----\n","# # -----------------\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL.model_id,\n","#     device_map=\"auto\",\n","#     torch_dtype=torch.bfloat16,\n","#     quantization_config=bnb_config\n","# )\n","for param in model.parameters():\n","    param.requires_grad = False\n","    if param.ndim ==1:\n","        param.data = param.data.to(torch.float32)\n","    \n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","# compute_metrics(None, subset_test_dataset)\n","# compute_metrics(None, dataset['test'])\n","\n","# ----------------\n","# ----- LoRA -----\n","# ----------------\n","peft_config = LoraConfig(\n","    lora_dropout=0.1,\n","    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n","    bias = 'none',\n","    # modules_to_save = ['lm_head', 'embed_tokens'],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","peft_config.save_pretrained(\"/kaggle/working\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:20.018739Z","iopub.status.busy":"2024-09-20T16:18:20.018126Z","iopub.status.idle":"2024-09-20T16:18:20.243931Z","shell.execute_reply":"2024-09-20T16:18:20.242977Z","shell.execute_reply.started":"2024-09-20T16:18:20.018704Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/New_FT_Weights/commit/4e4b309cde515910a27c57fdc8fcd74295c8bb64', commit_message='Upload adapter_config.json with huggingface_hub', commit_description='', oid='4e4b309cde515910a27c57fdc8fcd74295c8bb64', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi\n","\n","# Upload the adapter config to the model repo\n","api = HfApi()\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/adapter_config.json\",\n","    path_in_repo=\"adapter_config.json\",\n","    repo_id=\"Stratos-Kakalis/New_FT_Weights\",\n","    repo_type=\"model\",\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T16:18:20.245750Z","iopub.status.busy":"2024-09-20T16:18:20.245431Z","iopub.status.idle":"2024-09-20T21:14:53.858731Z","shell.execute_reply":"2024-09-20T21:14:53.857604Z","shell.execute_reply.started":"2024-09-20T16:18:20.245717Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, dataset_kwargs. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2692e33d988e4ac1adbc5cc58c9a5771","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/754 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"440dc8c118354015800891fa468fc461","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/92 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.18.1 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240920_161831-1w3pte5w</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">/kaggle/working/</a></strong> to <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w' target=\"_blank\">https://wandb.ai/stratoskakalis123-national-and-kapodistrian-university-o/huggingface/runs/1w3pte5w</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [376/376 4:54:01, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.739600</td>\n","      <td>1.559345</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.139500</td>\n","      <td>0.718919</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.529900</td>\n","      <td>0.432377</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.399900</td>\n","      <td>0.362587</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.352200</td>\n","      <td>0.328678</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.301300</td>\n","      <td>0.313414</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.287600</td>\n","      <td>0.303795</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.296000</td>\n","      <td>0.292512</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.281900</td>\n","      <td>0.283960</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.265400</td>\n","      <td>0.279635</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.265900</td>\n","      <td>0.271931</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.236800</td>\n","      <td>0.266753</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.253300</td>\n","      <td>0.264406</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.246000</td>\n","      <td>0.259101</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.223500</td>\n","      <td>0.261006</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.215600</td>\n","      <td>0.256545</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.232300</td>\n","      <td>0.253065</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.225400</td>\n","      <td>0.253050</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.220300</td>\n","      <td>0.247555</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.202300</td>\n","      <td>0.251507</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.206300</td>\n","      <td>0.246796</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.213500</td>\n","      <td>0.243952</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.202500</td>\n","      <td>0.245366</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.192500</td>\n","      <td>0.243025</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.190400</td>\n","      <td>0.244651</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.197500</td>\n","      <td>0.248072</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.189000</td>\n","      <td>0.240951</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.189900</td>\n","      <td>0.241467</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.179500</td>\n","      <td>0.244546</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.178400</td>\n","      <td>0.242785</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.172000</td>\n","      <td>0.241941</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.182800</td>\n","      <td>0.239014</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.180800</td>\n","      <td>0.240981</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.162400</td>\n","      <td>0.242569</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.172300</td>\n","      <td>0.242886</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.172600</td>\n","      <td>0.243584</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.168300</td>\n","      <td>0.242901</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 01:08]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"784be7f652834bf3abbfd279500a61b1","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"257c27890fed4a3ca64c01f8e8e95794","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45653f21e03e444e8099c41b753cd231","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Stratos-Kakalis/norm_trunc_no_rdfs_8_epoch/commit/a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', commit_message='Upload tokenizer', commit_description='', oid='a3baa8f92e6c97b4f34d2c2bb8d418e3181917ce', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from peft import LoraConfig, get_peft_model\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","# # ------------------------\n","# # ----- Quantization -----\n","# # ------------------------\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True, \n","#     bnb_4bit_use_double_quant=True, \n","#     bnb_4bit_quant_type=\"nf4\", \n","#     bnb_4bit_compute_dtype=torch.bfloat16\n","# )\n"," \n","# # -----------------\n","# # ----- Model -----\n","# # -----------------\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL.model_id,\n","#     device_map=\"auto\",\n","#     torch_dtype=torch.bfloat16,\n","#     quantization_config=bnb_config\n","# )\n","for param in model.parameters():\n","    param.requires_grad = False\n","    if param.ndim ==1:\n","        param.data = param.data.to(torch.float32)\n","    \n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","# compute_metrics(None, subset_test_dataset)\n","# compute_metrics(None, dataset['test'])\n","\n","# ----------------\n","# ----- LoRA -----\n","# ----------------\n","peft_config = LoraConfig(\n","    lora_dropout=0.1,\n","    # target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n","    bias = 'none',\n","    # modules_to_save = ['lm_head', 'embed_tokens'],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","peft_config.save_pretrained(\"path/to/save/adapter_config\")\n","model = get_peft_model(model, peft_config)\n"," \n","# --------------------\n","# ----- Training -----\n","# --------------------\n","args = TrainingArguments(\n","    output_dir=\"/kaggle/working/\",\n","    # Training length\n","    num_train_epochs=8,\n","    # Important for VRAM\n","    per_device_train_batch_size=8,          # batch size per device during training\n","    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n","    # Other\n","    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n","    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n","    bf16=True,                              # use bfloat16 precision\n","    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n","    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n","    # lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n","    learning_rate=0.0002,\n","    # Logging\n","    logging_dir=\"/kaggle/working/logs/\",\n","    logging_steps=10,\n","    # Evaluation\n","    evaluation_strategy=\"steps\",            # evaluate every 'eval_steps'\n","    eval_steps=10,                         # evaluation step frequency\n","    load_best_model_at_end=True,            # load the best model at the end of training\n","    metric_for_best_model=\"eval_loss\",        # metric to compare the best model\n","    greater_is_better=False\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","#     train_dataset=dataset['train'],\n","    train_dataset=dataset,              ## ????????????\n","    dataset_text_field='text',\n","    eval_dataset=val_set,\n","#     dataset_text_field=\"input\",\n","#     compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,  # We template with special tokens\n","        \"append_concat_token\": False, # No need to add additional separator token\n","    }\n",")\n","\n","trainer.train()\n","\n","trainer.evaluate()\n","\n","# print(\"FULL EVALUATION\")\n","# compute_metrics(None, dataset['test'])\n","\n","# ---------------------------------\n","# ----- Upload to HuggingFace -----\n","# ---------------------------------\n","model.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\", private=True)\n","tokenizer.push_to_hub(\"norm_trunc_no_rdfs_8_epoch\",private=True) "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5299649,"sourceId":8812575,"sourceType":"datasetVersion"},{"datasetId":5628448,"sourceId":9296326,"sourceType":"datasetVersion"},{"datasetId":5728478,"sourceId":9429401,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
