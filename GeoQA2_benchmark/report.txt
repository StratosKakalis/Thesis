Report for the progress since the last meeting:
    
    1.) Automated the process of evaluation: In the first tests I performed with wat, rel and gpt I manually evaluated all 1089 answers which was very tedious as a process and would not allow me to
        test multiple versions of a certain model (like gpt). Furthermore, the human error is quite noticeable after hand evaluating hundreds of questions. 
        
        This issue was resolved by automating the entire evaluation process. 
        
        1.1) I created a dataset with all the correct toponyms and correct corresponding wikipedia links for each of the 1089 questions in the given dataset to evaluate the models on toponym recognition 
        and wikification. Furthermore, I corrected some spelling errors in the original questions to allow for better predictions (even though later on I also take measures for typos). From now on I will
        refer to this labeled dataset as the ground truth values (toponyms and links).

        1.2) The evaluation is done based on three metrics. Precision, Recall and F1-score. In order to calculate those metrics we need the True-Positive(TP), False-Positive(FP) and False-Negative(FN) values.
        We can measure those by comparing the predicted toponyms with the ground truth toponyms in the dataset. The comparisson itself is complicated and analyzed in section 1.3). As for the TP, FP and FN
        values we calculate those this way: 
            * TP are all the toponyms that are predicted by the model and also exist in the ground truth. 
            * FP are all the toponyms that are predicted by the model but are not present in the ground truth and 
            * FN are all the toponyms that exist in the ground truth but were never predicted by the model.

        1.3) The method for the comparisson of a predicted toponym with a ground truth toponym has these characteristics:
            * It is dynamic: it allows for subsets of names to be considered the same (e.g. city of London = London, Central Greece = Central Greece Region).
            * It allows for slight typos that often originate from the original questions (e.g. Siros = Syros, Bublin = Dublin). This is achieved by measuring the similarity of two words as the Levenshtein 
              distance of those two words. A threshold was fine tuned to allow for small errors.
            * It even allows some abbreviations (e.g. UK = United Kingdom, USA = US = United Stated = United States of America). This is done because the LLM models tend to answer with a variety of common 
              names, so even if 'USA' is present in the given sentence the model might unwrap it to 'United States' which is not strictly wrong. This feature is available only for those two abbreviations as
              they are the only two present in multiple questions, it is hard coded/there is no other intelligent process to identify abbreviations.   
    
    2.) Performed inference and then evaluation on models:
        
        2.1) Initially as a test of the evaluation pipeline and as the creation of control values I re-run inference on the 4 previously tested models (WAT, REL and gpt with zero and few shot learning) and 
        received the following results: 
               Model| Precision|    Recall|   F1-Score| Wikification Accuracy
                 REL|0.97429306|0.94987469|0.961928934|64.66836734693877
                 WAT|0.40400327|0.81533388|0.540289538|5.188031841888553
             GPT-one|0.70484848|0.91001564|0.794398907|59.66119640021175
             GPT-few|0.97629899|0.89848993|0.935779816|69.84858459512837

            Observations: -WAT has poor precision because it acts as an entity linker as we had established on our previous test. It's wikification accuracy metric is not valid as WAT uses a differenct link 
                           format and there is not much reason to grade WAT's ability for wikification as it can be assumed as perfect (since it's probably from some database and not generated by AI). Even the
                           models recall is poor which gives the worst F1-score of all the models and deems it incapable to compete.
                          -REL has the best overall metrics with the highest F1-score (although it loses to GPT-few on precision) and the best wikification accuracy. Interestingly all the models have a fairly 
                           low wikification accuracy which means that the metric might need to be changed or the leniency of comparisson needs to be altered or that even the dataset needs to be fixed. I will
                           look into this in the future.
                          -GPT-one already shows promice with its high recall and decent precision outperforming WAT already. The wikification accuracy is quite low and with a f1-score of 0.79 it doesn't compare
                           with REL at a F1-score of 0.96.
                          -GPT-few shows impressive improvements over GPT-one with better overall stats, the best overall Precision and an impressive 0.94 f1-score which compares with REL. Also the wikification
                           accuracy is significantly better and not much worse than REL which shows promise for GPT to potentially overtake a model such as REL even for the task of wikification. 

        2.2) Performing prompt engineering to get better results from GPT-few and testing multiple prompts:
               Model| Precision|    Recall|   F1-Score| Wikification Accuracy
            GPT-few2|    0.9393|    0.9479|     0.9436| 80.66
            GPT-few3|    0.9552|    0.9488|     0.9520| 77.95
            GPT-few4|    0.9591|    0.7663|     0.8519| 67.12
            GPT-few5|    0.9185|    0.9523|     0.9350| 78.24
            GPT-few6|    0.5157|    0.5617|     0.5377| 63.40
            GPT-few7|    0.8389|    0.8683|     0.8533| 69.68
            GPT-few8|    0.9269|    0.9648|     0.9455| 80.56
            GPT-few9|    0.9083|    0.9674|     0.9369| 81.40

            Observations: -Since the gpt-few and few2 the model seems to realize well that it's task is toponym recognition and wikification and very rarely answers other questions or does not follow the required 
                           format. This means that we have formed a prompt that takes good care of that issue. One issue that remains is the model sometimes responds with a toponym followed by 'No toponym found'
                           also sometimes this way the model avoids providing a link. Furthermore, the model is currently incapable of saying it isn't confident for a link (some cases links don't exist and Unavailable. 
                           should be printed).
            
                          -GPT-few4 model was created with a major difference from previous models. It tests showing the examples of questions and answers in the user role with hopes of gpt completing the series 
                           of questions and answers rather than answering. This however introduces the error of gpt answering with full questions and answers in many occations which is the primary reason for this 
                           model's failure. Interestingly this model showcases the best wikification accuracy among all gpt models so far, even better than REL.
                          -GPT-few6: I tried explain the format of the output more clearly but the model ended up adopting the 'Wiki link:' in some of it's answers. The model also starts a lot of it's answers with 
                           'Toponym:' which is perceived as the toponym name by the evaluator. That's why this model failed. If we managed the same results without 'Toponym: and Wiki links:' this could prove powerful.
                          -GPT-few7: adopted a different pattern. While it still (sometimes) answers with questions and answers it often starts those faulty answers with 'Sorry I cannot browse the internet to provide
                           links' or something along that line. Clearly the model assumes we are expecting something different of it with this prompt.