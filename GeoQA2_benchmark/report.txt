Report for the progress since the last meeting:
    
    1.) Automated the process of evaluation: In the first tests I performed with wat, rel and gpt I manually evaluated all 1089 answers which was very tedious as a process and would not allow me to
        test multiple versions of a certain model (like gpt). Furthermore, the human error is quite noticeable after hand evaluating hundreds of questions. 
        
        This issue was resolved by automating the entire evaluation process. 
        
        1.1) I created a dataset with all the correct toponyms and correct corresponding wikipedia links for each of the 1089 questions in the given dataset to evaluate the models on toponym recognition 
        and wikification. Furthermore, I corrected some spelling error in the original questions to allow for better predictions (even though later on I also take measures for typos). From now on I will
        refer to this labeled dataset as the ground truth values (toponyms and links).

        1.2) The evaluation is done based on three metrics. Precision, Recall and F1-score. In order to calculate those metrics we need the True-Positive(TP), False-Positive(FP) and False-Negative(FN) values.
        We can measure those by comparing the predicted toponyms with the ground truth toponyms in the dataset. The comparisson itself is complicated and analyzed in section 1.3). As for the TP, FP and FN
        values we calculate those this way: 
            * TP are all the toponyms that are predicted by the model and also exist in the ground truth. 
            * FP are all the toponyms that are predicted by the model but are not present in the ground truth and 
            * FN are all the toponyms that exist in the ground truth but were never predicted by the model.

        1.3) The method for the comparisson of a predicted toponym with a ground truth toponym has these characteristics:
            * It is dynamic: it allows for subsets of names to be considered the same (e.g. city of London = London, Central Greece = Central Greece Region).
            * It allows for slight typos that often originate from the original questions (e.g. Siros = Syros, Bublin = Dublin). This is achieved by measuring the similarity of two words as the Levenshtein 
              distance of those two words. A threshold was fine tuned to allow for small errors.
            * It even allows some abbreviations (e.g. UK = United Kingdom, USA = US = United Stated = United States of America). This is done because the LLM models tend to answer with a variety of common 
              names, so even if 'USA' is present in the given sentence the model might unwrap it to 'United States' which is not strictly wrong. This feature is available only for those two abbreviations as
              they are the only two present in multiple questions, it is hard coded/there is no other intelligent process to identify abbreviations.   
    
    2.) Performed inference and then evaluation on models:
        
        2.1) Initially as a test of the evaluation pipeline and as the creation of control values I re-run inference on the 4 previously tested models (WAT, REL and gpt with zero and few shot learning) and 
        received the following results: 
               Model| Precision|    Recall|   F1-Score| Wikification Accuracy
                 REL|0.97429306|0.94987469|0.961928934|74.34166103983794  
                 WAT|0.40400327|0.81533388|0.540289538|0.0
             GPT-one|0.70484848|0.91001564|0.794398907|51.99386503067485
             GPT-few|0.97629899|0.89848993|0.935779816|60.44776119402984

            Observations: -WAT has poor precision because it acts as an entity linker as we had established on our previous test. It's wikification accuracy metric is not valid as WAT uses a differenct link 
                           format and there is not much reason to grade WAT's ability for wikification as it can be assumed as perfect (since it's probably from some database and not generated by AI). Even the
                           models recall is poor which gives the worst F1-score of all the models and deems it incapable to compete.
                          -REL has the best overall metrics with the highest F1-score (although it loses to GPT-few on precision) and the best wikification accuracy. Interestingly all the models have a fairly 
                           low wikification accuracy which means that the metric might need to be changed or the leniency of comparisson needs to be altered or that even the dataset needs to be fixed. I will
                           look into this in the future.
                          -GPT-one already shows promice with its high recall and decent precision outperforming WAT already. The wikification accuracy is quite low and with a f1-score of 0.79 it doesn't compare
                           with REL at a F1-score of 0.96.
                          -GPT-few shows impressive improvements over GPT-one with better overall stats, the best overall Precision and an impressive 0.94 f1-score which compares with REL. Also the wikification
                           accuracy is significantly better and not much worse than REL which shows promise for GPT to potentially overtake a model such as REL even for the task of wikification. 

        2.2) Performing prompt engineering to get better results from GPT-few and testing multiple prompts:
               Model| Precision|    Recall|   F1-Score| Wikification Accuracy
            GPT-few2|0.93931837|0.94798657| 0.94363256|71.26666666666667
                    |          |          |           |
                    |          |          |           |
                    |          |          |           |